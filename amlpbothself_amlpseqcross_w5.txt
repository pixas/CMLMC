2023-01-11 19:02:05 | INFO | fairseq.distributed_utils | distributed init (rank 3): tcp://localhost:15163
2023-01-11 19:02:05 | INFO | fairseq.distributed_utils | distributed init (rank 1): tcp://localhost:15163
2023-01-11 19:02:06 | INFO | fairseq.distributed_utils | distributed init (rank 0): tcp://localhost:15163
2023-01-11 19:02:06 | INFO | fairseq.distributed_utils | distributed init (rank 2): tcp://localhost:15163
2023-01-11 19:02:06 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-01-11 19:02:06 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-01-11 19:02:06 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-01-11 19:02:06 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-01-11 19:02:06 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for 4 nodes.
2023-01-11 19:02:06 | INFO | fairseq.distributed_utils | initialized host SH-IDC1-10-140-0-249 as rank 0
2023-01-11 19:02:06 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for 4 nodes.
2023-01-11 19:02:06 | INFO | fairseq.distributed_utils | initialized host SH-IDC1-10-140-0-249 as rank 2
2023-01-11 19:02:06 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for 4 nodes.
2023-01-11 19:02:06 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for 4 nodes.
2023-01-11 19:02:06 | INFO | fairseq.distributed_utils | initialized host SH-IDC1-10-140-0-249 as rank 1
2023-01-11 19:02:06 | INFO | fairseq.distributed_utils | initialized host SH-IDC1-10-140-0-249 as rank 3
2023-01-11 19:02:12 | INFO | fairseq_cli.train | Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, amlp_activation='softmax', apply_bert_init=True, arch='cmlm_transformer_wmt_en_de', attention_dropout=0.0, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_suffix='', clip_norm=0.0, concatPE=True, cpu=False, criterion='nat_loss', cross_self_attention=False, curriculum=0, data='/mnt/petrelfs/jiangshuyang/data-bin/wmt14_deen_distill_jointdict', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', decoder_attention_heads=8, decoder_cross_attention_type='amlpseq', decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=True, decoder_normalize_before=False, decoder_output_dim=512, decoder_self_attention_type='covamlp2', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:15163', distributed_no_spawn=False, distributed_num_procs=4, distributed_port=-1, distributed_rank=0, distributed_world_size=4, distributed_wrapper='DDP', dont_use_layernorm=False, dropout=0.2, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, encoder_self_attention_type='covamlp2', eval_bleu=True, eval_bleu_args='{"iter_decode_max_iter": 0, "iter_decode_with_beam": 1}', eval_bleu_detok='space', eval_bleu_detok_args=None, eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=True, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=7, fp16=True, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, insertCausalSelfAttn=False, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_last_epochs=20, label_smoothing=0.1, landmarks=16, left_pad_source='True', left_pad_target='False', length_loss_factor=0.1, load_alignments=False, localsgd_frequency=3, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', maskdistshiftpower=1.0, max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=16384, max_tokens_valid=16384, max_update=300000, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, model_parallel_size=1, ngram_predictor=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=True, no_token_positional_embeddings=False, noise='random_mask', nprocs_per_node=4, num_batch_buckets=0, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=None, pipeline_devices=None, pipeline_model_parallel=False, pred_length_offset=False, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, replacefactor=0.3, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='/mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, selfcorrection=0, sentence_avg=False, sg_length_pred=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, slowmo_algorithm='LocalSGD', slowmo_momentum=None, source_lang='de', src_embedding_copy=False, stop_time_hours=0, target_lang='en', task='translation_lev', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, use_old_adam=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, warmup_init_lr=1e-07, warmup_updates=40000, weight_decay=0.01, zero_sharding='none')
2023-01-11 19:02:12 | INFO | fairseq.tasks.translation | [de] dictionary: 39960 types
2023-01-11 19:02:12 | INFO | fairseq.tasks.translation | [en] dictionary: 39960 types
2023-01-11 19:02:12 | INFO | fairseq.data.data_utils | loaded 3000 examples from: /mnt/petrelfs/jiangshuyang/data-bin/wmt14_deen_distill_jointdict/valid.de-en.de
2023-01-11 19:02:12 | INFO | fairseq.data.data_utils | loaded 3000 examples from: /mnt/petrelfs/jiangshuyang/data-bin/wmt14_deen_distill_jointdict/valid.de-en.en
2023-01-11 19:02:12 | INFO | fairseq.tasks.translation | /mnt/petrelfs/jiangshuyang/data-bin/wmt14_deen_distill_jointdict valid de-en 3000 examples
2023-01-11 19:02:12 | INFO | root | Using efficient attention FSCovAMLP2
2023-01-11 19:02:12 | INFO | root | Using efficient attention FSCovAMLP2
2023-01-11 19:02:12 | INFO | root | Using efficient attention FSCovAMLP2
2023-01-11 19:02:12 | INFO | root | Using efficient attention FSCovAMLP2
2023-01-11 19:02:12 | INFO | root | Using efficient attention FSCovAMLP2
2023-01-11 19:02:12 | INFO | root | Using efficient attention FSCovAMLP2
2023-01-11 19:02:12 | INFO | root | Using efficient attention FSCovAMLP2
2023-01-11 19:02:12 | INFO | root | Using efficient attention FSAMLPSeq
2023-01-11 19:02:12 | INFO | root | Using efficient attention FSCovAMLP2
2023-01-11 19:02:12 | INFO | root | Using efficient attention FSAMLPSeq
2023-01-11 19:02:12 | INFO | root | Using efficient attention FSCovAMLP2
2023-01-11 19:02:13 | INFO | root | Using efficient attention FSAMLPSeq
2023-01-11 19:02:13 | INFO | root | Using efficient attention FSCovAMLP2
2023-01-11 19:02:13 | INFO | root | Using efficient attention FSAMLPSeq
2023-01-11 19:02:13 | INFO | root | Using efficient attention FSCovAMLP2
2023-01-11 19:02:13 | INFO | root | Using efficient attention FSAMLPSeq
2023-01-11 19:02:13 | INFO | root | Using efficient attention FSCovAMLP2
2023-01-11 19:02:13 | INFO | root | Using efficient attention FSAMLPSeq
2023-01-11 19:02:13 | INFO | fairseq_cli.train | CMLMNATransformerModel(
  (encoder): FairseqNATEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39960, 512, padding_idx=1)
    (embed_positions): LearnedPositionalEmbedding(1026, 512, padding_idx=1)
    (layers): ModuleList(
      (0): TransformerEncoderLayer(
        (self_attn): FSCls(
          (d_proj): Linear(in_features=512, out_features=512, bias=True)
          (d_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (c_proj): Linear(in_features=512, out_features=128, bias=True)
          (c_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (s_proj): Linear(in_features=64, out_features=64, bias=True)
          (in_proj): Linear(in_features=512, out_features=512, bias=True)
          (inner_proj): Linear(in_features=16, out_features=16, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=8)
          (drop): Dropout(p=0.0, inplace=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayer(
        (self_attn): FSCls(
          (d_proj): Linear(in_features=512, out_features=512, bias=True)
          (d_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (c_proj): Linear(in_features=512, out_features=128, bias=True)
          (c_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (s_proj): Linear(in_features=64, out_features=64, bias=True)
          (in_proj): Linear(in_features=512, out_features=512, bias=True)
          (inner_proj): Linear(in_features=16, out_features=16, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=8)
          (drop): Dropout(p=0.0, inplace=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayer(
        (self_attn): FSCls(
          (d_proj): Linear(in_features=512, out_features=512, bias=True)
          (d_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (c_proj): Linear(in_features=512, out_features=128, bias=True)
          (c_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (s_proj): Linear(in_features=64, out_features=64, bias=True)
          (in_proj): Linear(in_features=512, out_features=512, bias=True)
          (inner_proj): Linear(in_features=16, out_features=16, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=8)
          (drop): Dropout(p=0.0, inplace=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayer(
        (self_attn): FSCls(
          (d_proj): Linear(in_features=512, out_features=512, bias=True)
          (d_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (c_proj): Linear(in_features=512, out_features=128, bias=True)
          (c_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (s_proj): Linear(in_features=64, out_features=64, bias=True)
          (in_proj): Linear(in_features=512, out_features=512, bias=True)
          (inner_proj): Linear(in_features=16, out_features=16, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=8)
          (drop): Dropout(p=0.0, inplace=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayer(
        (self_attn): FSCls(
          (d_proj): Linear(in_features=512, out_features=512, bias=True)
          (d_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (c_proj): Linear(in_features=512, out_features=128, bias=True)
          (c_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (s_proj): Linear(in_features=64, out_features=64, bias=True)
          (in_proj): Linear(in_features=512, out_features=512, bias=True)
          (inner_proj): Linear(in_features=16, out_features=16, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=8)
          (drop): Dropout(p=0.0, inplace=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayer(
        (self_attn): FSCls(
          (d_proj): Linear(in_features=512, out_features=512, bias=True)
          (d_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (c_proj): Linear(in_features=512, out_features=128, bias=True)
          (c_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (s_proj): Linear(in_features=64, out_features=64, bias=True)
          (in_proj): Linear(in_features=512, out_features=512, bias=True)
          (inner_proj): Linear(in_features=16, out_features=16, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=8)
          (drop): Dropout(p=0.0, inplace=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (PEfc): Linear(in_features=1024, out_features=512, bias=True)
  )
  (decoder): NATransformerDecoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(39960, 512, padding_idx=1)
    (PEfc): Linear(in_features=1024, out_features=512, bias=True)
    (embed_positions): LearnedPositionalEmbedding(1026, 512, padding_idx=1)
    (layers): ModuleList(
      (0): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): FSCls(
          (d_proj): Linear(in_features=512, out_features=512, bias=True)
          (d_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (c_proj): Linear(in_features=512, out_features=128, bias=True)
          (c_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (s_proj): Linear(in_features=64, out_features=64, bias=True)
          (in_proj): Linear(in_features=512, out_features=512, bias=True)
          (inner_proj): Linear(in_features=16, out_features=16, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=8)
          (drop): Dropout(p=0.0, inplace=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): FSCls(
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (w_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (approx_out_proj): Linear(in_features=128, out_features=64, bias=True)
          (out_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): FSCls(
          (d_proj): Linear(in_features=512, out_features=512, bias=True)
          (d_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (c_proj): Linear(in_features=512, out_features=128, bias=True)
          (c_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (s_proj): Linear(in_features=64, out_features=64, bias=True)
          (in_proj): Linear(in_features=512, out_features=512, bias=True)
          (inner_proj): Linear(in_features=16, out_features=16, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=8)
          (drop): Dropout(p=0.0, inplace=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): FSCls(
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (w_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (approx_out_proj): Linear(in_features=128, out_features=64, bias=True)
          (out_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): FSCls(
          (d_proj): Linear(in_features=512, out_features=512, bias=True)
          (d_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (c_proj): Linear(in_features=512, out_features=128, bias=True)
          (c_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (s_proj): Linear(in_features=64, out_features=64, bias=True)
          (in_proj): Linear(in_features=512, out_features=512, bias=True)
          (inner_proj): Linear(in_features=16, out_features=16, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=8)
          (drop): Dropout(p=0.0, inplace=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): FSCls(
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (w_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (approx_out_proj): Linear(in_features=128, out_features=64, bias=True)
          (out_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): FSCls(
          (d_proj): Linear(in_features=512, out_features=512, bias=True)
          (d_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (c_proj): Linear(in_features=512, out_features=128, bias=True)
          (c_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (s_proj): Linear(in_features=64, out_features=64, bias=True)
          (in_proj): Linear(in_features=512, out_features=512, bias=True)
          (inner_proj): Linear(in_features=16, out_features=16, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=8)
          (drop): Dropout(p=0.0, inplace=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): FSCls(
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (w_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (approx_out_proj): Linear(in_features=128, out_features=64, bias=True)
          (out_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): FSCls(
          (d_proj): Linear(in_features=512, out_features=512, bias=True)
          (d_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (c_proj): Linear(in_features=512, out_features=128, bias=True)
          (c_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (s_proj): Linear(in_features=64, out_features=64, bias=True)
          (in_proj): Linear(in_features=512, out_features=512, bias=True)
          (inner_proj): Linear(in_features=16, out_features=16, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=8)
          (drop): Dropout(p=0.0, inplace=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): FSCls(
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (w_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (approx_out_proj): Linear(in_features=128, out_features=64, bias=True)
          (out_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayer(
        (dropout_module): FairseqDropout()
        (self_attn): FSCls(
          (d_proj): Linear(in_features=512, out_features=512, bias=True)
          (d_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (c_proj): Linear(in_features=512, out_features=128, bias=True)
          (c_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (s_proj): Linear(in_features=64, out_features=64, bias=True)
          (in_proj): Linear(in_features=512, out_features=512, bias=True)
          (inner_proj): Linear(in_features=16, out_features=16, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (conv): Conv2d(8, 8, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0), groups=8)
          (drop): Dropout(p=0.0, inplace=False)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        (encoder_attn): FSCls(
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (k_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (v_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (w_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (approx_out_proj): Linear(in_features=128, out_features=64, bias=True)
          (out_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
        )
        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (output_projection): Linear(in_features=512, out_features=39960, bias=False)
    (embed_length): Embedding(256, 512)
  )
)
2023-01-11 19:02:13 | INFO | fairseq_cli.train | task: translation_lev (TranslationLevenshteinTask)
2023-01-11 19:02:13 | INFO | fairseq_cli.train | model: cmlm_transformer_wmt_en_de (CMLMNATransformerModel)
2023-01-11 19:02:13 | INFO | fairseq_cli.train | criterion: nat_loss (LabelSmoothedDualImitationCriterion)
2023-01-11 19:02:13 | INFO | fairseq_cli.train | num. model params: 70979584 (num. trained: 70979584)
2023-01-11 19:02:13 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.embed_tokens.weight
2023-01-11 19:02:13 | INFO | fairseq.trainer | detected shared parameter: encoder.embed_tokens.weight <- decoder.output_projection.weight
2023-01-11 19:02:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2023-01-11 19:02:13 | INFO | fairseq.utils | rank   0: capabilities =  8.0  ; total memory = 79.347 GB ; name = NVIDIA A100-SXM4-80GB                   
2023-01-11 19:02:13 | INFO | fairseq.utils | rank   1: capabilities =  8.0  ; total memory = 79.347 GB ; name = NVIDIA A100-SXM4-80GB                   
2023-01-11 19:02:13 | INFO | fairseq.utils | rank   2: capabilities =  8.0  ; total memory = 79.347 GB ; name = NVIDIA A100-SXM4-80GB                   
2023-01-11 19:02:13 | INFO | fairseq.utils | rank   3: capabilities =  8.0  ; total memory = 79.347 GB ; name = NVIDIA A100-SXM4-80GB                   
2023-01-11 19:02:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 4 workers***********************
2023-01-11 19:02:13 | INFO | fairseq_cli.train | training on 4 devices (GPUs/TPUs)
2023-01-11 19:02:13 | INFO | fairseq_cli.train | max tokens per GPU = 16384 and max sentences per GPU = None
2023-01-11 19:02:17 | INFO | fairseq.trainer | loaded checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint_last.pt (epoch 109 @ 213591 updates)
2023-01-11 19:02:17 | INFO | fairseq.trainer | loading train data for epoch 109
2023-01-11 19:02:18 | INFO | fairseq.data.data_utils | loaded 3961179 examples from: /mnt/petrelfs/jiangshuyang/data-bin/wmt14_deen_distill_jointdict/train.de-en.de
2023-01-11 19:02:18 | INFO | fairseq.data.data_utils | loaded 3961179 examples from: /mnt/petrelfs/jiangshuyang/data-bin/wmt14_deen_distill_jointdict/train.de-en.en
2023-01-11 19:02:18 | INFO | fairseq.tasks.translation | /mnt/petrelfs/jiangshuyang/data-bin/wmt14_deen_distill_jointdict train de-en 3961179 examples
2023-01-11 19:02:21 | INFO | fairseq.trainer | begin training epoch 109
2023-01-11 19:02:48 | INFO | train_inner | epoch 109:      9 / 1978 loss=3.065, nll_loss=0.922, word_ins=2.743, length=3.217, ppl=8.37, wps=38524.3, ups=0.65, wpb=59454, bsz=2043.4, num_updates=213600, lr=0.000216371, gnorm=1.579, loss_scale=2048, train_wall=120, wall=0
2023-01-11 19:04:47 | INFO | train_inner | epoch 109:    109 / 1978 loss=3.056, nll_loss=0.916, word_ins=2.737, length=3.185, ppl=8.32, wps=49711.8, ups=0.84, wpb=59346.3, bsz=1978.6, num_updates=213700, lr=0.000216321, gnorm=1.539, loss_scale=2048, train_wall=117, wall=0
2023-01-11 19:06:47 | INFO | train_inner | epoch 109:    209 / 1978 loss=3.026, nll_loss=0.896, word_ins=2.719, length=3.079, ppl=8.15, wps=49788.2, ups=0.84, wpb=59337.1, bsz=2091.8, num_updates=213800, lr=0.00021627, gnorm=1.529, loss_scale=2048, train_wall=119, wall=0
2023-01-11 19:08:45 | INFO | train_inner | epoch 109:    309 / 1978 loss=3.051, nll_loss=0.911, word_ins=2.732, length=3.184, ppl=8.29, wps=50638.6, ups=0.85, wpb=59842.2, bsz=2040.4, num_updates=213900, lr=0.000216219, gnorm=1.562, loss_scale=2048, train_wall=118, wall=0
2023-01-11 19:10:42 | INFO | train_inner | epoch 109:    409 / 1978 loss=3.086, nll_loss=0.939, word_ins=2.758, length=3.282, ppl=8.49, wps=50134.9, ups=0.85, wpb=58955.9, bsz=1917.7, num_updates=214000, lr=0.000216169, gnorm=1.617, loss_scale=2048, train_wall=117, wall=0
2023-01-11 19:12:41 | INFO | train_inner | epoch 109:    509 / 1978 loss=3.08, nll_loss=0.943, word_ins=2.762, length=3.18, ppl=8.45, wps=49899.7, ups=0.84, wpb=59163, bsz=1956.3, num_updates=214100, lr=0.000216118, gnorm=1.587, loss_scale=2048, train_wall=118, wall=0
2023-01-11 19:14:39 | INFO | train_inner | epoch 109:    609 / 1978 loss=3.034, nll_loss=0.899, word_ins=2.722, length=3.125, ppl=8.19, wps=50318.3, ups=0.84, wpb=59610.2, bsz=2029.4, num_updates=214200, lr=0.000216068, gnorm=1.537, loss_scale=2048, train_wall=118, wall=0
2023-01-11 19:16:37 | INFO | train_inner | epoch 109:    709 / 1978 loss=3.077, nll_loss=0.936, word_ins=2.755, length=3.212, ppl=8.44, wps=50285.4, ups=0.85, wpb=59063.4, bsz=1960.8, num_updates=214300, lr=0.000216017, gnorm=1.609, loss_scale=2048, train_wall=117, wall=0
2023-01-11 19:18:36 | INFO | train_inner | epoch 109:    809 / 1978 loss=3.043, nll_loss=0.905, word_ins=2.727, length=3.155, ppl=8.24, wps=49500.4, ups=0.84, wpb=58990.5, bsz=2075.8, num_updates=214400, lr=0.000215967, gnorm=1.513, loss_scale=2048, train_wall=119, wall=0
2023-01-11 19:20:35 | INFO | train_inner | epoch 109:    909 / 1978 loss=3.036, nll_loss=0.902, word_ins=2.724, length=3.126, ppl=8.2, wps=50384.5, ups=0.84, wpb=60059.8, bsz=2099.4, num_updates=214500, lr=0.000215917, gnorm=1.562, loss_scale=2048, train_wall=119, wall=0
2023-01-11 19:22:34 | INFO | train_inner | epoch 109:   1009 / 1978 loss=3.061, nll_loss=0.921, word_ins=2.741, length=3.196, ppl=8.35, wps=49774.2, ups=0.84, wpb=59050.9, bsz=1977.5, num_updates=214600, lr=0.000215866, gnorm=1.527, loss_scale=2048, train_wall=118, wall=0
2023-01-11 19:24:33 | INFO | train_inner | epoch 109:   1109 / 1978 loss=3.044, nll_loss=0.907, word_ins=2.729, length=3.153, ppl=8.25, wps=50134.1, ups=0.84, wpb=59531.1, bsz=2045.5, num_updates=214700, lr=0.000215816, gnorm=1.586, loss_scale=2048, train_wall=119, wall=0
2023-01-11 19:26:32 | INFO | train_inner | epoch 109:   1209 / 1978 loss=3.051, nll_loss=0.912, word_ins=2.733, length=3.179, ppl=8.29, wps=49705.3, ups=0.84, wpb=59386.2, bsz=2109, num_updates=214800, lr=0.000215766, gnorm=1.526, loss_scale=2048, train_wall=119, wall=0
2023-01-11 19:28:32 | INFO | train_inner | epoch 109:   1309 / 1978 loss=3.063, nll_loss=0.927, word_ins=2.747, length=3.163, ppl=8.36, wps=50330.1, ups=0.84, wpb=60075.8, bsz=2013.1, num_updates=214900, lr=0.000215716, gnorm=1.552, loss_scale=2048, train_wall=119, wall=0
2023-01-11 19:30:29 | INFO | train_inner | epoch 109:   1409 / 1978 loss=3.084, nll_loss=0.941, word_ins=2.759, length=3.241, ppl=8.48, wps=49984.3, ups=0.85, wpb=58617.4, bsz=1912.3, num_updates=215000, lr=0.000215666, gnorm=1.568, loss_scale=2048, train_wall=117, wall=0
2023-01-11 19:32:32 | INFO | train_inner | epoch 109:   1509 / 1978 loss=3.068, nll_loss=0.924, word_ins=2.744, length=3.24, ppl=8.39, wps=48236.4, ups=0.81, wpb=59566.6, bsz=1985.8, num_updates=215100, lr=0.000215615, gnorm=1.6, loss_scale=2048, train_wall=123, wall=0
2023-01-11 19:34:30 | INFO | train_inner | epoch 109:   1609 / 1978 loss=3.091, nll_loss=0.948, word_ins=2.766, length=3.241, ppl=8.52, wps=50169.9, ups=0.85, wpb=59082.3, bsz=1951, num_updates=215200, lr=0.000215565, gnorm=1.618, loss_scale=2048, train_wall=118, wall=0
2023-01-11 19:36:28 | INFO | train_inner | epoch 109:   1709 / 1978 loss=3.058, nll_loss=0.922, word_ins=2.742, length=3.166, ppl=8.33, wps=50308.5, ups=0.84, wpb=59545.7, bsz=2028.2, num_updates=215300, lr=0.000215515, gnorm=1.542, loss_scale=2048, train_wall=118, wall=0
2023-01-11 19:38:26 | INFO | train_inner | epoch 109:   1809 / 1978 loss=3.076, nll_loss=0.928, word_ins=2.747, length=3.29, ppl=8.43, wps=50039.7, ups=0.85, wpb=58756, bsz=1925.6, num_updates=215400, lr=0.000215465, gnorm=1.606, loss_scale=2048, train_wall=117, wall=0
2023-01-11 19:40:24 | INFO | train_inner | epoch 109:   1909 / 1978 loss=3.063, nll_loss=0.927, word_ins=2.746, length=3.171, ppl=8.36, wps=49628.3, ups=0.85, wpb=58684.4, bsz=2015.1, num_updates=215500, lr=0.000215415, gnorm=1.563, loss_scale=2048, train_wall=118, wall=0
2023-01-11 19:41:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-11 19:42:00 | INFO | valid | epoch 109 | valid on 'valid' subset | loss 4.449 | nll_loss 1.991 | word_ins 3.755 | length 6.937 | ppl 21.83 | wps 149835 | wpb 40242.5 | bsz 1500 | num_updates 215569 | best_loss 4.274
2023-01-11 19:42:00 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-11 19:42:03 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint109.pt (epoch 109 @ 215569 updates, score 4.449) (writing took 2.823762087151408 seconds)
2023-01-11 19:42:03 | INFO | fairseq_cli.train | end of epoch 109 (average epoch stats below)
2023-01-11 19:42:03 | INFO | train | epoch 109 | loss 3.062 | nll_loss 0.923 | word_ins 2.743 | length 3.193 | ppl 8.35 | wps 20243 | ups 0.34 | wpb 59286.9 | bsz 2002.2 | num_updates 215569 | lr 0.000215381 | gnorm 1.573 | loss_scale 2048 | train_wall 14382 | wall 0
2023-01-11 19:42:03 | INFO | fairseq.trainer | begin training epoch 110
2023-01-11 19:42:54 | INFO | train_inner | epoch 110:     31 / 1978 loss=3.08, nll_loss=0.931, word_ins=2.751, length=3.29, ppl=8.45, wps=39113.3, ups=0.67, wpb=58791.9, bsz=1925.2, num_updates=215600, lr=0.000215365, gnorm=1.544, loss_scale=2048, train_wall=117, wall=0
2023-01-11 19:44:52 | INFO | train_inner | epoch 110:    131 / 1978 loss=3.075, nll_loss=0.934, word_ins=2.753, length=3.213, ppl=8.43, wps=50674.1, ups=0.85, wpb=59577, bsz=1938.6, num_updates=215700, lr=0.000215315, gnorm=1.514, loss_scale=2048, train_wall=117, wall=0
2023-01-11 19:46:50 | INFO | train_inner | epoch 110:    231 / 1978 loss=3.059, nll_loss=0.918, word_ins=2.739, length=3.201, ppl=8.33, wps=49830.6, ups=0.85, wpb=58866.2, bsz=1959.9, num_updates=215800, lr=0.000215265, gnorm=1.537, loss_scale=2048, train_wall=118, wall=0
2023-01-11 19:49:06 | INFO | train_inner | epoch 110:    331 / 1978 loss=3.066, nll_loss=0.923, word_ins=2.744, length=3.219, ppl=8.37, wps=43637.4, ups=0.74, wpb=59336.7, bsz=1931.4, num_updates=215900, lr=0.000215216, gnorm=1.654, loss_scale=2048, train_wall=119, wall=0
2023-01-11 19:51:05 | INFO | train_inner | epoch 110:    431 / 1978 loss=3.041, nll_loss=0.902, word_ins=2.724, length=3.172, ppl=8.23, wps=50007.7, ups=0.84, wpb=59289.7, bsz=2024, num_updates=216000, lr=0.000215166, gnorm=1.594, loss_scale=2048, train_wall=118, wall=0
2023-01-11 19:53:03 | INFO | train_inner | epoch 110:    531 / 1978 loss=3.055, nll_loss=0.915, word_ins=2.736, length=3.185, ppl=8.31, wps=49963.3, ups=0.84, wpb=59243.8, bsz=2044.3, num_updates=216100, lr=0.000215116, gnorm=1.587, loss_scale=2048, train_wall=118, wall=0
2023-01-11 19:55:02 | INFO | train_inner | epoch 110:    631 / 1978 loss=3.048, nll_loss=0.91, word_ins=2.731, length=3.17, ppl=8.27, wps=49770.4, ups=0.84, wpb=59206, bsz=2043.9, num_updates=216200, lr=0.000215066, gnorm=1.538, loss_scale=2048, train_wall=119, wall=0
2023-01-11 19:57:03 | INFO | train_inner | epoch 110:    731 / 1978 loss=3.039, nll_loss=0.906, word_ins=2.728, length=3.116, ppl=8.22, wps=49129.6, ups=0.83, wpb=59533.3, bsz=2060, num_updates=216300, lr=0.000215016, gnorm=1.527, loss_scale=2048, train_wall=121, wall=0
2023-01-11 19:59:02 | INFO | train_inner | epoch 110:    831 / 1978 loss=3.056, nll_loss=0.918, word_ins=2.739, length=3.174, ppl=8.32, wps=50516.7, ups=0.84, wpb=60098, bsz=2020, num_updates=216400, lr=0.000214967, gnorm=1.64, loss_scale=2048, train_wall=119, wall=0
2023-01-11 20:01:00 | INFO | train_inner | epoch 110:    931 / 1978 loss=3.063, nll_loss=0.922, word_ins=2.742, length=3.213, ppl=8.36, wps=50301.6, ups=0.85, wpb=59050.8, bsz=1946.2, num_updates=216500, lr=0.000214917, gnorm=1.548, loss_scale=2048, train_wall=117, wall=0
2023-01-11 20:02:58 | INFO | train_inner | epoch 110:   1031 / 1978 loss=3.058, nll_loss=0.919, word_ins=2.739, length=3.187, ppl=8.33, wps=49853.3, ups=0.84, wpb=59020.8, bsz=2043.5, num_updates=216600, lr=0.000214868, gnorm=1.55, loss_scale=2048, train_wall=118, wall=0
2023-01-11 20:04:56 | INFO | train_inner | epoch 110:   1131 / 1978 loss=3.084, nll_loss=0.937, word_ins=2.756, length=3.274, ppl=8.48, wps=49933.2, ups=0.85, wpb=58628.4, bsz=1923.4, num_updates=216700, lr=0.000214818, gnorm=1.59, loss_scale=2048, train_wall=117, wall=0
2023-01-11 20:06:54 | INFO | train_inner | epoch 110:   1231 / 1978 loss=3.042, nll_loss=0.905, word_ins=2.727, length=3.148, ppl=8.23, wps=49975.9, ups=0.84, wpb=59260.9, bsz=2075.3, num_updates=216800, lr=0.000214768, gnorm=1.527, loss_scale=2048, train_wall=118, wall=0
2023-01-11 20:08:53 | INFO | train_inner | epoch 110:   1331 / 1978 loss=3.048, nll_loss=0.91, word_ins=2.731, length=3.17, ppl=8.27, wps=50158.2, ups=0.84, wpb=59614, bsz=2056.6, num_updates=216900, lr=0.000214719, gnorm=1.515, loss_scale=2048, train_wall=119, wall=0
2023-01-11 20:10:50 | INFO | train_inner | epoch 110:   1431 / 1978 loss=3.066, nll_loss=0.927, word_ins=2.747, length=3.191, ppl=8.37, wps=50553.1, ups=0.85, wpb=59350.8, bsz=1997.4, num_updates=217000, lr=0.000214669, gnorm=1.547, loss_scale=2048, train_wall=117, wall=0
2023-01-11 20:12:49 | INFO | train_inner | epoch 110:   1531 / 1978 loss=3.065, nll_loss=0.927, word_ins=2.747, length=3.187, ppl=8.37, wps=50076.2, ups=0.84, wpb=59266.3, bsz=1992.1, num_updates=217100, lr=0.00021462, gnorm=1.595, loss_scale=2048, train_wall=118, wall=0
2023-01-11 20:14:47 | INFO | train_inner | epoch 110:   1631 / 1978 loss=3.072, nll_loss=0.93, word_ins=2.749, length=3.229, ppl=8.41, wps=50037.9, ups=0.84, wpb=59218.9, bsz=2006.7, num_updates=217200, lr=0.000214571, gnorm=1.568, loss_scale=2048, train_wall=118, wall=0
2023-01-11 20:16:46 | INFO | train_inner | epoch 110:   1731 / 1978 loss=3.056, nll_loss=0.919, word_ins=2.739, length=3.172, ppl=8.32, wps=49970.2, ups=0.84, wpb=59307.1, bsz=1999.4, num_updates=217300, lr=0.000214521, gnorm=1.525, loss_scale=2048, train_wall=119, wall=0
2023-01-11 20:18:44 | INFO | train_inner | epoch 110:   1831 / 1978 loss=3.08, nll_loss=0.939, word_ins=2.758, length=3.22, ppl=8.45, wps=50225.7, ups=0.85, wpb=59218.7, bsz=1957.9, num_updates=217400, lr=0.000214472, gnorm=1.555, loss_scale=2048, train_wall=118, wall=0
2023-01-11 20:20:42 | INFO | train_inner | epoch 110:   1931 / 1978 loss=3.056, nll_loss=0.915, word_ins=2.736, length=3.201, ppl=8.32, wps=50084.6, ups=0.84, wpb=59492.1, bsz=2041.4, num_updates=217500, lr=0.000214423, gnorm=1.568, loss_scale=2048, train_wall=119, wall=0
2023-01-11 20:21:39 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-11 20:21:56 | INFO | valid | epoch 110 | valid on 'valid' subset | loss 4.422 | nll_loss 1.995 | word_ins 3.755 | length 6.675 | ppl 21.44 | wps 121856 | wpb 40242.5 | bsz 1500 | num_updates 217547 | best_loss 4.274
2023-01-11 20:21:56 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-11 20:22:00 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint110.pt (epoch 110 @ 217547 updates, score 4.422) (writing took 3.0597633169963956 seconds)
2023-01-11 20:22:00 | INFO | fairseq_cli.train | end of epoch 110 (average epoch stats below)
2023-01-11 20:22:00 | INFO | train | epoch 110 | loss 3.059 | nll_loss 0.92 | word_ins 2.74 | length 3.192 | ppl 8.34 | wps 48935.8 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 217547 | lr 0.000214399 | gnorm 1.56 | loss_scale 2048 | train_wall 2340 | wall 0
2023-01-11 20:22:00 | INFO | fairseq.trainer | begin training epoch 111
2023-01-11 20:23:18 | INFO | train_inner | epoch 111:     53 / 1978 loss=3.055, nll_loss=0.912, word_ins=2.733, length=3.218, ppl=8.31, wps=38092.4, ups=0.64, wpb=59388, bsz=1931.7, num_updates=217600, lr=0.000214373, gnorm=1.555, loss_scale=2048, train_wall=120, wall=0
2023-01-11 20:25:17 | INFO | train_inner | epoch 111:    153 / 1978 loss=3.034, nll_loss=0.898, word_ins=2.721, length=3.133, ppl=8.19, wps=50129.5, ups=0.84, wpb=59553.6, bsz=2022.2, num_updates=217700, lr=0.000214324, gnorm=1.532, loss_scale=4096, train_wall=119, wall=0
2023-01-11 20:27:16 | INFO | train_inner | epoch 111:    253 / 1978 loss=3.045, nll_loss=0.906, word_ins=2.729, length=3.162, ppl=8.25, wps=49509.9, ups=0.85, wpb=58582.1, bsz=2039.6, num_updates=217800, lr=0.000214275, gnorm=1.51, loss_scale=4096, train_wall=118, wall=0
2023-01-11 20:29:14 | INFO | train_inner | epoch 111:    353 / 1978 loss=3.059, nll_loss=0.92, word_ins=2.74, length=3.186, ppl=8.33, wps=50115.6, ups=0.84, wpb=59351.6, bsz=1972.3, num_updates=217900, lr=0.000214226, gnorm=1.565, loss_scale=4096, train_wall=118, wall=0
2023-01-11 20:31:13 | INFO | train_inner | epoch 111:    453 / 1978 loss=3.049, nll_loss=0.916, word_ins=2.737, length=3.114, ppl=8.27, wps=49644.1, ups=0.84, wpb=59215.1, bsz=2067.2, num_updates=218000, lr=0.000214176, gnorm=1.538, loss_scale=4096, train_wall=119, wall=0
2023-01-11 20:33:12 | INFO | train_inner | epoch 111:    553 / 1978 loss=3.037, nll_loss=0.9, word_ins=2.722, length=3.152, ppl=8.21, wps=50310.6, ups=0.84, wpb=59600.8, bsz=2027.2, num_updates=218100, lr=0.000214127, gnorm=1.517, loss_scale=4096, train_wall=118, wall=0
2023-01-11 20:35:10 | INFO | train_inner | epoch 111:    653 / 1978 loss=3.052, nll_loss=0.915, word_ins=2.737, length=3.152, ppl=8.29, wps=50248.8, ups=0.85, wpb=59428, bsz=2009.2, num_updates=218200, lr=0.000214078, gnorm=1.54, loss_scale=4096, train_wall=118, wall=0
2023-01-11 20:37:08 | INFO | train_inner | epoch 111:    753 / 1978 loss=3.083, nll_loss=0.937, word_ins=2.757, length=3.259, ppl=8.47, wps=49657.4, ups=0.84, wpb=58770, bsz=1929, num_updates=218300, lr=0.000214029, gnorm=1.572, loss_scale=4096, train_wall=118, wall=0
2023-01-11 20:39:07 | INFO | train_inner | epoch 111:    853 / 1978 loss=3.051, nll_loss=0.911, word_ins=2.733, length=3.18, ppl=8.29, wps=50057.3, ups=0.84, wpb=59430.7, bsz=1978.6, num_updates=218400, lr=0.00021398, gnorm=1.569, loss_scale=4096, train_wall=119, wall=0
2023-01-11 20:41:05 | INFO | train_inner | epoch 111:    953 / 1978 loss=3.058, nll_loss=0.918, word_ins=2.738, length=3.203, ppl=8.33, wps=50398.1, ups=0.84, wpb=59679.2, bsz=1953.1, num_updates=218500, lr=0.000213931, gnorm=1.591, loss_scale=4096, train_wall=118, wall=0
2023-01-11 20:43:04 | INFO | train_inner | epoch 111:   1053 / 1978 loss=3.05, nll_loss=0.914, word_ins=2.736, length=3.146, ppl=8.28, wps=49697.3, ups=0.84, wpb=59148.5, bsz=2094.5, num_updates=218600, lr=0.000213882, gnorm=1.534, loss_scale=4096, train_wall=119, wall=0
2023-01-11 20:45:02 | INFO | train_inner | epoch 111:   1153 / 1978 loss=3.068, nll_loss=0.923, word_ins=2.743, length=3.248, ppl=8.38, wps=50067.4, ups=0.85, wpb=59023.2, bsz=1974.3, num_updates=218700, lr=0.000213833, gnorm=1.581, loss_scale=4096, train_wall=118, wall=0
2023-01-11 20:47:01 | INFO | train_inner | epoch 111:   1253 / 1978 loss=3.057, nll_loss=0.918, word_ins=2.739, length=3.185, ppl=8.32, wps=49941.7, ups=0.84, wpb=59168.8, bsz=2026.4, num_updates=218800, lr=0.000213785, gnorm=1.521, loss_scale=4096, train_wall=118, wall=0
2023-01-11 20:48:59 | INFO | train_inner | epoch 111:   1353 / 1978 loss=3.057, nll_loss=0.919, word_ins=2.74, length=3.171, ppl=8.32, wps=49738.9, ups=0.84, wpb=58975.2, bsz=1997, num_updates=218900, lr=0.000213736, gnorm=1.54, loss_scale=4096, train_wall=118, wall=0
2023-01-11 20:50:58 | INFO | train_inner | epoch 111:   1453 / 1978 loss=3.059, nll_loss=0.923, word_ins=2.743, length=3.154, ppl=8.33, wps=50148.7, ups=0.84, wpb=59454, bsz=2021.1, num_updates=219000, lr=0.000213687, gnorm=1.558, loss_scale=4096, train_wall=118, wall=0
2023-01-11 20:52:56 | INFO | train_inner | epoch 111:   1553 / 1978 loss=3.063, nll_loss=0.924, word_ins=2.744, length=3.193, ppl=8.36, wps=50428.3, ups=0.85, wpb=59490.9, bsz=1970.9, num_updates=219100, lr=0.000213638, gnorm=1.521, loss_scale=4096, train_wall=118, wall=0
2023-01-11 20:54:53 | INFO | train_inner | epoch 111:   1653 / 1978 loss=3.068, nll_loss=0.923, word_ins=2.743, length=3.246, ppl=8.39, wps=50336.1, ups=0.85, wpb=59148.7, bsz=1991.4, num_updates=219200, lr=0.000213589, gnorm=1.548, loss_scale=4096, train_wall=117, wall=0
2023-01-11 20:56:52 | INFO | train_inner | epoch 111:   1753 / 1978 loss=3.083, nll_loss=0.94, word_ins=2.759, length=3.24, ppl=8.47, wps=49741.3, ups=0.85, wpb=58752.6, bsz=1938.8, num_updates=219300, lr=0.000213541, gnorm=1.566, loss_scale=4096, train_wall=118, wall=0
2023-01-11 20:58:50 | INFO | train_inner | epoch 111:   1853 / 1978 loss=3.047, nll_loss=0.912, word_ins=2.733, length=3.135, ppl=8.26, wps=50265, ups=0.84, wpb=59659.8, bsz=2045.5, num_updates=219400, lr=0.000213492, gnorm=1.539, loss_scale=4096, train_wall=118, wall=0
2023-01-11 21:00:49 | INFO | train_inner | epoch 111:   1953 / 1978 loss=3.06, nll_loss=0.919, word_ins=2.739, length=3.208, ppl=8.34, wps=50374.4, ups=0.84, wpb=59786.3, bsz=2025.6, num_updates=219500, lr=0.000213443, gnorm=1.574, loss_scale=4096, train_wall=118, wall=0
2023-01-11 21:01:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-11 21:01:38 | INFO | valid | epoch 111 | valid on 'valid' subset | loss 4.429 | nll_loss 1.99 | word_ins 3.747 | length 6.821 | ppl 21.54 | wps 120592 | wpb 40242.5 | bsz 1500 | num_updates 219525 | best_loss 4.274
2023-01-11 21:01:38 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-11 21:01:41 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint111.pt (epoch 111 @ 219525 updates, score 4.429) (writing took 2.9213696708902717 seconds)
2023-01-11 21:01:41 | INFO | fairseq_cli.train | end of epoch 111 (average epoch stats below)
2023-01-11 21:01:41 | INFO | train | epoch 111 | loss 3.056 | nll_loss 0.917 | word_ins 2.738 | length 3.182 | ppl 8.32 | wps 49244.3 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 219525 | lr 0.000213431 | gnorm 1.548 | loss_scale 4096 | train_wall 2340 | wall 0
2023-01-11 21:01:41 | INFO | fairseq.trainer | begin training epoch 112
2023-01-11 21:03:25 | INFO | train_inner | epoch 112:     75 / 1978 loss=3.024, nll_loss=0.892, word_ins=2.715, length=3.096, ppl=8.14, wps=38145, ups=0.64, wpb=59464.2, bsz=2154.3, num_updates=219600, lr=0.000213395, gnorm=1.524, loss_scale=4096, train_wall=120, wall=0
2023-01-11 21:04:49 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2023-01-11 21:05:25 | INFO | train_inner | epoch 112:    176 / 1978 loss=3.046, nll_loss=0.907, word_ins=2.729, length=3.169, ppl=8.26, wps=49428.5, ups=0.83, wpb=59551.2, bsz=2009.8, num_updates=219700, lr=0.000213346, gnorm=1.572, loss_scale=2048, train_wall=120, wall=0
2023-01-11 21:07:23 | INFO | train_inner | epoch 112:    276 / 1978 loss=3.049, nll_loss=0.911, word_ins=2.733, length=3.164, ppl=8.28, wps=50401.3, ups=0.85, wpb=59445.5, bsz=2010.4, num_updates=219800, lr=0.000213298, gnorm=1.51, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:09:21 | INFO | train_inner | epoch 112:    376 / 1978 loss=3.048, nll_loss=0.909, word_ins=2.73, length=3.188, ppl=8.27, wps=50358.8, ups=0.85, wpb=59249.2, bsz=1973.5, num_updates=219900, lr=0.000213249, gnorm=1.564, loss_scale=2048, train_wall=117, wall=0
2023-01-11 21:11:18 | INFO | train_inner | epoch 112:    476 / 1978 loss=3.072, nll_loss=0.929, word_ins=2.749, length=3.226, ppl=8.41, wps=50645.2, ups=0.85, wpb=59503.6, bsz=1892.2, num_updates=220000, lr=0.000213201, gnorm=1.532, loss_scale=2048, train_wall=117, wall=0
2023-01-11 21:13:17 | INFO | train_inner | epoch 112:    576 / 1978 loss=3.046, nll_loss=0.906, word_ins=2.728, length=3.177, ppl=8.26, wps=50380.7, ups=0.84, wpb=59865.7, bsz=2042.6, num_updates=220100, lr=0.000213152, gnorm=1.568, loss_scale=2048, train_wall=119, wall=0
2023-01-11 21:15:15 | INFO | train_inner | epoch 112:    676 / 1978 loss=3.066, nll_loss=0.924, word_ins=2.745, length=3.219, ppl=8.38, wps=49852.5, ups=0.85, wpb=58654, bsz=1954.8, num_updates=220200, lr=0.000213104, gnorm=1.504, loss_scale=2048, train_wall=117, wall=0
2023-01-11 21:17:13 | INFO | train_inner | epoch 112:    776 / 1978 loss=3.064, nll_loss=0.921, word_ins=2.741, length=3.225, ppl=8.36, wps=50017.4, ups=0.85, wpb=59142.1, bsz=1981.8, num_updates=220300, lr=0.000213056, gnorm=1.561, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:19:11 | INFO | train_inner | epoch 112:    876 / 1978 loss=3.058, nll_loss=0.916, word_ins=2.737, length=3.208, ppl=8.33, wps=50153, ups=0.85, wpb=59155.6, bsz=1968.1, num_updates=220400, lr=0.000213007, gnorm=1.524, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:21:10 | INFO | train_inner | epoch 112:    976 / 1978 loss=3.052, nll_loss=0.912, word_ins=2.733, length=3.182, ppl=8.29, wps=49362.8, ups=0.84, wpb=58881, bsz=2019.6, num_updates=220500, lr=0.000212959, gnorm=1.57, loss_scale=2048, train_wall=119, wall=0
2023-01-11 21:23:08 | INFO | train_inner | epoch 112:   1076 / 1978 loss=3.068, nll_loss=0.926, word_ins=2.745, length=3.224, ppl=8.39, wps=50568, ups=0.85, wpb=59401.7, bsz=1918.6, num_updates=220600, lr=0.000212911, gnorm=1.583, loss_scale=2048, train_wall=117, wall=0
2023-01-11 21:25:06 | INFO | train_inner | epoch 112:   1176 / 1978 loss=3.076, nll_loss=0.938, word_ins=2.757, length=3.192, ppl=8.43, wps=50207.5, ups=0.84, wpb=59485.9, bsz=1974.3, num_updates=220700, lr=0.000212862, gnorm=1.567, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:27:05 | INFO | train_inner | epoch 112:   1276 / 1978 loss=3.049, nll_loss=0.912, word_ins=2.733, length=3.154, ppl=8.27, wps=50374.7, ups=0.84, wpb=59915.1, bsz=2008.2, num_updates=220800, lr=0.000212814, gnorm=1.599, loss_scale=2048, train_wall=119, wall=0
2023-01-11 21:29:03 | INFO | train_inner | epoch 112:   1376 / 1978 loss=3.074, nll_loss=0.929, word_ins=2.749, length=3.247, ppl=8.42, wps=50004.4, ups=0.85, wpb=59067.5, bsz=1986.7, num_updates=220900, lr=0.000212766, gnorm=1.54, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:31:02 | INFO | train_inner | epoch 112:   1476 / 1978 loss=3.054, nll_loss=0.914, word_ins=2.735, length=3.186, ppl=8.31, wps=49626.6, ups=0.84, wpb=58754.4, bsz=1983.6, num_updates=221000, lr=0.000212718, gnorm=1.524, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:33:00 | INFO | train_inner | epoch 112:   1576 / 1978 loss=3.036, nll_loss=0.9, word_ins=2.722, length=3.143, ppl=8.2, wps=50050.2, ups=0.84, wpb=59354.1, bsz=2025.2, num_updates=221100, lr=0.00021267, gnorm=1.547, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:34:59 | INFO | train_inner | epoch 112:   1676 / 1978 loss=3.059, nll_loss=0.924, word_ins=2.743, length=3.154, ppl=8.33, wps=49691.2, ups=0.84, wpb=59171.6, bsz=2079.1, num_updates=221200, lr=0.000212622, gnorm=1.54, loss_scale=2048, train_wall=119, wall=0
2023-01-11 21:36:58 | INFO | train_inner | epoch 112:   1776 / 1978 loss=3.052, nll_loss=0.914, word_ins=2.735, length=3.17, ppl=8.29, wps=49941.8, ups=0.84, wpb=59188, bsz=1991.4, num_updates=221300, lr=0.000212574, gnorm=1.595, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:38:56 | INFO | train_inner | epoch 112:   1876 / 1978 loss=3.051, nll_loss=0.917, word_ins=2.738, length=3.13, ppl=8.29, wps=50262.5, ups=0.84, wpb=59567.8, bsz=2085.3, num_updates=221400, lr=0.000212526, gnorm=1.554, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:40:55 | INFO | train_inner | epoch 112:   1976 / 1978 loss=3.055, nll_loss=0.916, word_ins=2.737, length=3.185, ppl=8.31, wps=49975, ups=0.85, wpb=59140.1, bsz=2033.9, num_updates=221500, lr=0.000212478, gnorm=1.498, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:40:57 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-11 21:41:13 | INFO | valid | epoch 112 | valid on 'valid' subset | loss 4.442 | nll_loss 1.98 | word_ins 3.742 | length 6.999 | ppl 21.74 | wps 163406 | wpb 40242.5 | bsz 1500 | num_updates 221502 | best_loss 4.274
2023-01-11 21:41:13 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-11 21:41:15 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint112.pt (epoch 112 @ 221502 updates, score 4.442) (writing took 2.782825666014105 seconds)
2023-01-11 21:41:15 | INFO | fairseq_cli.train | end of epoch 112 (average epoch stats below)
2023-01-11 21:41:15 | INFO | train | epoch 112 | loss 3.056 | nll_loss 0.916 | word_ins 2.737 | length 3.185 | ppl 8.31 | wps 49362.5 | ups 0.83 | wpb 59288.3 | bsz 2002.4 | num_updates 221502 | lr 0.000212477 | gnorm 1.55 | loss_scale 2048 | train_wall 2338 | wall 0
2023-01-11 21:41:15 | INFO | fairseq.trainer | begin training epoch 113
2023-01-11 21:43:26 | INFO | train_inner | epoch 113:     98 / 1978 loss=3.056, nll_loss=0.917, word_ins=2.738, length=3.186, ppl=8.32, wps=38805.1, ups=0.66, wpb=58536.4, bsz=2034.6, num_updates=221600, lr=0.00021243, gnorm=1.55, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:45:24 | INFO | train_inner | epoch 113:    198 / 1978 loss=3.059, nll_loss=0.921, word_ins=2.741, length=3.174, ppl=8.33, wps=50572.1, ups=0.85, wpb=59681, bsz=1980.5, num_updates=221700, lr=0.000212382, gnorm=1.54, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:47:23 | INFO | train_inner | epoch 113:    298 / 1978 loss=3.032, nll_loss=0.896, word_ins=2.718, length=3.141, ppl=8.18, wps=49661.8, ups=0.84, wpb=59063.2, bsz=2089.5, num_updates=221800, lr=0.000212334, gnorm=1.499, loss_scale=2048, train_wall=119, wall=0
2023-01-11 21:49:22 | INFO | train_inner | epoch 113:    398 / 1978 loss=3.027, nll_loss=0.894, word_ins=2.717, length=3.099, ppl=8.15, wps=50000.7, ups=0.84, wpb=59709.3, bsz=2113.9, num_updates=221900, lr=0.000212286, gnorm=1.533, loss_scale=2048, train_wall=119, wall=0
2023-01-11 21:51:20 | INFO | train_inner | epoch 113:    498 / 1978 loss=3.052, nll_loss=0.909, word_ins=2.731, length=3.217, ppl=8.3, wps=49830.8, ups=0.85, wpb=58894.4, bsz=2003, num_updates=222000, lr=0.000212238, gnorm=1.553, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:53:18 | INFO | train_inner | epoch 113:    598 / 1978 loss=3.067, nll_loss=0.927, word_ins=2.747, length=3.202, ppl=8.38, wps=49938.5, ups=0.85, wpb=58938.3, bsz=1921.1, num_updates=222100, lr=0.00021219, gnorm=1.574, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:55:16 | INFO | train_inner | epoch 113:    698 / 1978 loss=3.065, nll_loss=0.922, word_ins=2.743, length=3.223, ppl=8.37, wps=50346.3, ups=0.85, wpb=59310.5, bsz=1930.2, num_updates=222200, lr=0.000212143, gnorm=1.57, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:57:14 | INFO | train_inner | epoch 113:    798 / 1978 loss=3.055, nll_loss=0.916, word_ins=2.737, length=3.183, ppl=8.31, wps=49978.1, ups=0.85, wpb=59048.7, bsz=1971.2, num_updates=222300, lr=0.000212095, gnorm=1.526, loss_scale=2048, train_wall=118, wall=0
2023-01-11 21:59:13 | INFO | train_inner | epoch 113:    898 / 1978 loss=3.064, nll_loss=0.924, word_ins=2.744, length=3.204, ppl=8.36, wps=50048.7, ups=0.84, wpb=59403.6, bsz=1966.7, num_updates=222400, lr=0.000212047, gnorm=1.555, loss_scale=2048, train_wall=118, wall=0
2023-01-11 22:01:10 | INFO | train_inner | epoch 113:    998 / 1978 loss=3.06, nll_loss=0.918, word_ins=2.738, length=3.22, ppl=8.34, wps=50513.9, ups=0.85, wpb=59374.9, bsz=1952, num_updates=222500, lr=0.000212, gnorm=1.564, loss_scale=2048, train_wall=117, wall=0
2023-01-11 22:03:09 | INFO | train_inner | epoch 113:   1098 / 1978 loss=3.048, nll_loss=0.913, word_ins=2.734, length=3.135, ppl=8.27, wps=50017.7, ups=0.84, wpb=59343.2, bsz=2065, num_updates=222600, lr=0.000211952, gnorm=1.516, loss_scale=2048, train_wall=118, wall=0
2023-01-11 22:05:07 | INFO | train_inner | epoch 113:   1198 / 1978 loss=3.083, nll_loss=0.939, word_ins=2.758, length=3.246, ppl=8.47, wps=50135.2, ups=0.85, wpb=59134.1, bsz=1881.6, num_updates=222700, lr=0.000211904, gnorm=1.606, loss_scale=2048, train_wall=118, wall=0
2023-01-11 22:07:06 | INFO | train_inner | epoch 113:   1298 / 1978 loss=3.058, nll_loss=0.921, word_ins=2.742, length=3.164, ppl=8.33, wps=49804.8, ups=0.84, wpb=59349.6, bsz=2038, num_updates=222800, lr=0.000211857, gnorm=1.532, loss_scale=2048, train_wall=119, wall=0
2023-01-11 22:09:03 | INFO | train_inner | epoch 113:   1398 / 1978 loss=3.074, nll_loss=0.928, word_ins=2.747, length=3.268, ppl=8.42, wps=50469.6, ups=0.85, wpb=59119.3, bsz=1924.3, num_updates=222900, lr=0.000211809, gnorm=1.517, loss_scale=2048, train_wall=117, wall=0
2023-01-11 22:11:02 | INFO | train_inner | epoch 113:   1498 / 1978 loss=3.033, nll_loss=0.899, word_ins=2.721, length=3.123, ppl=8.19, wps=50190.3, ups=0.84, wpb=59744.7, bsz=2051.8, num_updates=223000, lr=0.000211762, gnorm=1.53, loss_scale=2048, train_wall=119, wall=0
2023-01-11 22:13:00 | INFO | train_inner | epoch 113:   1598 / 1978 loss=3.056, nll_loss=0.915, word_ins=2.735, length=3.208, ppl=8.31, wps=50811.1, ups=0.85, wpb=59648.3, bsz=1981.8, num_updates=223100, lr=0.000211714, gnorm=1.578, loss_scale=2048, train_wall=117, wall=0
2023-01-11 22:15:03 | INFO | train_inner | epoch 113:   1698 / 1978 loss=3.047, nll_loss=0.909, word_ins=2.73, length=3.173, ppl=8.27, wps=47918.5, ups=0.81, wpb=58958.3, bsz=2050.6, num_updates=223200, lr=0.000211667, gnorm=1.492, loss_scale=2048, train_wall=123, wall=0
2023-01-11 22:17:02 | INFO | train_inner | epoch 113:   1798 / 1978 loss=3.071, nll_loss=0.931, word_ins=2.75, length=3.202, ppl=8.4, wps=49635.2, ups=0.84, wpb=59233.3, bsz=2038.7, num_updates=223300, lr=0.000211619, gnorm=1.576, loss_scale=2048, train_wall=119, wall=0
2023-01-11 22:19:00 | INFO | train_inner | epoch 113:   1898 / 1978 loss=3.048, nll_loss=0.905, word_ins=2.726, length=3.216, ppl=8.27, wps=50280.4, ups=0.85, wpb=59310.7, bsz=2022.5, num_updates=223400, lr=0.000211572, gnorm=1.559, loss_scale=2048, train_wall=118, wall=0
2023-01-11 22:20:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-11 22:20:56 | INFO | valid | epoch 113 | valid on 'valid' subset | loss 4.387 | nll_loss 1.986 | word_ins 3.747 | length 6.397 | ppl 20.92 | wps 124232 | wpb 40242.5 | bsz 1500 | num_updates 223480 | best_loss 4.274
2023-01-11 22:20:56 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-11 22:20:59 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint113.pt (epoch 113 @ 223480 updates, score 4.387) (writing took 2.823072333354503 seconds)
2023-01-11 22:20:59 | INFO | fairseq_cli.train | end of epoch 113 (average epoch stats below)
2023-01-11 22:20:59 | INFO | train | epoch 113 | loss 3.056 | nll_loss 0.916 | word_ins 2.737 | length 3.188 | ppl 8.31 | wps 49198.8 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 223480 | lr 0.000211534 | gnorm 1.548 | loss_scale 2048 | train_wall 2342 | wall 0
2023-01-11 22:20:59 | INFO | fairseq.trainer | begin training epoch 114
2023-01-11 22:21:38 | INFO | train_inner | epoch 114:     20 / 1978 loss=3.051, nll_loss=0.914, word_ins=2.735, length=3.16, ppl=8.29, wps=37981.6, ups=0.63, wpb=59826.9, bsz=2028.7, num_updates=223500, lr=0.000211525, gnorm=1.586, loss_scale=2048, train_wall=118, wall=0
2023-01-11 22:23:37 | INFO | train_inner | epoch 114:    120 / 1978 loss=3.041, nll_loss=0.906, word_ins=2.728, length=3.125, ppl=8.23, wps=50160, ups=0.84, wpb=59817.1, bsz=2019.2, num_updates=223600, lr=0.000211477, gnorm=1.567, loss_scale=2048, train_wall=119, wall=0
2023-01-11 22:25:36 | INFO | train_inner | epoch 114:    220 / 1978 loss=3.056, nll_loss=0.913, word_ins=2.734, length=3.221, ppl=8.32, wps=50086.2, ups=0.84, wpb=59576.8, bsz=1989.3, num_updates=223700, lr=0.00021143, gnorm=1.581, loss_scale=2048, train_wall=119, wall=0
2023-01-11 22:27:34 | INFO | train_inner | epoch 114:    320 / 1978 loss=3.069, nll_loss=0.925, word_ins=2.745, length=3.244, ppl=8.39, wps=50168.5, ups=0.84, wpb=59435.7, bsz=1950.9, num_updates=223800, lr=0.000211383, gnorm=1.609, loss_scale=4096, train_wall=118, wall=0
2023-01-11 22:29:32 | INFO | train_inner | epoch 114:    420 / 1978 loss=3.051, nll_loss=0.912, word_ins=2.733, length=3.182, ppl=8.29, wps=50661.6, ups=0.85, wpb=59589.9, bsz=1984.3, num_updates=223900, lr=0.000211336, gnorm=1.581, loss_scale=4096, train_wall=117, wall=0
2023-01-11 22:31:30 | INFO | train_inner | epoch 114:    520 / 1978 loss=3.047, nll_loss=0.906, word_ins=2.728, length=3.189, ppl=8.26, wps=49756.6, ups=0.85, wpb=58763.9, bsz=2024.6, num_updates=224000, lr=0.000211289, gnorm=1.51, loss_scale=4096, train_wall=118, wall=0
2023-01-11 22:33:28 | INFO | train_inner | epoch 114:    620 / 1978 loss=3.044, nll_loss=0.906, word_ins=2.727, length=3.166, ppl=8.25, wps=50062.4, ups=0.85, wpb=59220.6, bsz=2029.3, num_updates=224100, lr=0.000211241, gnorm=1.529, loss_scale=4096, train_wall=118, wall=0
2023-01-11 22:35:26 | INFO | train_inner | epoch 114:    720 / 1978 loss=3.08, nll_loss=0.933, word_ins=2.753, length=3.275, ppl=8.46, wps=50278.3, ups=0.85, wpb=58931.5, bsz=1913.4, num_updates=224200, lr=0.000211194, gnorm=1.552, loss_scale=4096, train_wall=117, wall=0
2023-01-11 22:37:24 | INFO | train_inner | epoch 114:    820 / 1978 loss=3.035, nll_loss=0.898, word_ins=2.722, length=3.138, ppl=8.2, wps=49657, ups=0.84, wpb=58961.5, bsz=2080.3, num_updates=224300, lr=0.000211147, gnorm=1.521, loss_scale=4096, train_wall=119, wall=0
2023-01-11 22:39:23 | INFO | train_inner | epoch 114:    920 / 1978 loss=3.052, nll_loss=0.915, word_ins=2.736, length=3.154, ppl=8.29, wps=50237.1, ups=0.84, wpb=59654.5, bsz=2042.1, num_updates=224400, lr=0.0002111, gnorm=1.512, loss_scale=4096, train_wall=119, wall=0
2023-01-11 22:41:22 | INFO | train_inner | epoch 114:   1020 / 1978 loss=3.047, nll_loss=0.908, word_ins=2.73, length=3.171, ppl=8.27, wps=49848.4, ups=0.84, wpb=59412.1, bsz=2051.5, num_updates=224500, lr=0.000211053, gnorm=1.548, loss_scale=4096, train_wall=119, wall=0
2023-01-11 22:43:21 | INFO | train_inner | epoch 114:   1120 / 1978 loss=3.052, nll_loss=0.913, word_ins=2.734, length=3.181, ppl=8.29, wps=50002.1, ups=0.84, wpb=59365.8, bsz=2015.5, num_updates=224600, lr=0.000211006, gnorm=1.537, loss_scale=4096, train_wall=119, wall=0
2023-01-11 22:45:19 | INFO | train_inner | epoch 114:   1220 / 1978 loss=3.04, nll_loss=0.901, word_ins=2.723, length=3.164, ppl=8.22, wps=50052.1, ups=0.84, wpb=59260.8, bsz=2031, num_updates=224700, lr=0.000210959, gnorm=1.53, loss_scale=4096, train_wall=118, wall=0
2023-01-11 22:47:17 | INFO | train_inner | epoch 114:   1320 / 1978 loss=3.058, nll_loss=0.918, word_ins=2.738, length=3.199, ppl=8.33, wps=50267.9, ups=0.85, wpb=59321.6, bsz=1970.2, num_updates=224800, lr=0.000210912, gnorm=1.508, loss_scale=4096, train_wall=118, wall=0
2023-01-11 22:49:15 | INFO | train_inner | epoch 114:   1420 / 1978 loss=3.04, nll_loss=0.907, word_ins=2.729, length=3.113, ppl=8.22, wps=50042.4, ups=0.85, wpb=59109.6, bsz=2037.4, num_updates=224900, lr=0.000210865, gnorm=1.515, loss_scale=4096, train_wall=118, wall=0
2023-01-11 22:51:14 | INFO | train_inner | epoch 114:   1520 / 1978 loss=3.066, nll_loss=0.923, word_ins=2.743, length=3.228, ppl=8.38, wps=50166.7, ups=0.85, wpb=59365.3, bsz=1938.4, num_updates=225000, lr=0.000210819, gnorm=1.55, loss_scale=4096, train_wall=118, wall=0
2023-01-11 22:53:12 | INFO | train_inner | epoch 114:   1620 / 1978 loss=3.086, nll_loss=0.943, word_ins=2.761, length=3.246, ppl=8.49, wps=49866, ups=0.85, wpb=58932.8, bsz=1925.7, num_updates=225100, lr=0.000210772, gnorm=1.628, loss_scale=4096, train_wall=118, wall=0
2023-01-11 22:55:10 | INFO | train_inner | epoch 114:   1720 / 1978 loss=3.058, nll_loss=0.92, word_ins=2.74, length=3.179, ppl=8.33, wps=50389, ups=0.85, wpb=59298.8, bsz=2000.6, num_updates=225200, lr=0.000210725, gnorm=1.536, loss_scale=4096, train_wall=117, wall=0
2023-01-11 22:57:09 | INFO | train_inner | epoch 114:   1820 / 1978 loss=3.049, nll_loss=0.914, word_ins=2.735, length=3.142, ppl=8.28, wps=49930.4, ups=0.84, wpb=59452.6, bsz=2080.3, num_updates=225300, lr=0.000210678, gnorm=1.503, loss_scale=4096, train_wall=119, wall=0
2023-01-11 22:59:07 | INFO | train_inner | epoch 114:   1920 / 1978 loss=3.072, nll_loss=0.93, word_ins=2.749, length=3.229, ppl=8.41, wps=49763.1, ups=0.85, wpb=58733.5, bsz=1929.6, num_updates=225400, lr=0.000210631, gnorm=1.506, loss_scale=4096, train_wall=118, wall=0
2023-01-11 23:00:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-11 23:00:32 | INFO | valid | epoch 114 | valid on 'valid' subset | loss 4.45 | nll_loss 2.003 | word_ins 3.755 | length 6.942 | ppl 21.85 | wps 165248 | wpb 40242.5 | bsz 1500 | num_updates 225458 | best_loss 4.274
2023-01-11 23:00:32 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-11 23:00:35 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint114.pt (epoch 114 @ 225458 updates, score 4.45) (writing took 2.813034236896783 seconds)
2023-01-11 23:00:35 | INFO | fairseq_cli.train | end of epoch 114 (average epoch stats below)
2023-01-11 23:00:35 | INFO | train | epoch 114 | loss 3.054 | nll_loss 0.915 | word_ins 2.735 | length 3.185 | ppl 8.3 | wps 49356.2 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 225458 | lr 0.000210604 | gnorm 1.544 | loss_scale 4096 | train_wall 2338 | wall 0
2023-01-11 23:00:35 | INFO | fairseq.trainer | begin training epoch 115
2023-01-11 23:01:39 | INFO | train_inner | epoch 115:     42 / 1978 loss=3.032, nll_loss=0.898, word_ins=2.72, length=3.121, ppl=8.18, wps=39121.8, ups=0.66, wpb=59554.6, bsz=2044.5, num_updates=225500, lr=0.000210585, gnorm=1.543, loss_scale=4096, train_wall=119, wall=0
2023-01-11 23:03:37 | INFO | train_inner | epoch 115:    142 / 1978 loss=3.07, nll_loss=0.924, word_ins=2.744, length=3.256, ppl=8.4, wps=49627.2, ups=0.85, wpb=58443.5, bsz=1969.6, num_updates=225600, lr=0.000210538, gnorm=1.56, loss_scale=4096, train_wall=118, wall=0
2023-01-11 23:05:35 | INFO | train_inner | epoch 115:    242 / 1978 loss=3.033, nll_loss=0.896, word_ins=2.718, length=3.15, ppl=8.19, wps=50349.8, ups=0.84, wpb=59647.4, bsz=2006.6, num_updates=225700, lr=0.000210491, gnorm=1.502, loss_scale=4096, train_wall=118, wall=0
2023-01-11 23:07:33 | INFO | train_inner | epoch 115:    342 / 1978 loss=3.051, nll_loss=0.91, word_ins=2.731, length=3.201, ppl=8.29, wps=50388.8, ups=0.85, wpb=59510.4, bsz=1927.7, num_updates=225800, lr=0.000210445, gnorm=1.615, loss_scale=4096, train_wall=118, wall=0
2023-01-11 23:08:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2023-01-11 23:09:33 | INFO | train_inner | epoch 115:    443 / 1978 loss=3.044, nll_loss=0.909, word_ins=2.73, length=3.142, ppl=8.25, wps=49838.2, ups=0.84, wpb=59427.9, bsz=1995.8, num_updates=225900, lr=0.000210398, gnorm=1.53, loss_scale=2048, train_wall=119, wall=0
2023-01-11 23:11:30 | INFO | train_inner | epoch 115:    543 / 1978 loss=3.055, nll_loss=0.918, word_ins=2.739, length=3.161, ppl=8.31, wps=51001.6, ups=0.85, wpb=60113.5, bsz=1977.7, num_updates=226000, lr=0.000210352, gnorm=1.535, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:13:28 | INFO | train_inner | epoch 115:    643 / 1978 loss=3.054, nll_loss=0.913, word_ins=2.734, length=3.202, ppl=8.3, wps=50100.1, ups=0.85, wpb=59061.6, bsz=1941.5, num_updates=226100, lr=0.000210305, gnorm=1.512, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:15:29 | INFO | train_inner | epoch 115:    743 / 1978 loss=3.033, nll_loss=0.898, word_ins=2.72, length=3.128, ppl=8.18, wps=49684.6, ups=0.83, wpb=59772.8, bsz=2063.4, num_updates=226200, lr=0.000210259, gnorm=1.586, loss_scale=2048, train_wall=120, wall=0
2023-01-11 23:17:26 | INFO | train_inner | epoch 115:    843 / 1978 loss=3.08, nll_loss=0.938, word_ins=2.757, length=3.23, ppl=8.45, wps=50209.1, ups=0.85, wpb=59009.4, bsz=1870.6, num_updates=226300, lr=0.000210212, gnorm=1.547, loss_scale=2048, train_wall=117, wall=0
2023-01-11 23:19:25 | INFO | train_inner | epoch 115:    943 / 1978 loss=3.035, nll_loss=0.9, word_ins=2.721, length=3.132, ppl=8.19, wps=50388.9, ups=0.84, wpb=59685.4, bsz=2058.2, num_updates=226400, lr=0.000210166, gnorm=1.524, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:21:24 | INFO | train_inner | epoch 115:   1043 / 1978 loss=3.034, nll_loss=0.9, word_ins=2.722, length=3.122, ppl=8.19, wps=49934.5, ups=0.84, wpb=59576.6, bsz=2097.4, num_updates=226500, lr=0.000210119, gnorm=1.532, loss_scale=2048, train_wall=119, wall=0
2023-01-11 23:23:22 | INFO | train_inner | epoch 115:   1143 / 1978 loss=3.063, nll_loss=0.926, word_ins=2.746, length=3.164, ppl=8.36, wps=49685.3, ups=0.84, wpb=58850.2, bsz=2056.2, num_updates=226600, lr=0.000210073, gnorm=1.517, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:25:21 | INFO | train_inner | epoch 115:   1243 / 1978 loss=3.043, nll_loss=0.904, word_ins=2.725, length=3.174, ppl=8.24, wps=50095, ups=0.84, wpb=59416.4, bsz=2032.2, num_updates=226700, lr=0.000210027, gnorm=1.571, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:27:19 | INFO | train_inner | epoch 115:   1343 / 1978 loss=3.069, nll_loss=0.923, word_ins=2.744, length=3.253, ppl=8.39, wps=49820.6, ups=0.85, wpb=58805.1, bsz=1941.3, num_updates=226800, lr=0.00020998, gnorm=1.559, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:29:17 | INFO | train_inner | epoch 115:   1443 / 1978 loss=3.057, nll_loss=0.92, word_ins=2.74, length=3.173, ppl=8.32, wps=50369.7, ups=0.85, wpb=59319, bsz=1966.5, num_updates=226900, lr=0.000209934, gnorm=1.537, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:31:15 | INFO | train_inner | epoch 115:   1543 / 1978 loss=3.054, nll_loss=0.914, word_ins=2.734, length=3.199, ppl=8.31, wps=50493.2, ups=0.85, wpb=59638.1, bsz=1996.1, num_updates=227000, lr=0.000209888, gnorm=1.538, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:33:14 | INFO | train_inner | epoch 115:   1643 / 1978 loss=3.04, nll_loss=0.905, word_ins=2.727, length=3.134, ppl=8.23, wps=49686.7, ups=0.84, wpb=59053.1, bsz=2115, num_updates=227100, lr=0.000209842, gnorm=1.494, loss_scale=2048, train_wall=119, wall=0
2023-01-11 23:35:11 | INFO | train_inner | epoch 115:   1743 / 1978 loss=3.072, nll_loss=0.931, word_ins=2.75, length=3.219, ppl=8.41, wps=49971.3, ups=0.85, wpb=58750, bsz=2003.2, num_updates=227200, lr=0.000209795, gnorm=1.526, loss_scale=2048, train_wall=117, wall=0
2023-01-11 23:37:10 | INFO | train_inner | epoch 115:   1843 / 1978 loss=3.05, nll_loss=0.909, word_ins=2.73, length=3.208, ppl=8.28, wps=49916, ups=0.84, wpb=59220.1, bsz=2012.6, num_updates=227300, lr=0.000209749, gnorm=1.49, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:39:08 | INFO | train_inner | epoch 115:   1943 / 1978 loss=3.057, nll_loss=0.921, word_ins=2.74, length=3.173, ppl=8.33, wps=50189, ups=0.85, wpb=59146.2, bsz=1992.4, num_updates=227400, lr=0.000209703, gnorm=1.525, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:39:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-11 23:40:06 | INFO | valid | epoch 115 | valid on 'valid' subset | loss 4.417 | nll_loss 1.99 | word_ins 3.75 | length 6.674 | ppl 21.37 | wps 198498 | wpb 40242.5 | bsz 1500 | num_updates 227435 | best_loss 4.274
2023-01-11 23:40:06 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-11 23:40:09 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint115.pt (epoch 115 @ 227435 updates, score 4.417) (writing took 2.962855346966535 seconds)
2023-01-11 23:40:09 | INFO | fairseq_cli.train | end of epoch 115 (average epoch stats below)
2023-01-11 23:40:09 | INFO | train | epoch 115 | loss 3.052 | nll_loss 0.913 | word_ins 2.734 | length 3.176 | ppl 8.29 | wps 49368.9 | ups 0.83 | wpb 59286.7 | bsz 2001.8 | num_updates 227435 | lr 0.000209687 | gnorm 1.535 | loss_scale 2048 | train_wall 2336 | wall 0
2023-01-11 23:40:09 | INFO | fairseq.trainer | begin training epoch 116
2023-01-11 23:41:42 | INFO | train_inner | epoch 116:     65 / 1978 loss=3.047, nll_loss=0.913, word_ins=2.734, length=3.134, ppl=8.27, wps=38356.6, ups=0.65, wpb=59160.4, bsz=1986.4, num_updates=227500, lr=0.000209657, gnorm=1.489, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:43:40 | INFO | train_inner | epoch 116:    165 / 1978 loss=3.021, nll_loss=0.888, word_ins=2.711, length=3.104, ppl=8.12, wps=50696.7, ups=0.84, wpb=60011, bsz=2022.6, num_updates=227600, lr=0.000209611, gnorm=1.497, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:45:39 | INFO | train_inner | epoch 116:    265 / 1978 loss=3.041, nll_loss=0.901, word_ins=2.724, length=3.172, ppl=8.23, wps=49883.1, ups=0.85, wpb=58979.1, bsz=1984.2, num_updates=227700, lr=0.000209565, gnorm=1.556, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:47:37 | INFO | train_inner | epoch 116:    365 / 1978 loss=3.062, nll_loss=0.924, word_ins=2.743, length=3.189, ppl=8.35, wps=49315.1, ups=0.84, wpb=58477.1, bsz=2006.4, num_updates=227800, lr=0.000209519, gnorm=1.458, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:49:36 | INFO | train_inner | epoch 116:    465 / 1978 loss=3.035, nll_loss=0.903, word_ins=2.725, length=3.101, ppl=8.2, wps=49646.8, ups=0.84, wpb=59174, bsz=2093, num_updates=227900, lr=0.000209473, gnorm=1.49, loss_scale=2048, train_wall=119, wall=0
2023-01-11 23:51:35 | INFO | train_inner | epoch 116:    565 / 1978 loss=3.053, nll_loss=0.915, word_ins=2.735, length=3.175, ppl=8.3, wps=50326, ups=0.84, wpb=59592, bsz=2006.8, num_updates=228000, lr=0.000209427, gnorm=1.498, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:53:33 | INFO | train_inner | epoch 116:    665 / 1978 loss=3.074, nll_loss=0.93, word_ins=2.75, length=3.235, ppl=8.42, wps=50162.4, ups=0.85, wpb=59146, bsz=1932.1, num_updates=228100, lr=0.000209381, gnorm=1.563, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:55:31 | INFO | train_inner | epoch 116:    765 / 1978 loss=3.031, nll_loss=0.893, word_ins=2.715, length=3.159, ppl=8.18, wps=49936.8, ups=0.85, wpb=59035, bsz=2088.4, num_updates=228200, lr=0.000209335, gnorm=1.472, loss_scale=2048, train_wall=118, wall=0
2023-01-11 23:57:30 | INFO | train_inner | epoch 116:    865 / 1978 loss=3.047, nll_loss=0.913, word_ins=2.733, length=3.137, ppl=8.26, wps=49992.4, ups=0.84, wpb=59357.4, bsz=2028.6, num_updates=228300, lr=0.000209289, gnorm=1.492, loss_scale=2048, train_wall=119, wall=0
2023-01-11 23:59:28 | INFO | train_inner | epoch 116:    965 / 1978 loss=3.068, nll_loss=0.926, word_ins=2.745, length=3.23, ppl=8.39, wps=50146.4, ups=0.85, wpb=59097.9, bsz=1928.4, num_updates=228400, lr=0.000209243, gnorm=1.595, loss_scale=2048, train_wall=118, wall=0
2023-01-12 00:01:27 | INFO | train_inner | epoch 116:   1065 / 1978 loss=3.026, nll_loss=0.891, word_ins=2.713, length=3.131, ppl=8.15, wps=49951.7, ups=0.84, wpb=59531.6, bsz=2019.4, num_updates=228500, lr=0.000209198, gnorm=1.475, loss_scale=2048, train_wall=119, wall=0
2023-01-12 00:03:25 | INFO | train_inner | epoch 116:   1165 / 1978 loss=3.033, nll_loss=0.896, word_ins=2.719, length=3.142, ppl=8.18, wps=50144.6, ups=0.84, wpb=59470.3, bsz=2100.8, num_updates=228600, lr=0.000209152, gnorm=1.503, loss_scale=2048, train_wall=118, wall=0
2023-01-12 00:05:24 | INFO | train_inner | epoch 116:   1265 / 1978 loss=3.057, nll_loss=0.917, word_ins=2.738, length=3.194, ppl=8.32, wps=50185.2, ups=0.84, wpb=59667.7, bsz=1994.5, num_updates=228700, lr=0.000209106, gnorm=1.589, loss_scale=2048, train_wall=119, wall=0
2023-01-12 00:07:23 | INFO | train_inner | epoch 116:   1365 / 1978 loss=3.053, nll_loss=0.918, word_ins=2.738, length=3.145, ppl=8.3, wps=50107.4, ups=0.85, wpb=59277.6, bsz=1998.5, num_updates=228800, lr=0.000209061, gnorm=1.552, loss_scale=2048, train_wall=118, wall=0
2023-01-12 00:09:21 | INFO | train_inner | epoch 116:   1465 / 1978 loss=3.046, nll_loss=0.908, word_ins=2.729, length=3.175, ppl=8.26, wps=49769.6, ups=0.84, wpb=59098.4, bsz=2010, num_updates=228900, lr=0.000209015, gnorm=1.495, loss_scale=2048, train_wall=119, wall=0
2023-01-12 00:11:19 | INFO | train_inner | epoch 116:   1565 / 1978 loss=3.065, nll_loss=0.922, word_ins=2.742, length=3.233, ppl=8.37, wps=50508.9, ups=0.85, wpb=59561.5, bsz=1912.6, num_updates=229000, lr=0.000208969, gnorm=1.588, loss_scale=2048, train_wall=118, wall=0
2023-01-12 00:13:18 | INFO | train_inner | epoch 116:   1665 / 1978 loss=3.047, nll_loss=0.913, word_ins=2.734, length=3.131, ppl=8.26, wps=49788.8, ups=0.84, wpb=59070, bsz=2021.9, num_updates=229100, lr=0.000208924, gnorm=1.504, loss_scale=2048, train_wall=118, wall=0
2023-01-12 00:15:16 | INFO | train_inner | epoch 116:   1765 / 1978 loss=3.083, nll_loss=0.937, word_ins=2.755, length=3.272, ppl=8.47, wps=50021.9, ups=0.85, wpb=59096.9, bsz=1930.7, num_updates=229200, lr=0.000208878, gnorm=1.598, loss_scale=2048, train_wall=118, wall=0
2023-01-12 00:17:15 | INFO | train_inner | epoch 116:   1865 / 1978 loss=3.046, nll_loss=0.906, word_ins=2.727, length=3.194, ppl=8.26, wps=50048.1, ups=0.84, wpb=59545.7, bsz=2005.8, num_updates=229300, lr=0.000208832, gnorm=1.57, loss_scale=2048, train_wall=119, wall=0
2023-01-12 00:19:13 | INFO | train_inner | epoch 116:   1965 / 1978 loss=3.058, nll_loss=0.92, word_ins=2.74, length=3.187, ppl=8.33, wps=50178.5, ups=0.85, wpb=59251.6, bsz=1975.8, num_updates=229400, lr=0.000208787, gnorm=1.503, loss_scale=2048, train_wall=118, wall=0
2023-01-12 00:19:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 00:19:46 | INFO | valid | epoch 116 | valid on 'valid' subset | loss 4.443 | nll_loss 2.005 | word_ins 3.761 | length 6.82 | ppl 21.75 | wps 96662.5 | wpb 40242.5 | bsz 1500 | num_updates 229413 | best_loss 4.274
2023-01-12 00:19:46 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 00:19:49 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint116.pt (epoch 116 @ 229413 updates, score 4.443) (writing took 3.0031868149526417 seconds)
2023-01-12 00:19:49 | INFO | fairseq_cli.train | end of epoch 116 (average epoch stats below)
2023-01-12 00:19:49 | INFO | train | epoch 116 | loss 3.05 | nll_loss 0.912 | word_ins 2.732 | length 3.173 | ppl 8.28 | wps 49266.2 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 229413 | lr 0.000208781 | gnorm 1.526 | loss_scale 2048 | train_wall 2340 | wall 0
2023-01-12 00:19:49 | INFO | fairseq.trainer | begin training epoch 117
2023-01-12 00:21:45 | INFO | train_inner | epoch 117:     87 / 1978 loss=3.044, nll_loss=0.908, word_ins=2.729, length=3.147, ppl=8.25, wps=38932.4, ups=0.66, wpb=59235.6, bsz=1993.4, num_updates=229500, lr=0.000208741, gnorm=1.546, loss_scale=2048, train_wall=118, wall=0
2023-01-12 00:23:44 | INFO | train_inner | epoch 117:    187 / 1978 loss=3.054, nll_loss=0.911, word_ins=2.732, length=3.219, ppl=8.3, wps=50195.5, ups=0.84, wpb=59520.5, bsz=1994, num_updates=229600, lr=0.000208696, gnorm=1.584, loss_scale=2048, train_wall=118, wall=0
2023-01-12 00:25:43 | INFO | train_inner | epoch 117:    287 / 1978 loss=3.054, nll_loss=0.916, word_ins=2.737, length=3.176, ppl=8.31, wps=49871, ups=0.84, wpb=59261, bsz=1979.8, num_updates=229700, lr=0.000208651, gnorm=1.588, loss_scale=2048, train_wall=119, wall=0
2023-01-12 00:27:41 | INFO | train_inner | epoch 117:    387 / 1978 loss=3.05, nll_loss=0.912, word_ins=2.733, length=3.171, ppl=8.28, wps=50051.7, ups=0.84, wpb=59357.9, bsz=2007.8, num_updates=229800, lr=0.000208605, gnorm=1.515, loss_scale=2048, train_wall=118, wall=0
2023-01-12 00:29:39 | INFO | train_inner | epoch 117:    487 / 1978 loss=3.05, nll_loss=0.909, word_ins=2.731, length=3.199, ppl=8.28, wps=50208.1, ups=0.85, wpb=59056.6, bsz=1970.7, num_updates=229900, lr=0.00020856, gnorm=1.499, loss_scale=2048, train_wall=117, wall=0
2023-01-12 00:31:36 | INFO | train_inner | epoch 117:    587 / 1978 loss=3.062, nll_loss=0.919, word_ins=2.739, length=3.226, ppl=8.35, wps=50384.4, ups=0.85, wpb=59255.2, bsz=1911, num_updates=230000, lr=0.000208514, gnorm=1.562, loss_scale=4096, train_wall=117, wall=0
2023-01-12 00:33:34 | INFO | train_inner | epoch 117:    687 / 1978 loss=3.059, nll_loss=0.918, word_ins=2.739, length=3.201, ppl=8.33, wps=50319.3, ups=0.85, wpb=59158, bsz=1965.2, num_updates=230100, lr=0.000208469, gnorm=1.563, loss_scale=4096, train_wall=117, wall=0
2023-01-12 00:35:32 | INFO | train_inner | epoch 117:    787 / 1978 loss=3.046, nll_loss=0.907, word_ins=2.728, length=3.181, ppl=8.26, wps=50057.2, ups=0.85, wpb=58994, bsz=1982.7, num_updates=230200, lr=0.000208424, gnorm=1.466, loss_scale=4096, train_wall=118, wall=0
2023-01-12 00:37:31 | INFO | train_inner | epoch 117:    887 / 1978 loss=3.031, nll_loss=0.893, word_ins=2.716, length=3.156, ppl=8.18, wps=50242.8, ups=0.84, wpb=59874.9, bsz=2085.2, num_updates=230300, lr=0.000208379, gnorm=1.479, loss_scale=4096, train_wall=119, wall=0
2023-01-12 00:39:30 | INFO | train_inner | epoch 117:    987 / 1978 loss=3.027, nll_loss=0.895, word_ins=2.718, length=3.092, ppl=8.15, wps=50080.4, ups=0.84, wpb=59423, bsz=2091.9, num_updates=230400, lr=0.000208333, gnorm=1.499, loss_scale=4096, train_wall=118, wall=0
2023-01-12 00:41:29 | INFO | train_inner | epoch 117:   1087 / 1978 loss=3.025, nll_loss=0.894, word_ins=2.716, length=3.088, ppl=8.14, wps=49756.5, ups=0.84, wpb=59149, bsz=2109.3, num_updates=230500, lr=0.000208288, gnorm=1.512, loss_scale=4096, train_wall=119, wall=0
2023-01-12 00:43:27 | INFO | train_inner | epoch 117:   1187 / 1978 loss=3.063, nll_loss=0.922, word_ins=2.742, length=3.213, ppl=8.36, wps=49890.6, ups=0.84, wpb=59271.6, bsz=2021.8, num_updates=230600, lr=0.000208243, gnorm=1.504, loss_scale=4096, train_wall=119, wall=0
2023-01-12 00:45:25 | INFO | train_inner | epoch 117:   1287 / 1978 loss=3.052, nll_loss=0.915, word_ins=2.737, length=3.153, ppl=8.29, wps=49840, ups=0.85, wpb=58751.2, bsz=1999.8, num_updates=230700, lr=0.000208198, gnorm=1.523, loss_scale=4096, train_wall=118, wall=0
2023-01-12 00:47:23 | INFO | train_inner | epoch 117:   1387 / 1978 loss=3.06, nll_loss=0.924, word_ins=2.744, length=3.164, ppl=8.34, wps=50178.3, ups=0.85, wpb=59105.3, bsz=1930.6, num_updates=230800, lr=0.000208153, gnorm=1.477, loss_scale=4096, train_wall=118, wall=0
2023-01-12 00:49:22 | INFO | train_inner | epoch 117:   1487 / 1978 loss=3.05, nll_loss=0.911, word_ins=2.732, length=3.176, ppl=8.28, wps=49653.8, ups=0.84, wpb=58901.3, bsz=2027.8, num_updates=230900, lr=0.000208108, gnorm=1.538, loss_scale=4096, train_wall=118, wall=0
2023-01-12 00:51:20 | INFO | train_inner | epoch 117:   1587 / 1978 loss=3.054, nll_loss=0.915, word_ins=2.736, length=3.185, ppl=8.31, wps=50374, ups=0.84, wpb=59635.6, bsz=1960.7, num_updates=231000, lr=0.000208063, gnorm=1.605, loss_scale=4096, train_wall=118, wall=0
2023-01-12 00:53:18 | INFO | train_inner | epoch 117:   1687 / 1978 loss=3.026, nll_loss=0.893, word_ins=2.716, length=3.103, ppl=8.15, wps=50339.2, ups=0.85, wpb=59350.9, bsz=2063.1, num_updates=231100, lr=0.000208018, gnorm=1.468, loss_scale=4096, train_wall=118, wall=0
2023-01-12 00:55:16 | INFO | train_inner | epoch 117:   1787 / 1978 loss=3.074, nll_loss=0.928, word_ins=2.748, length=3.263, ppl=8.42, wps=49902.8, ups=0.84, wpb=59091.4, bsz=1936.4, num_updates=231200, lr=0.000207973, gnorm=1.522, loss_scale=4096, train_wall=118, wall=0
2023-01-12 00:57:14 | INFO | train_inner | epoch 117:   1887 / 1978 loss=3.055, nll_loss=0.916, word_ins=2.737, length=3.186, ppl=8.31, wps=50165.2, ups=0.85, wpb=59272.1, bsz=1961.8, num_updates=231300, lr=0.000207928, gnorm=1.534, loss_scale=4096, train_wall=118, wall=0
2023-01-12 00:59:03 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 00:59:20 | INFO | valid | epoch 117 | valid on 'valid' subset | loss 4.477 | nll_loss 2.007 | word_ins 3.767 | length 7.096 | ppl 22.27 | wps 95850.2 | wpb 40242.5 | bsz 1500 | num_updates 231391 | best_loss 4.274
2023-01-12 00:59:20 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 00:59:23 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint117.pt (epoch 117 @ 231391 updates, score 4.477) (writing took 2.844740048982203 seconds)
2023-01-12 00:59:23 | INFO | fairseq_cli.train | end of epoch 117 (average epoch stats below)
2023-01-12 00:59:23 | INFO | train | epoch 117 | loss 3.048 | nll_loss 0.91 | word_ins 2.731 | length 3.17 | ppl 8.27 | wps 49404 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 231391 | lr 0.000207887 | gnorm 1.525 | loss_scale 4096 | train_wall 2336 | wall 0
2023-01-12 00:59:23 | INFO | fairseq.trainer | begin training epoch 118
2023-01-12 00:59:47 | INFO | train_inner | epoch 118:      9 / 1978 loss=3.026, nll_loss=0.893, word_ins=2.715, length=3.113, ppl=8.15, wps=39072.7, ups=0.65, wpb=59770.6, bsz=2042.2, num_updates=231400, lr=0.000207883, gnorm=1.522, loss_scale=4096, train_wall=119, wall=0
2023-01-12 01:01:46 | INFO | train_inner | epoch 118:    109 / 1978 loss=3.046, nll_loss=0.906, word_ins=2.729, length=3.173, ppl=8.26, wps=49509.6, ups=0.84, wpb=58880.3, bsz=2020.6, num_updates=231500, lr=0.000207838, gnorm=1.53, loss_scale=4096, train_wall=119, wall=0
2023-01-12 01:03:44 | INFO | train_inner | epoch 118:    209 / 1978 loss=3.079, nll_loss=0.938, word_ins=2.757, length=3.224, ppl=8.45, wps=50189.6, ups=0.85, wpb=59112.7, bsz=1874.7, num_updates=231600, lr=0.000207793, gnorm=1.534, loss_scale=4096, train_wall=118, wall=0
2023-01-12 01:05:42 | INFO | train_inner | epoch 118:    309 / 1978 loss=3.044, nll_loss=0.903, word_ins=2.725, length=3.182, ppl=8.25, wps=50247.4, ups=0.85, wpb=59454.1, bsz=1963, num_updates=231700, lr=0.000207748, gnorm=1.582, loss_scale=4096, train_wall=118, wall=0
2023-01-12 01:07:42 | INFO | train_inner | epoch 118:    409 / 1978 loss=3.016, nll_loss=0.882, word_ins=2.706, length=3.098, ppl=8.09, wps=49978.5, ups=0.84, wpb=59749.8, bsz=2104.2, num_updates=231800, lr=0.000207703, gnorm=1.445, loss_scale=4096, train_wall=119, wall=0
2023-01-12 01:09:41 | INFO | train_inner | epoch 118:    509 / 1978 loss=3.056, nll_loss=0.916, word_ins=2.737, length=3.188, ppl=8.32, wps=50405.2, ups=0.84, wpb=59716.8, bsz=1967, num_updates=231900, lr=0.000207658, gnorm=1.497, loss_scale=4096, train_wall=118, wall=0
2023-01-12 01:11:39 | INFO | train_inner | epoch 118:    609 / 1978 loss=3.041, nll_loss=0.901, word_ins=2.723, length=3.182, ppl=8.23, wps=49929.7, ups=0.85, wpb=59040.6, bsz=2042.4, num_updates=232000, lr=0.000207614, gnorm=1.476, loss_scale=4096, train_wall=118, wall=0
2023-01-12 01:13:36 | INFO | train_inner | epoch 118:    709 / 1978 loss=3.057, nll_loss=0.92, word_ins=2.741, length=3.165, ppl=8.32, wps=50227.5, ups=0.85, wpb=58845.2, bsz=1909.4, num_updates=232100, lr=0.000207569, gnorm=1.507, loss_scale=4096, train_wall=117, wall=0
2023-01-12 01:15:35 | INFO | train_inner | epoch 118:    809 / 1978 loss=3.035, nll_loss=0.903, word_ins=2.725, length=3.094, ppl=8.19, wps=49851.6, ups=0.84, wpb=59458, bsz=2039.5, num_updates=232200, lr=0.000207524, gnorm=1.511, loss_scale=4096, train_wall=119, wall=0
2023-01-12 01:17:34 | INFO | train_inner | epoch 118:    909 / 1978 loss=3.066, nll_loss=0.923, word_ins=2.743, length=3.227, ppl=8.37, wps=50062.9, ups=0.84, wpb=59314.4, bsz=1951.4, num_updates=232300, lr=0.00020748, gnorm=1.532, loss_scale=4096, train_wall=118, wall=0
2023-01-12 01:19:33 | INFO | train_inner | epoch 118:   1009 / 1978 loss=3.025, nll_loss=0.892, word_ins=2.714, length=3.106, ppl=8.14, wps=49956.4, ups=0.84, wpb=59393.7, bsz=2090.2, num_updates=232400, lr=0.000207435, gnorm=1.5, loss_scale=4096, train_wall=119, wall=0
2023-01-12 01:21:32 | INFO | train_inner | epoch 118:   1109 / 1978 loss=3.043, nll_loss=0.906, word_ins=2.727, length=3.16, ppl=8.24, wps=49969.1, ups=0.84, wpb=59487.6, bsz=2012.3, num_updates=232500, lr=0.00020739, gnorm=1.545, loss_scale=4096, train_wall=119, wall=0
2023-01-12 01:23:29 | INFO | train_inner | epoch 118:   1209 / 1978 loss=3.053, nll_loss=0.913, word_ins=2.734, length=3.189, ppl=8.3, wps=50072.9, ups=0.85, wpb=58779.1, bsz=1919.9, num_updates=232600, lr=0.000207346, gnorm=1.5, loss_scale=4096, train_wall=117, wall=0
2023-01-12 01:25:27 | INFO | train_inner | epoch 118:   1309 / 1978 loss=3.034, nll_loss=0.896, word_ins=2.718, length=3.156, ppl=8.19, wps=50604.5, ups=0.84, wpb=59963.9, bsz=2035.9, num_updates=232700, lr=0.000207301, gnorm=1.564, loss_scale=4096, train_wall=118, wall=0
2023-01-12 01:27:27 | INFO | train_inner | epoch 118:   1409 / 1978 loss=3.046, nll_loss=0.908, word_ins=2.728, length=3.182, ppl=8.26, wps=49886.9, ups=0.84, wpb=59510.2, bsz=2061.4, num_updates=232800, lr=0.000207257, gnorm=1.542, loss_scale=4096, train_wall=119, wall=0
2023-01-12 01:29:26 | INFO | train_inner | epoch 118:   1509 / 1978 loss=3.038, nll_loss=0.904, word_ins=2.725, length=3.124, ppl=8.21, wps=50331.4, ups=0.84, wpb=59771.6, bsz=2018.3, num_updates=232900, lr=0.000207212, gnorm=1.414, loss_scale=4096, train_wall=119, wall=0
2023-01-12 01:31:23 | INFO | train_inner | epoch 118:   1609 / 1978 loss=3.07, nll_loss=0.927, word_ins=2.746, length=3.24, ppl=8.4, wps=50217, ups=0.85, wpb=58919.4, bsz=1944.5, num_updates=233000, lr=0.000207168, gnorm=1.536, loss_scale=4096, train_wall=117, wall=0
2023-01-12 01:33:22 | INFO | train_inner | epoch 118:   1709 / 1978 loss=3.05, nll_loss=0.909, word_ins=2.73, length=3.202, ppl=8.28, wps=50253.3, ups=0.84, wpb=59735.6, bsz=2001.4, num_updates=233100, lr=0.000207123, gnorm=1.547, loss_scale=4096, train_wall=119, wall=0
2023-01-12 01:35:21 | INFO | train_inner | epoch 118:   1809 / 1978 loss=3.03, nll_loss=0.895, word_ins=2.717, length=3.125, ppl=8.17, wps=49915.9, ups=0.84, wpb=59359.4, bsz=2088.6, num_updates=233200, lr=0.000207079, gnorm=1.488, loss_scale=4096, train_wall=119, wall=0
2023-01-12 01:37:19 | INFO | train_inner | epoch 118:   1909 / 1978 loss=3.033, nll_loss=0.894, word_ins=2.717, length=3.156, ppl=8.18, wps=49582.9, ups=0.85, wpb=58590.5, bsz=2014.1, num_updates=233300, lr=0.000207034, gnorm=1.459, loss_scale=4096, train_wall=118, wall=0
2023-01-12 01:38:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 01:38:58 | INFO | valid | epoch 118 | valid on 'valid' subset | loss 4.444 | nll_loss 1.99 | word_ins 3.749 | length 6.947 | ppl 21.77 | wps 151984 | wpb 40242.5 | bsz 1500 | num_updates 233369 | best_loss 4.274
2023-01-12 01:38:58 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 01:39:01 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint118.pt (epoch 118 @ 233369 updates, score 4.444) (writing took 2.8409896292723715 seconds)
2023-01-12 01:39:01 | INFO | fairseq_cli.train | end of epoch 118 (average epoch stats below)
2023-01-12 01:39:01 | INFO | train | epoch 118 | loss 3.046 | nll_loss 0.908 | word_ins 2.729 | length 3.169 | ppl 8.26 | wps 49301 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 233369 | lr 0.000207004 | gnorm 1.512 | loss_scale 4096 | train_wall 2339 | wall 0
2023-01-12 01:39:01 | INFO | fairseq.trainer | begin training epoch 119
2023-01-12 01:39:52 | INFO | train_inner | epoch 119:     31 / 1978 loss=3.049, nll_loss=0.912, word_ins=2.732, length=3.167, ppl=8.28, wps=38240.4, ups=0.65, wpb=58674.6, bsz=2023.5, num_updates=233400, lr=0.00020699, gnorm=1.489, loss_scale=4096, train_wall=118, wall=0
2023-01-12 01:41:50 | INFO | train_inner | epoch 119:    131 / 1978 loss=3.043, nll_loss=0.905, word_ins=2.727, length=3.161, ppl=8.24, wps=50322.6, ups=0.85, wpb=59330.4, bsz=1963.3, num_updates=233500, lr=0.000206946, gnorm=1.496, loss_scale=4096, train_wall=118, wall=0
2023-01-12 01:43:49 | INFO | train_inner | epoch 119:    231 / 1978 loss=3.037, nll_loss=0.897, word_ins=2.719, length=3.177, ppl=8.21, wps=49933.8, ups=0.84, wpb=59277, bsz=1973.1, num_updates=233600, lr=0.000206901, gnorm=1.539, loss_scale=4096, train_wall=119, wall=0
2023-01-12 01:45:48 | INFO | train_inner | epoch 119:    331 / 1978 loss=3.03, nll_loss=0.891, word_ins=2.714, length=3.16, ppl=8.17, wps=50040, ups=0.84, wpb=59564.5, bsz=2046.7, num_updates=233700, lr=0.000206857, gnorm=1.497, loss_scale=4096, train_wall=119, wall=0
2023-01-12 01:47:46 | INFO | train_inner | epoch 119:    431 / 1978 loss=3.045, nll_loss=0.908, word_ins=2.73, length=3.146, ppl=8.25, wps=49984.1, ups=0.85, wpb=59088, bsz=2022.6, num_updates=233800, lr=0.000206813, gnorm=1.516, loss_scale=4096, train_wall=118, wall=0
2023-01-12 01:49:44 | INFO | train_inner | epoch 119:    531 / 1978 loss=3.042, nll_loss=0.903, word_ins=2.725, length=3.174, ppl=8.24, wps=50052.4, ups=0.85, wpb=59121.1, bsz=1942.3, num_updates=233900, lr=0.000206769, gnorm=1.528, loss_scale=4096, train_wall=118, wall=0
2023-01-12 01:51:43 | INFO | train_inner | epoch 119:    631 / 1978 loss=3.057, nll_loss=0.917, word_ins=2.737, length=3.2, ppl=8.32, wps=49685, ups=0.84, wpb=58933.7, bsz=2009.3, num_updates=234000, lr=0.000206725, gnorm=1.578, loss_scale=4096, train_wall=118, wall=0
2023-01-12 01:53:41 | INFO | train_inner | epoch 119:    731 / 1978 loss=3.041, nll_loss=0.9, word_ins=2.722, length=3.189, ppl=8.23, wps=50507.5, ups=0.85, wpb=59577, bsz=1965.2, num_updates=234100, lr=0.00020668, gnorm=1.545, loss_scale=8192, train_wall=118, wall=0
2023-01-12 01:55:39 | INFO | train_inner | epoch 119:    831 / 1978 loss=3.04, nll_loss=0.907, word_ins=2.729, length=3.109, ppl=8.22, wps=50113.9, ups=0.84, wpb=59418.5, bsz=1979.4, num_updates=234200, lr=0.000206636, gnorm=1.504, loss_scale=8192, train_wall=118, wall=0
2023-01-12 01:57:38 | INFO | train_inner | epoch 119:    931 / 1978 loss=3.02, nll_loss=0.887, word_ins=2.71, length=3.096, ppl=8.11, wps=50102.2, ups=0.84, wpb=59498.9, bsz=2083.7, num_updates=234300, lr=0.000206592, gnorm=1.503, loss_scale=8192, train_wall=119, wall=0
2023-01-12 01:59:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2023-01-12 01:59:38 | INFO | train_inner | epoch 119:   1032 / 1978 loss=3.05, nll_loss=0.914, word_ins=2.735, length=3.154, ppl=8.28, wps=49477.3, ups=0.84, wpb=59161, bsz=2002.2, num_updates=234400, lr=0.000206548, gnorm=1.441, loss_scale=4096, train_wall=119, wall=0
2023-01-12 02:01:37 | INFO | train_inner | epoch 119:   1132 / 1978 loss=3.059, nll_loss=0.922, word_ins=2.742, length=3.169, ppl=8.33, wps=49656.6, ups=0.84, wpb=59025.1, bsz=1981.2, num_updates=234500, lr=0.000206504, gnorm=1.531, loss_scale=4096, train_wall=119, wall=0
2023-01-12 02:03:35 | INFO | train_inner | epoch 119:   1232 / 1978 loss=3.029, nll_loss=0.893, word_ins=2.716, length=3.132, ppl=8.16, wps=50249.5, ups=0.84, wpb=59579.6, bsz=2082, num_updates=234600, lr=0.00020646, gnorm=1.45, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:05:32 | INFO | train_inner | epoch 119:   1332 / 1978 loss=3.064, nll_loss=0.921, word_ins=2.741, length=3.228, ppl=8.36, wps=50226.1, ups=0.85, wpb=58927.9, bsz=1975.9, num_updates=234700, lr=0.000206416, gnorm=1.495, loss_scale=4096, train_wall=117, wall=0
2023-01-12 02:07:31 | INFO | train_inner | epoch 119:   1432 / 1978 loss=3.031, nll_loss=0.901, word_ins=2.722, length=3.088, ppl=8.17, wps=50470.9, ups=0.84, wpb=59854.1, bsz=2061.1, num_updates=234800, lr=0.000206372, gnorm=1.516, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:09:30 | INFO | train_inner | epoch 119:   1532 / 1978 loss=3.043, nll_loss=0.902, word_ins=2.724, length=3.195, ppl=8.24, wps=49931.3, ups=0.84, wpb=59390.7, bsz=2031.1, num_updates=234900, lr=0.000206328, gnorm=1.526, loss_scale=4096, train_wall=119, wall=0
2023-01-12 02:11:28 | INFO | train_inner | epoch 119:   1632 / 1978 loss=3.061, nll_loss=0.923, word_ins=2.742, length=3.189, ppl=8.35, wps=50159, ups=0.85, wpb=59333.1, bsz=2006.6, num_updates=235000, lr=0.000206284, gnorm=1.55, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:13:27 | INFO | train_inner | epoch 119:   1732 / 1978 loss=3.055, nll_loss=0.918, word_ins=2.739, length=3.161, ppl=8.31, wps=50043.1, ups=0.85, wpb=59185, bsz=2015.6, num_updates=235100, lr=0.00020624, gnorm=1.492, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:15:25 | INFO | train_inner | epoch 119:   1832 / 1978 loss=3.062, nll_loss=0.923, word_ins=2.742, length=3.201, ppl=8.35, wps=50650.1, ups=0.85, wpb=59763.1, bsz=1929.6, num_updates=235200, lr=0.000206197, gnorm=1.569, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:17:23 | INFO | train_inner | epoch 119:   1932 / 1978 loss=3.055, nll_loss=0.919, word_ins=2.739, length=3.162, ppl=8.31, wps=49896, ups=0.85, wpb=58864, bsz=2020.8, num_updates=235300, lr=0.000206153, gnorm=1.536, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:18:17 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 02:18:34 | INFO | valid | epoch 119 | valid on 'valid' subset | loss 4.41 | nll_loss 1.972 | word_ins 3.732 | length 6.772 | ppl 21.26 | wps 178892 | wpb 40242.5 | bsz 1500 | num_updates 235346 | best_loss 4.274
2023-01-12 02:18:34 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 02:18:37 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint119.pt (epoch 119 @ 235346 updates, score 4.41) (writing took 2.8096904763951898 seconds)
2023-01-12 02:18:37 | INFO | fairseq_cli.train | end of epoch 119 (average epoch stats below)
2023-01-12 02:18:37 | INFO | train | epoch 119 | loss 3.045 | nll_loss 0.908 | word_ins 2.729 | length 3.162 | ppl 8.26 | wps 49343.9 | ups 0.83 | wpb 59286.7 | bsz 2003 | num_updates 235346 | lr 0.000206133 | gnorm 1.515 | loss_scale 4096 | train_wall 2338 | wall 0
2023-01-12 02:18:37 | INFO | fairseq.trainer | begin training epoch 120
2023-01-12 02:19:52 | INFO | train_inner | epoch 120:     54 / 1978 loss=3.061, nll_loss=0.923, word_ins=2.743, length=3.173, ppl=8.34, wps=39402.8, ups=0.67, wpb=59049.7, bsz=1883.9, num_updates=235400, lr=0.000206109, gnorm=1.513, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:21:52 | INFO | train_inner | epoch 120:    154 / 1978 loss=3.019, nll_loss=0.883, word_ins=2.707, length=3.114, ppl=8.1, wps=50024.3, ups=0.84, wpb=59695.2, bsz=2088.3, num_updates=235500, lr=0.000206065, gnorm=1.533, loss_scale=4096, train_wall=119, wall=0
2023-01-12 02:23:51 | INFO | train_inner | epoch 120:    254 / 1978 loss=3.019, nll_loss=0.887, word_ins=2.71, length=3.082, ppl=8.1, wps=50123.1, ups=0.84, wpb=59523, bsz=2043, num_updates=235600, lr=0.000206021, gnorm=1.496, loss_scale=4096, train_wall=119, wall=0
2023-01-12 02:25:49 | INFO | train_inner | epoch 120:    354 / 1978 loss=3.034, nll_loss=0.898, word_ins=2.721, length=3.135, ppl=8.19, wps=50314.4, ups=0.85, wpb=59464.8, bsz=2006.2, num_updates=235700, lr=0.000205978, gnorm=1.482, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:27:46 | INFO | train_inner | epoch 120:    454 / 1978 loss=3.072, nll_loss=0.93, word_ins=2.749, length=3.222, ppl=8.41, wps=50308.2, ups=0.85, wpb=58959.4, bsz=1939, num_updates=235800, lr=0.000205934, gnorm=1.482, loss_scale=4096, train_wall=117, wall=0
2023-01-12 02:29:45 | INFO | train_inner | epoch 120:    554 / 1978 loss=3.052, nll_loss=0.91, word_ins=2.731, length=3.204, ppl=8.29, wps=49551.2, ups=0.84, wpb=58921.7, bsz=2022.9, num_updates=235900, lr=0.00020589, gnorm=1.498, loss_scale=4096, train_wall=119, wall=0
2023-01-12 02:31:43 | INFO | train_inner | epoch 120:    654 / 1978 loss=3.052, nll_loss=0.914, word_ins=2.735, length=3.173, ppl=8.29, wps=50323, ups=0.85, wpb=59498.5, bsz=1960.2, num_updates=236000, lr=0.000205847, gnorm=1.567, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:33:42 | INFO | train_inner | epoch 120:    754 / 1978 loss=3.023, nll_loss=0.887, word_ins=2.71, length=3.128, ppl=8.13, wps=49751.6, ups=0.84, wpb=59421.6, bsz=2072.2, num_updates=236100, lr=0.000205803, gnorm=1.496, loss_scale=4096, train_wall=119, wall=0
2023-01-12 02:35:40 | INFO | train_inner | epoch 120:    854 / 1978 loss=3.041, nll_loss=0.902, word_ins=2.723, length=3.174, ppl=8.23, wps=50250.3, ups=0.85, wpb=59149.6, bsz=2001.4, num_updates=236200, lr=0.00020576, gnorm=1.507, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:37:38 | INFO | train_inner | epoch 120:    954 / 1978 loss=3.071, nll_loss=0.927, word_ins=2.747, length=3.236, ppl=8.4, wps=50110.1, ups=0.85, wpb=58799.9, bsz=1929.1, num_updates=236300, lr=0.000205716, gnorm=1.503, loss_scale=4096, train_wall=117, wall=0
2023-01-12 02:39:36 | INFO | train_inner | epoch 120:   1054 / 1978 loss=3.066, nll_loss=0.93, word_ins=2.749, length=3.171, ppl=8.38, wps=50037.2, ups=0.85, wpb=59052, bsz=1927.7, num_updates=236400, lr=0.000205673, gnorm=1.543, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:41:34 | INFO | train_inner | epoch 120:   1154 / 1978 loss=3.037, nll_loss=0.9, word_ins=2.722, length=3.149, ppl=8.21, wps=49824, ups=0.85, wpb=58955.8, bsz=2010.9, num_updates=236500, lr=0.000205629, gnorm=1.532, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:43:33 | INFO | train_inner | epoch 120:   1254 / 1978 loss=3.052, nll_loss=0.913, word_ins=2.734, length=3.181, ppl=8.29, wps=49927.9, ups=0.84, wpb=59323.5, bsz=2039.1, num_updates=236600, lr=0.000205586, gnorm=1.507, loss_scale=4096, train_wall=119, wall=0
2023-01-12 02:45:31 | INFO | train_inner | epoch 120:   1354 / 1978 loss=3.06, nll_loss=0.919, word_ins=2.739, length=3.203, ppl=8.34, wps=49968.6, ups=0.85, wpb=58970, bsz=1917.4, num_updates=236700, lr=0.000205542, gnorm=1.562, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:47:29 | INFO | train_inner | epoch 120:   1454 / 1978 loss=3.053, nll_loss=0.912, word_ins=2.733, length=3.2, ppl=8.3, wps=49978.3, ups=0.84, wpb=59193, bsz=1965.3, num_updates=236800, lr=0.000205499, gnorm=1.568, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:49:27 | INFO | train_inner | epoch 120:   1554 / 1978 loss=3.027, nll_loss=0.893, word_ins=2.716, length=3.11, ppl=8.15, wps=50284.7, ups=0.85, wpb=59402.6, bsz=2061.1, num_updates=236900, lr=0.000205455, gnorm=1.496, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:51:25 | INFO | train_inner | epoch 120:   1654 / 1978 loss=3.075, nll_loss=0.934, word_ins=2.753, length=3.226, ppl=8.43, wps=50100, ups=0.85, wpb=59217.3, bsz=1935.7, num_updates=237000, lr=0.000205412, gnorm=1.511, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:53:24 | INFO | train_inner | epoch 120:   1754 / 1978 loss=3.04, nll_loss=0.901, word_ins=2.722, length=3.185, ppl=8.23, wps=50486.3, ups=0.84, wpb=59820.2, bsz=2023.4, num_updates=237100, lr=0.000205369, gnorm=1.497, loss_scale=4096, train_wall=118, wall=0
2023-01-12 02:55:23 | INFO | train_inner | epoch 120:   1854 / 1978 loss=3.019, nll_loss=0.885, word_ins=2.708, length=3.107, ppl=8.11, wps=50058.5, ups=0.84, wpb=59565.8, bsz=2136.6, num_updates=237200, lr=0.000205325, gnorm=1.486, loss_scale=4096, train_wall=119, wall=0
2023-01-12 02:57:22 | INFO | train_inner | epoch 120:   1954 / 1978 loss=3.035, nll_loss=0.901, word_ins=2.722, length=3.124, ppl=8.2, wps=50130.2, ups=0.84, wpb=59629.9, bsz=2031.9, num_updates=237300, lr=0.000205282, gnorm=1.492, loss_scale=4096, train_wall=119, wall=0
2023-01-12 02:57:50 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 02:58:11 | INFO | valid | epoch 120 | valid on 'valid' subset | loss 4.442 | nll_loss 2.004 | word_ins 3.762 | length 6.81 | ppl 21.74 | wps 139388 | wpb 40242.5 | bsz 1500 | num_updates 237324 | best_loss 4.274
2023-01-12 02:58:11 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 02:58:14 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint120.pt (epoch 120 @ 237324 updates, score 4.442) (writing took 2.813276111148298 seconds)
2023-01-12 02:58:14 | INFO | fairseq_cli.train | end of epoch 120 (average epoch stats below)
2023-01-12 02:58:14 | INFO | train | epoch 120 | loss 3.045 | nll_loss 0.907 | word_ins 2.728 | length 3.164 | ppl 8.25 | wps 49332.1 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 237324 | lr 0.000205272 | gnorm 1.514 | loss_scale 4096 | train_wall 2338 | wall 0
2023-01-12 02:58:14 | INFO | fairseq.trainer | begin training epoch 121
2023-01-12 03:00:00 | INFO | train_inner | epoch 121:     76 / 1978 loss=3.061, nll_loss=0.92, word_ins=2.74, length=3.207, ppl=8.35, wps=37243.1, ups=0.63, wpb=58801.7, bsz=1938.6, num_updates=237400, lr=0.000205239, gnorm=1.562, loss_scale=4096, train_wall=118, wall=0
2023-01-12 03:01:58 | INFO | train_inner | epoch 121:    176 / 1978 loss=3.031, nll_loss=0.895, word_ins=2.718, length=3.137, ppl=8.18, wps=50119.4, ups=0.84, wpb=59419.2, bsz=2015.3, num_updates=237500, lr=0.000205196, gnorm=1.501, loss_scale=4096, train_wall=118, wall=0
2023-01-12 03:03:57 | INFO | train_inner | epoch 121:    276 / 1978 loss=3.048, nll_loss=0.906, word_ins=2.727, length=3.211, ppl=8.27, wps=50182.1, ups=0.84, wpb=59545, bsz=2001.1, num_updates=237600, lr=0.000205152, gnorm=1.537, loss_scale=4096, train_wall=118, wall=0
2023-01-12 03:05:55 | INFO | train_inner | epoch 121:    376 / 1978 loss=3.048, nll_loss=0.907, word_ins=2.728, length=3.198, ppl=8.27, wps=50145.2, ups=0.85, wpb=58992.5, bsz=1953.6, num_updates=237700, lr=0.000205109, gnorm=1.561, loss_scale=4096, train_wall=117, wall=0
2023-01-12 03:07:53 | INFO | train_inner | epoch 121:    476 / 1978 loss=3.039, nll_loss=0.9, word_ins=2.722, length=3.167, ppl=8.22, wps=49763, ups=0.85, wpb=58690.7, bsz=1945.7, num_updates=237800, lr=0.000205066, gnorm=1.499, loss_scale=4096, train_wall=118, wall=0
2023-01-12 03:09:51 | INFO | train_inner | epoch 121:    576 / 1978 loss=3.027, nll_loss=0.894, word_ins=2.716, length=3.109, ppl=8.15, wps=50374.4, ups=0.84, wpb=59630, bsz=2094.2, num_updates=237900, lr=0.000205023, gnorm=1.432, loss_scale=4096, train_wall=118, wall=0
2023-01-12 03:11:49 | INFO | train_inner | epoch 121:    676 / 1978 loss=3.044, nll_loss=0.906, word_ins=2.727, length=3.169, ppl=8.25, wps=50499, ups=0.85, wpb=59556.6, bsz=1934.1, num_updates=238000, lr=0.00020498, gnorm=1.522, loss_scale=4096, train_wall=118, wall=0
2023-01-12 03:13:47 | INFO | train_inner | epoch 121:    776 / 1978 loss=3.033, nll_loss=0.896, word_ins=2.718, length=3.146, ppl=8.18, wps=49688.4, ups=0.85, wpb=58775.6, bsz=2012.5, num_updates=238100, lr=0.000204937, gnorm=1.478, loss_scale=4096, train_wall=118, wall=0
2023-01-12 03:15:46 | INFO | train_inner | epoch 121:    876 / 1978 loss=3.035, nll_loss=0.899, word_ins=2.721, length=3.135, ppl=8.2, wps=50400.4, ups=0.84, wpb=59821.5, bsz=2037.6, num_updates=238200, lr=0.000204894, gnorm=1.507, loss_scale=4096, train_wall=119, wall=0
2023-01-12 03:17:44 | INFO | train_inner | epoch 121:    976 / 1978 loss=3.064, nll_loss=0.925, word_ins=2.745, length=3.193, ppl=8.36, wps=49966.3, ups=0.85, wpb=58925.9, bsz=1888.9, num_updates=238300, lr=0.000204851, gnorm=1.564, loss_scale=4096, train_wall=118, wall=0
2023-01-12 03:19:42 | INFO | train_inner | epoch 121:   1076 / 1978 loss=3.052, nll_loss=0.913, word_ins=2.734, length=3.186, ppl=8.3, wps=50017.6, ups=0.85, wpb=59191.5, bsz=1972, num_updates=238400, lr=0.000204808, gnorm=1.519, loss_scale=4096, train_wall=118, wall=0
2023-01-12 03:21:41 | INFO | train_inner | epoch 121:   1176 / 1978 loss=3.046, nll_loss=0.906, word_ins=2.728, length=3.188, ppl=8.26, wps=49780.3, ups=0.84, wpb=59297.1, bsz=2060.7, num_updates=238500, lr=0.000204765, gnorm=1.494, loss_scale=8192, train_wall=119, wall=0
2023-01-12 03:23:39 | INFO | train_inner | epoch 121:   1276 / 1978 loss=3.055, nll_loss=0.916, word_ins=2.737, length=3.183, ppl=8.31, wps=49892.2, ups=0.85, wpb=58831.5, bsz=1964.7, num_updates=238600, lr=0.000204722, gnorm=1.503, loss_scale=8192, train_wall=118, wall=0
2023-01-12 03:25:39 | INFO | train_inner | epoch 121:   1376 / 1978 loss=3.024, nll_loss=0.893, word_ins=2.716, length=3.079, ppl=8.13, wps=49692.5, ups=0.84, wpb=59323.5, bsz=2107.6, num_updates=238700, lr=0.000204679, gnorm=1.528, loss_scale=8192, train_wall=119, wall=0
2023-01-12 03:27:37 | INFO | train_inner | epoch 121:   1476 / 1978 loss=3.059, nll_loss=0.918, word_ins=2.738, length=3.206, ppl=8.33, wps=50242.4, ups=0.84, wpb=59591.6, bsz=1945, num_updates=238800, lr=0.000204636, gnorm=1.494, loss_scale=8192, train_wall=118, wall=0
2023-01-12 03:29:37 | INFO | train_inner | epoch 121:   1576 / 1978 loss=3.012, nll_loss=0.878, word_ins=2.702, length=3.1, ppl=8.06, wps=49889.5, ups=0.84, wpb=59681.7, bsz=2089.7, num_updates=238900, lr=0.000204594, gnorm=1.432, loss_scale=8192, train_wall=119, wall=0
2023-01-12 03:31:37 | INFO | train_inner | epoch 121:   1676 / 1978 loss=3.017, nll_loss=0.886, word_ins=2.709, length=3.084, ppl=8.1, wps=49808.4, ups=0.83, wpb=59656.3, bsz=2119.8, num_updates=239000, lr=0.000204551, gnorm=1.501, loss_scale=8192, train_wall=120, wall=0
2023-01-12 03:33:35 | INFO | train_inner | epoch 121:   1776 / 1978 loss=3.037, nll_loss=0.9, word_ins=2.722, length=3.15, ppl=8.21, wps=50225.7, ups=0.85, wpb=59385.8, bsz=2004.1, num_updates=239100, lr=0.000204508, gnorm=1.477, loss_scale=8192, train_wall=118, wall=0
2023-01-12 03:35:34 | INFO | train_inner | epoch 121:   1876 / 1978 loss=3.05, nll_loss=0.909, word_ins=2.729, length=3.208, ppl=8.28, wps=49899.9, ups=0.84, wpb=59244.4, bsz=2026.6, num_updates=239200, lr=0.000204465, gnorm=1.51, loss_scale=8192, train_wall=119, wall=0
2023-01-12 03:37:32 | INFO | train_inner | epoch 121:   1976 / 1978 loss=3.05, nll_loss=0.911, word_ins=2.732, length=3.173, ppl=8.28, wps=50087.8, ups=0.85, wpb=59265.3, bsz=1970.9, num_updates=239300, lr=0.000204422, gnorm=1.504, loss_scale=8192, train_wall=118, wall=0
2023-01-12 03:37:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 03:37:53 | INFO | valid | epoch 121 | valid on 'valid' subset | loss 4.439 | nll_loss 1.988 | word_ins 3.746 | length 6.941 | ppl 21.7 | wps 192137 | wpb 40242.5 | bsz 1500 | num_updates 239302 | best_loss 4.274
2023-01-12 03:37:53 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 03:37:56 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint121.pt (epoch 121 @ 239302 updates, score 4.439) (writing took 2.9319918318651617 seconds)
2023-01-12 03:37:56 | INFO | fairseq_cli.train | end of epoch 121 (average epoch stats below)
2023-01-12 03:37:56 | INFO | train | epoch 121 | loss 3.042 | nll_loss 0.904 | word_ins 2.726 | length 3.163 | ppl 8.24 | wps 49231.1 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 239302 | lr 0.000204422 | gnorm 1.506 | loss_scale 8192 | train_wall 2340 | wall 0
2023-01-12 03:37:56 | INFO | fairseq.trainer | begin training epoch 122
2023-01-12 03:40:04 | INFO | train_inner | epoch 122:     98 / 1978 loss=3.052, nll_loss=0.909, word_ins=2.73, length=3.213, ppl=8.29, wps=38488.9, ups=0.66, wpb=58692.7, bsz=1894.1, num_updates=239400, lr=0.00020438, gnorm=1.527, loss_scale=8192, train_wall=117, wall=0
2023-01-12 03:42:03 | INFO | train_inner | epoch 122:    198 / 1978 loss=3.02, nll_loss=0.889, word_ins=2.712, length=3.081, ppl=8.11, wps=50064.4, ups=0.84, wpb=59375, bsz=2056, num_updates=239500, lr=0.000204337, gnorm=1.461, loss_scale=8192, train_wall=118, wall=0
2023-01-12 03:44:01 | INFO | train_inner | epoch 122:    298 / 1978 loss=3.045, nll_loss=0.905, word_ins=2.727, length=3.189, ppl=8.26, wps=50277.2, ups=0.85, wpb=59243.4, bsz=1929.8, num_updates=239600, lr=0.000204294, gnorm=1.491, loss_scale=8192, train_wall=118, wall=0
2023-01-12 03:45:59 | INFO | train_inner | epoch 122:    398 / 1978 loss=3.032, nll_loss=0.896, word_ins=2.718, length=3.138, ppl=8.18, wps=50104.9, ups=0.84, wpb=59385.9, bsz=2009.4, num_updates=239700, lr=0.000204252, gnorm=1.476, loss_scale=8192, train_wall=118, wall=0
2023-01-12 03:47:58 | INFO | train_inner | epoch 122:    498 / 1978 loss=3.044, nll_loss=0.906, word_ins=2.727, length=3.168, ppl=8.25, wps=50232.6, ups=0.84, wpb=59520.6, bsz=2041.9, num_updates=239800, lr=0.000204209, gnorm=1.486, loss_scale=8192, train_wall=118, wall=0
2023-01-12 03:49:57 | INFO | train_inner | epoch 122:    598 / 1978 loss=3.023, nll_loss=0.891, word_ins=2.714, length=3.094, ppl=8.13, wps=50079.6, ups=0.84, wpb=59813, bsz=2065.5, num_updates=239900, lr=0.000204167, gnorm=1.522, loss_scale=8192, train_wall=119, wall=0
2023-01-12 03:51:56 | INFO | train_inner | epoch 122:    698 / 1978 loss=3.033, nll_loss=0.897, word_ins=2.72, length=3.134, ppl=8.19, wps=49440, ups=0.84, wpb=58740, bsz=2082.5, num_updates=240000, lr=0.000204124, gnorm=1.483, loss_scale=8192, train_wall=119, wall=0
2023-01-12 03:53:54 | INFO | train_inner | epoch 122:    798 / 1978 loss=3.065, nll_loss=0.925, word_ins=2.745, length=3.203, ppl=8.37, wps=49959.5, ups=0.85, wpb=58921.4, bsz=1945.9, num_updates=240100, lr=0.000204082, gnorm=1.512, loss_scale=8192, train_wall=118, wall=0
2023-01-12 03:55:53 | INFO | train_inner | epoch 122:    898 / 1978 loss=3.048, nll_loss=0.909, word_ins=2.73, length=3.175, ppl=8.27, wps=50033.9, ups=0.84, wpb=59463.4, bsz=1999.8, num_updates=240200, lr=0.000204039, gnorm=1.516, loss_scale=8192, train_wall=119, wall=0
2023-01-12 03:57:51 | INFO | train_inner | epoch 122:    998 / 1978 loss=3.052, nll_loss=0.912, word_ins=2.733, length=3.189, ppl=8.29, wps=50666.8, ups=0.85, wpb=59646.1, bsz=1918.3, num_updates=240300, lr=0.000203997, gnorm=1.602, loss_scale=8192, train_wall=118, wall=0
2023-01-12 03:59:48 | INFO | train_inner | epoch 122:   1098 / 1978 loss=3.036, nll_loss=0.897, word_ins=2.719, length=3.169, ppl=8.2, wps=50353.3, ups=0.85, wpb=59364.4, bsz=1981.3, num_updates=240400, lr=0.000203954, gnorm=1.547, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:01:47 | INFO | train_inner | epoch 122:   1198 / 1978 loss=3.033, nll_loss=0.898, word_ins=2.72, length=3.136, ppl=8.19, wps=49895.7, ups=0.84, wpb=59201.1, bsz=2048, num_updates=240500, lr=0.000203912, gnorm=1.473, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:03:46 | INFO | train_inner | epoch 122:   1298 / 1978 loss=3.029, nll_loss=0.893, word_ins=2.716, length=3.132, ppl=8.16, wps=50455.2, ups=0.84, wpb=59885.3, bsz=2045.2, num_updates=240600, lr=0.000203869, gnorm=1.553, loss_scale=8192, train_wall=119, wall=0
2023-01-12 04:05:44 | INFO | train_inner | epoch 122:   1398 / 1978 loss=3.048, nll_loss=0.909, word_ins=2.73, length=3.179, ppl=8.27, wps=49951.8, ups=0.85, wpb=59047.4, bsz=2013.4, num_updates=240700, lr=0.000203827, gnorm=1.509, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:07:41 | INFO | train_inner | epoch 122:   1498 / 1978 loss=3.058, nll_loss=0.918, word_ins=2.738, length=3.203, ppl=8.33, wps=50408.7, ups=0.85, wpb=59196.4, bsz=1943.5, num_updates=240800, lr=0.000203785, gnorm=1.563, loss_scale=8192, train_wall=117, wall=0
2023-01-12 04:09:41 | INFO | train_inner | epoch 122:   1598 / 1978 loss=3.034, nll_loss=0.904, word_ins=2.726, length=3.084, ppl=8.19, wps=49830.6, ups=0.84, wpb=59352.4, bsz=2065.6, num_updates=240900, lr=0.000203742, gnorm=1.465, loss_scale=8192, train_wall=119, wall=0
2023-01-12 04:11:39 | INFO | train_inner | epoch 122:   1698 / 1978 loss=3.036, nll_loss=0.901, word_ins=2.723, length=3.134, ppl=8.2, wps=49971.8, ups=0.84, wpb=59370.6, bsz=2014.9, num_updates=241000, lr=0.0002037, gnorm=1.514, loss_scale=8192, train_wall=119, wall=0
2023-01-12 04:13:38 | INFO | train_inner | epoch 122:   1798 / 1978 loss=3.049, nll_loss=0.909, word_ins=2.731, length=3.184, ppl=8.28, wps=49922.1, ups=0.84, wpb=59216.1, bsz=1998.2, num_updates=241100, lr=0.000203658, gnorm=1.498, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:15:35 | INFO | train_inner | epoch 122:   1898 / 1978 loss=3.049, nll_loss=0.908, word_ins=2.729, length=3.201, ppl=8.28, wps=50447.5, ups=0.85, wpb=59211.1, bsz=1979.6, num_updates=241200, lr=0.000203616, gnorm=1.5, loss_scale=8192, train_wall=117, wall=0
2023-01-12 04:17:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 04:17:25 | INFO | valid | epoch 122 | valid on 'valid' subset | loss 4.441 | nll_loss 2.014 | word_ins 3.771 | length 6.704 | ppl 21.72 | wps 190083 | wpb 40242.5 | bsz 1500 | num_updates 241280 | best_loss 4.274
2023-01-12 04:17:25 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 04:17:27 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint122.pt (epoch 122 @ 241280 updates, score 4.441) (writing took 2.8118764646351337 seconds)
2023-01-12 04:17:27 | INFO | fairseq_cli.train | end of epoch 122 (average epoch stats below)
2023-01-12 04:17:27 | INFO | train | epoch 122 | loss 3.041 | nll_loss 0.904 | word_ins 2.725 | length 3.157 | ppl 8.23 | wps 49438.7 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 241280 | lr 0.000203582 | gnorm 1.509 | loss_scale 8192 | train_wall 2337 | wall 0
2023-01-12 04:17:27 | INFO | fairseq.trainer | begin training epoch 123
2023-01-12 04:18:05 | INFO | train_inner | epoch 123:     20 / 1978 loss=3.043, nll_loss=0.907, word_ins=2.729, length=3.148, ppl=8.24, wps=39273.2, ups=0.67, wpb=58752.8, bsz=1988.7, num_updates=241300, lr=0.000203574, gnorm=1.485, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:20:03 | INFO | train_inner | epoch 123:    120 / 1978 loss=3.039, nll_loss=0.9, word_ins=2.722, length=3.169, ppl=8.22, wps=50164.6, ups=0.85, wpb=59332.5, bsz=1986.6, num_updates=241400, lr=0.000203531, gnorm=1.506, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:22:02 | INFO | train_inner | epoch 123:    220 / 1978 loss=3.019, nll_loss=0.885, word_ins=2.708, length=3.112, ppl=8.11, wps=50296.6, ups=0.85, wpb=59514.8, bsz=2033.2, num_updates=241500, lr=0.000203489, gnorm=1.523, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:24:00 | INFO | train_inner | epoch 123:    320 / 1978 loss=3.034, nll_loss=0.896, word_ins=2.718, length=3.153, ppl=8.19, wps=49743.2, ups=0.84, wpb=59104, bsz=2050.1, num_updates=241600, lr=0.000203447, gnorm=1.522, loss_scale=8192, train_wall=119, wall=0
2023-01-12 04:25:59 | INFO | train_inner | epoch 123:    420 / 1978 loss=3.03, nll_loss=0.894, word_ins=2.716, length=3.134, ppl=8.17, wps=50233, ups=0.84, wpb=59642.4, bsz=1977.9, num_updates=241700, lr=0.000203405, gnorm=1.509, loss_scale=8192, train_wall=119, wall=0
2023-01-12 04:27:57 | INFO | train_inner | epoch 123:    520 / 1978 loss=3.054, nll_loss=0.916, word_ins=2.737, length=3.17, ppl=8.3, wps=49868.5, ups=0.85, wpb=58871.2, bsz=1891.7, num_updates=241800, lr=0.000203363, gnorm=1.469, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:29:55 | INFO | train_inner | epoch 123:    620 / 1978 loss=3.037, nll_loss=0.901, word_ins=2.723, length=3.145, ppl=8.21, wps=49931.7, ups=0.85, wpb=58841.1, bsz=1979.7, num_updates=241900, lr=0.000203321, gnorm=1.473, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:31:54 | INFO | train_inner | epoch 123:    720 / 1978 loss=3.016, nll_loss=0.88, word_ins=2.704, length=3.114, ppl=8.09, wps=49732.1, ups=0.84, wpb=59114.7, bsz=2073.3, num_updates=242000, lr=0.000203279, gnorm=1.442, loss_scale=8192, train_wall=119, wall=0
2023-01-12 04:33:53 | INFO | train_inner | epoch 123:    820 / 1978 loss=3.019, nll_loss=0.883, word_ins=2.707, length=3.122, ppl=8.11, wps=49775.1, ups=0.84, wpb=59250.1, bsz=2114.6, num_updates=242100, lr=0.000203237, gnorm=1.505, loss_scale=8192, train_wall=119, wall=0
2023-01-12 04:35:51 | INFO | train_inner | epoch 123:    920 / 1978 loss=3.061, nll_loss=0.92, word_ins=2.741, length=3.206, ppl=8.35, wps=49968.5, ups=0.85, wpb=59049.1, bsz=1914.1, num_updates=242200, lr=0.000203195, gnorm=1.547, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:37:49 | INFO | train_inner | epoch 123:   1020 / 1978 loss=3.034, nll_loss=0.894, word_ins=2.717, length=3.179, ppl=8.19, wps=50254.5, ups=0.85, wpb=59242.7, bsz=2008.7, num_updates=242300, lr=0.000203153, gnorm=1.491, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:39:48 | INFO | train_inner | epoch 123:   1120 / 1978 loss=3.054, nll_loss=0.915, word_ins=2.735, length=3.184, ppl=8.3, wps=49937.8, ups=0.84, wpb=59197.8, bsz=2007.5, num_updates=242400, lr=0.000203111, gnorm=1.566, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:41:47 | INFO | train_inner | epoch 123:   1220 / 1978 loss=3.014, nll_loss=0.883, word_ins=2.707, length=3.07, ppl=8.08, wps=50523.4, ups=0.84, wpb=60410.4, bsz=2099.4, num_updates=242500, lr=0.000203069, gnorm=1.487, loss_scale=8192, train_wall=119, wall=0
2023-01-12 04:43:00 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2023-01-12 04:43:46 | INFO | train_inner | epoch 123:   1321 / 1978 loss=3.066, nll_loss=0.928, word_ins=2.748, length=3.184, ppl=8.37, wps=49410.9, ups=0.84, wpb=58887.8, bsz=1932.6, num_updates=242600, lr=0.000203027, gnorm=1.497, loss_scale=8192, train_wall=119, wall=0
2023-01-12 04:45:45 | INFO | train_inner | epoch 123:   1421 / 1978 loss=3.033, nll_loss=0.901, word_ins=2.722, length=3.11, ppl=8.19, wps=50212.7, ups=0.84, wpb=59526.3, bsz=2090.5, num_updates=242700, lr=0.000202986, gnorm=1.471, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:47:43 | INFO | train_inner | epoch 123:   1521 / 1978 loss=3.03, nll_loss=0.9, word_ins=2.721, length=3.083, ppl=8.17, wps=50212, ups=0.84, wpb=59505.8, bsz=2036.6, num_updates=242800, lr=0.000202944, gnorm=1.51, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:49:42 | INFO | train_inner | epoch 123:   1621 / 1978 loss=3.075, nll_loss=0.93, word_ins=2.749, length=3.26, ppl=8.43, wps=49741.1, ups=0.85, wpb=58813.2, bsz=1932.2, num_updates=242900, lr=0.000202902, gnorm=1.59, loss_scale=8192, train_wall=118, wall=0
2023-01-12 04:51:39 | INFO | train_inner | epoch 123:   1721 / 1978 loss=3.046, nll_loss=0.907, word_ins=2.728, length=3.183, ppl=8.26, wps=50480, ups=0.85, wpb=59374.2, bsz=1969.8, num_updates=243000, lr=0.00020286, gnorm=1.529, loss_scale=8192, train_wall=117, wall=0
2023-01-12 04:53:39 | INFO | train_inner | epoch 123:   1821 / 1978 loss=3.036, nll_loss=0.902, word_ins=2.723, length=3.13, ppl=8.2, wps=49898.9, ups=0.84, wpb=59657.9, bsz=2046.1, num_updates=243100, lr=0.000202818, gnorm=1.53, loss_scale=8192, train_wall=119, wall=0
2023-01-12 04:55:36 | INFO | train_inner | epoch 123:   1921 / 1978 loss=3.069, nll_loss=0.926, word_ins=2.746, length=3.235, ppl=8.39, wps=50313.5, ups=0.85, wpb=59016.4, bsz=1884.4, num_updates=243200, lr=0.000202777, gnorm=1.553, loss_scale=8192, train_wall=117, wall=0
2023-01-12 04:56:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 04:57:02 | INFO | valid | epoch 123 | valid on 'valid' subset | loss 4.464 | nll_loss 2.014 | word_ins 3.771 | length 6.938 | ppl 22.08 | wps 188256 | wpb 40242.5 | bsz 1500 | num_updates 243257 | best_loss 4.274
2023-01-12 04:57:02 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 04:57:05 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint123.pt (epoch 123 @ 243257 updates, score 4.464) (writing took 2.975179329048842 seconds)
2023-01-12 04:57:05 | INFO | fairseq_cli.train | end of epoch 123 (average epoch stats below)
2023-01-12 04:57:05 | INFO | train | epoch 123 | loss 3.04 | nll_loss 0.903 | word_ins 2.724 | length 3.154 | ppl 8.22 | wps 49296.3 | ups 0.83 | wpb 59289 | bsz 2003.1 | num_updates 243257 | lr 0.000202753 | gnorm 1.51 | loss_scale 8192 | train_wall 2339 | wall 0
2023-01-12 04:57:05 | INFO | fairseq.trainer | begin training epoch 124
2023-01-12 04:58:09 | INFO | train_inner | epoch 124:     43 / 1978 loss=3.045, nll_loss=0.909, word_ins=2.73, length=3.151, ppl=8.25, wps=38812.1, ups=0.65, wpb=59421.1, bsz=1954.3, num_updates=243300, lr=0.000202735, gnorm=1.486, loss_scale=8192, train_wall=118, wall=0
2023-01-12 05:00:07 | INFO | train_inner | epoch 124:    143 / 1978 loss=3.034, nll_loss=0.895, word_ins=2.717, length=3.162, ppl=8.19, wps=50016, ups=0.85, wpb=58783.2, bsz=2005.3, num_updates=243400, lr=0.000202693, gnorm=1.495, loss_scale=8192, train_wall=117, wall=0
2023-01-12 05:02:05 | INFO | train_inner | epoch 124:    243 / 1978 loss=3.013, nll_loss=0.878, word_ins=2.702, length=3.109, ppl=8.07, wps=49856.2, ups=0.84, wpb=59058, bsz=2100.3, num_updates=243500, lr=0.000202652, gnorm=1.439, loss_scale=8192, train_wall=118, wall=0
2023-01-12 05:04:04 | INFO | train_inner | epoch 124:    343 / 1978 loss=3.036, nll_loss=0.899, word_ins=2.721, length=3.143, ppl=8.2, wps=50306.6, ups=0.84, wpb=59740.7, bsz=2023.1, num_updates=243600, lr=0.00020261, gnorm=1.514, loss_scale=8192, train_wall=119, wall=0
2023-01-12 05:06:02 | INFO | train_inner | epoch 124:    443 / 1978 loss=3.058, nll_loss=0.915, word_ins=2.736, length=3.223, ppl=8.33, wps=50278.6, ups=0.85, wpb=59229.3, bsz=1900.2, num_updates=243700, lr=0.000202569, gnorm=1.601, loss_scale=8192, train_wall=118, wall=0
2023-01-12 05:08:01 | INFO | train_inner | epoch 124:    543 / 1978 loss=3.01, nll_loss=0.882, word_ins=2.705, length=3.044, ppl=8.05, wps=49971.5, ups=0.84, wpb=59457.6, bsz=2123.9, num_updates=243800, lr=0.000202527, gnorm=1.464, loss_scale=8192, train_wall=119, wall=0
2023-01-12 05:09:59 | INFO | train_inner | epoch 124:    643 / 1978 loss=3.04, nll_loss=0.902, word_ins=2.724, length=3.163, ppl=8.23, wps=50154.6, ups=0.85, wpb=59112.9, bsz=2006.2, num_updates=243900, lr=0.000202486, gnorm=1.575, loss_scale=8192, train_wall=118, wall=0
2023-01-12 05:11:57 | INFO | train_inner | epoch 124:    743 / 1978 loss=3.05, nll_loss=0.912, word_ins=2.732, length=3.179, ppl=8.28, wps=50331.5, ups=0.85, wpb=59517.2, bsz=1978.4, num_updates=244000, lr=0.000202444, gnorm=1.571, loss_scale=8192, train_wall=118, wall=0
2023-01-12 05:13:55 | INFO | train_inner | epoch 124:    843 / 1978 loss=3.025, nll_loss=0.895, word_ins=2.717, length=3.077, ppl=8.14, wps=50003, ups=0.85, wpb=58972.7, bsz=2009.4, num_updates=244100, lr=0.000202403, gnorm=1.437, loss_scale=8192, train_wall=118, wall=0
2023-01-12 05:15:53 | INFO | train_inner | epoch 124:    943 / 1978 loss=3.044, nll_loss=0.902, word_ins=2.724, length=3.197, ppl=8.25, wps=49926.7, ups=0.84, wpb=59187.6, bsz=2001, num_updates=244200, lr=0.000202361, gnorm=1.522, loss_scale=8192, train_wall=118, wall=0
2023-01-12 05:16:07 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2023-01-12 05:17:53 | INFO | train_inner | epoch 124:   1044 / 1978 loss=3.049, nll_loss=0.911, word_ins=2.732, length=3.162, ppl=8.27, wps=49225.6, ups=0.84, wpb=58848.7, bsz=2002.3, num_updates=244300, lr=0.00020232, gnorm=1.497, loss_scale=4096, train_wall=119, wall=0
2023-01-12 05:19:53 | INFO | train_inner | epoch 124:   1144 / 1978 loss=3.018, nll_loss=0.883, word_ins=2.706, length=3.113, ppl=8.1, wps=49491.2, ups=0.83, wpb=59285.6, bsz=2110.4, num_updates=244400, lr=0.000202278, gnorm=1.526, loss_scale=4096, train_wall=120, wall=0
2023-01-12 05:21:52 | INFO | train_inner | epoch 124:   1244 / 1978 loss=3.061, nll_loss=0.927, word_ins=2.746, length=3.153, ppl=8.35, wps=50165.9, ups=0.84, wpb=59754.9, bsz=1904.1, num_updates=244500, lr=0.000202237, gnorm=1.546, loss_scale=4096, train_wall=119, wall=0
2023-01-12 05:23:50 | INFO | train_inner | epoch 124:   1344 / 1978 loss=3.026, nll_loss=0.889, word_ins=2.712, length=3.14, ppl=8.14, wps=50047.7, ups=0.84, wpb=59335.3, bsz=2039.8, num_updates=244600, lr=0.000202196, gnorm=1.538, loss_scale=4096, train_wall=118, wall=0
2023-01-12 05:25:48 | INFO | train_inner | epoch 124:   1444 / 1978 loss=3.048, nll_loss=0.907, word_ins=2.728, length=3.205, ppl=8.27, wps=50390.3, ups=0.85, wpb=59328.4, bsz=1976.2, num_updates=244700, lr=0.000202154, gnorm=1.567, loss_scale=4096, train_wall=118, wall=0
2023-01-12 05:27:47 | INFO | train_inner | epoch 124:   1544 / 1978 loss=3.031, nll_loss=0.896, word_ins=2.718, length=3.124, ppl=8.17, wps=49963.6, ups=0.84, wpb=59496.3, bsz=2059.6, num_updates=244800, lr=0.000202113, gnorm=1.503, loss_scale=4096, train_wall=119, wall=0
2023-01-12 05:29:46 | INFO | train_inner | epoch 124:   1644 / 1978 loss=3.046, nll_loss=0.905, word_ins=2.726, length=3.195, ppl=8.26, wps=49797.8, ups=0.84, wpb=59108.2, bsz=2007.2, num_updates=244900, lr=0.000202072, gnorm=1.479, loss_scale=4096, train_wall=118, wall=0
2023-01-12 05:31:45 | INFO | train_inner | epoch 124:   1744 / 1978 loss=3.033, nll_loss=0.896, word_ins=2.718, length=3.148, ppl=8.18, wps=50108.5, ups=0.84, wpb=59488.4, bsz=2007.4, num_updates=245000, lr=0.000202031, gnorm=1.498, loss_scale=4096, train_wall=119, wall=0
2023-01-12 05:33:43 | INFO | train_inner | epoch 124:   1844 / 1978 loss=3.026, nll_loss=0.893, word_ins=2.715, length=3.103, ppl=8.14, wps=50660.7, ups=0.85, wpb=59947.4, bsz=2060.6, num_updates=245100, lr=0.000201989, gnorm=1.523, loss_scale=4096, train_wall=118, wall=0
2023-01-12 05:35:41 | INFO | train_inner | epoch 124:   1944 / 1978 loss=3.071, nll_loss=0.93, word_ins=2.749, length=3.222, ppl=8.4, wps=50092, ups=0.85, wpb=59103.6, bsz=1914.1, num_updates=245200, lr=0.000201948, gnorm=1.544, loss_scale=4096, train_wall=118, wall=0
2023-01-12 05:36:21 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 05:36:36 | INFO | valid | epoch 124 | valid on 'valid' subset | loss 4.469 | nll_loss 1.998 | word_ins 3.754 | length 7.164 | ppl 22.15 | wps 104629 | wpb 40242.5 | bsz 1500 | num_updates 245234 | best_loss 4.274
2023-01-12 05:36:36 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 05:36:39 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint124.pt (epoch 124 @ 245234 updates, score 4.469) (writing took 2.9243721701204777 seconds)
2023-01-12 05:36:39 | INFO | fairseq_cli.train | end of epoch 124 (average epoch stats below)
2023-01-12 05:36:39 | INFO | train | epoch 124 | loss 3.039 | nll_loss 0.902 | word_ins 2.724 | length 3.154 | ppl 8.22 | wps 49379.4 | ups 0.83 | wpb 59286.4 | bsz 2003 | num_updates 245234 | lr 0.000201934 | gnorm 1.517 | loss_scale 4096 | train_wall 2338 | wall 0
2023-01-12 05:36:39 | INFO | fairseq.trainer | begin training epoch 125
2023-01-12 05:38:17 | INFO | train_inner | epoch 125:     66 / 1978 loss=3.038, nll_loss=0.9, word_ins=2.723, length=3.151, ppl=8.21, wps=37601.6, ups=0.64, wpb=58532.2, bsz=1924.1, num_updates=245300, lr=0.000201907, gnorm=1.476, loss_scale=4096, train_wall=118, wall=0
2023-01-12 05:40:14 | INFO | train_inner | epoch 125:    166 / 1978 loss=3.039, nll_loss=0.898, word_ins=2.72, length=3.185, ppl=8.22, wps=50180.2, ups=0.85, wpb=59110.3, bsz=1964.2, num_updates=245400, lr=0.000201866, gnorm=1.523, loss_scale=4096, train_wall=118, wall=0
2023-01-12 05:42:12 | INFO | train_inner | epoch 125:    266 / 1978 loss=3.036, nll_loss=0.898, word_ins=2.72, length=3.159, ppl=8.2, wps=50163.3, ups=0.85, wpb=59021.7, bsz=1970.6, num_updates=245500, lr=0.000201825, gnorm=1.445, loss_scale=4096, train_wall=117, wall=0
2023-01-12 05:44:11 | INFO | train_inner | epoch 125:    366 / 1978 loss=3.034, nll_loss=0.896, word_ins=2.719, length=3.157, ppl=8.19, wps=49775.9, ups=0.84, wpb=59182.9, bsz=2015.8, num_updates=245600, lr=0.000201784, gnorm=1.584, loss_scale=4096, train_wall=119, wall=0
2023-01-12 05:46:09 | INFO | train_inner | epoch 125:    466 / 1978 loss=3.035, nll_loss=0.898, word_ins=2.72, length=3.151, ppl=8.2, wps=49837.4, ups=0.85, wpb=58939.2, bsz=2034.2, num_updates=245700, lr=0.000201743, gnorm=1.5, loss_scale=4096, train_wall=118, wall=0
2023-01-12 05:48:07 | INFO | train_inner | epoch 125:    566 / 1978 loss=3.028, nll_loss=0.894, word_ins=2.717, length=3.115, ppl=8.16, wps=50436.8, ups=0.85, wpb=59499.9, bsz=1994.6, num_updates=245800, lr=0.000201701, gnorm=1.547, loss_scale=4096, train_wall=118, wall=0
2023-01-12 05:50:05 | INFO | train_inner | epoch 125:    666 / 1978 loss=3.027, nll_loss=0.893, word_ins=2.716, length=3.113, ppl=8.15, wps=50190.2, ups=0.85, wpb=59339.2, bsz=1994.9, num_updates=245900, lr=0.00020166, gnorm=1.504, loss_scale=4096, train_wall=118, wall=0
2023-01-12 05:52:03 | INFO | train_inner | epoch 125:    766 / 1978 loss=3.061, nll_loss=0.92, word_ins=2.74, length=3.207, ppl=8.34, wps=50175.8, ups=0.85, wpb=59059.2, bsz=1901.7, num_updates=246000, lr=0.000201619, gnorm=1.554, loss_scale=4096, train_wall=118, wall=0
2023-01-12 05:54:01 | INFO | train_inner | epoch 125:    866 / 1978 loss=3.035, nll_loss=0.902, word_ins=2.724, length=3.104, ppl=8.19, wps=50547.7, ups=0.85, wpb=59783.6, bsz=2011.7, num_updates=246100, lr=0.000201578, gnorm=1.572, loss_scale=4096, train_wall=118, wall=0
2023-01-12 05:55:59 | INFO | train_inner | epoch 125:    966 / 1978 loss=3.045, nll_loss=0.904, word_ins=2.725, length=3.2, ppl=8.26, wps=50080.6, ups=0.85, wpb=59053.4, bsz=1959.7, num_updates=246200, lr=0.000201538, gnorm=1.458, loss_scale=4096, train_wall=118, wall=0
2023-01-12 05:57:58 | INFO | train_inner | epoch 125:   1066 / 1978 loss=3.044, nll_loss=0.901, word_ins=2.722, length=3.22, ppl=8.25, wps=50421.2, ups=0.84, wpb=59784.1, bsz=1959.2, num_updates=246300, lr=0.000201497, gnorm=1.599, loss_scale=4096, train_wall=118, wall=0
2023-01-12 05:59:56 | INFO | train_inner | epoch 125:   1166 / 1978 loss=3.038, nll_loss=0.903, word_ins=2.725, length=3.132, ppl=8.21, wps=49789.2, ups=0.84, wpb=59068, bsz=1997.4, num_updates=246400, lr=0.000201456, gnorm=1.534, loss_scale=4096, train_wall=118, wall=0
2023-01-12 06:01:56 | INFO | train_inner | epoch 125:   1266 / 1978 loss=3.028, nll_loss=0.892, word_ins=2.715, length=3.128, ppl=8.16, wps=49507.5, ups=0.84, wpb=58971.4, bsz=2129.9, num_updates=246500, lr=0.000201415, gnorm=1.474, loss_scale=4096, train_wall=119, wall=0
2023-01-12 06:03:54 | INFO | train_inner | epoch 125:   1366 / 1978 loss=3.046, nll_loss=0.906, word_ins=2.727, length=3.19, ppl=8.26, wps=49933.9, ups=0.84, wpb=59148.7, bsz=1994.2, num_updates=246600, lr=0.000201374, gnorm=1.496, loss_scale=4096, train_wall=118, wall=0
2023-01-12 06:05:52 | INFO | train_inner | epoch 125:   1466 / 1978 loss=3.063, nll_loss=0.922, word_ins=2.742, length=3.211, ppl=8.36, wps=49390, ups=0.85, wpb=58365.8, bsz=2060.9, num_updates=246700, lr=0.000201333, gnorm=1.513, loss_scale=4096, train_wall=118, wall=0
2023-01-12 06:07:51 | INFO | train_inner | epoch 125:   1566 / 1978 loss=3.026, nll_loss=0.891, word_ins=2.714, length=3.125, ppl=8.15, wps=50262.2, ups=0.84, wpb=59556.4, bsz=2025.4, num_updates=246800, lr=0.000201292, gnorm=1.462, loss_scale=4096, train_wall=118, wall=0
2023-01-12 06:09:50 | INFO | train_inner | epoch 125:   1666 / 1978 loss=3.026, nll_loss=0.89, word_ins=2.712, length=3.14, ppl=8.15, wps=50218.2, ups=0.84, wpb=59689.6, bsz=2076.5, num_updates=246900, lr=0.000201252, gnorm=1.542, loss_scale=4096, train_wall=119, wall=0
2023-01-12 06:11:48 | INFO | train_inner | epoch 125:   1766 / 1978 loss=3.031, nll_loss=0.893, word_ins=2.715, length=3.158, ppl=8.17, wps=50778.4, ups=0.85, wpb=59977.8, bsz=1930.5, num_updates=247000, lr=0.000201211, gnorm=1.543, loss_scale=4096, train_wall=118, wall=0
2023-01-12 06:13:46 | INFO | train_inner | epoch 125:   1866 / 1978 loss=3.059, nll_loss=0.92, word_ins=2.74, length=3.188, ppl=8.33, wps=50151, ups=0.84, wpb=59372.8, bsz=1943.8, num_updates=247100, lr=0.00020117, gnorm=1.53, loss_scale=4096, train_wall=118, wall=0
2023-01-12 06:15:44 | INFO | train_inner | epoch 125:   1966 / 1978 loss=3.023, nll_loss=0.885, word_ins=2.708, length=3.147, ppl=8.13, wps=50882.6, ups=0.85, wpb=60068.1, bsz=2060.2, num_updates=247200, lr=0.000201129, gnorm=1.533, loss_scale=4096, train_wall=118, wall=0
2023-01-12 06:15:58 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 06:16:16 | INFO | valid | epoch 125 | valid on 'valid' subset | loss 4.444 | nll_loss 1.98 | word_ins 3.737 | length 7.069 | ppl 21.76 | wps 166314 | wpb 40242.5 | bsz 1500 | num_updates 247212 | best_loss 4.274
2023-01-12 06:16:16 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 06:16:19 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint125.pt (epoch 125 @ 247212 updates, score 4.444) (writing took 2.941089645959437 seconds)
2023-01-12 06:16:19 | INFO | fairseq_cli.train | end of epoch 125 (average epoch stats below)
2023-01-12 06:16:19 | INFO | train | epoch 125 | loss 3.037 | nll_loss 0.9 | word_ins 2.722 | length 3.156 | ppl 8.21 | wps 49273.8 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 247212 | lr 0.000201125 | gnorm 1.52 | loss_scale 4096 | train_wall 2336 | wall 0
2023-01-12 06:16:19 | INFO | fairseq.trainer | begin training epoch 126
2023-01-12 06:18:17 | INFO | train_inner | epoch 126:     88 / 1978 loss=3.023, nll_loss=0.891, word_ins=2.714, length=3.089, ppl=8.13, wps=38907.3, ups=0.66, wpb=59354.2, bsz=2003.4, num_updates=247300, lr=0.000201089, gnorm=1.499, loss_scale=4096, train_wall=118, wall=0
2023-01-12 06:20:14 | INFO | train_inner | epoch 126:    188 / 1978 loss=3.03, nll_loss=0.893, word_ins=2.716, length=3.142, ppl=8.17, wps=50224, ups=0.85, wpb=59081.9, bsz=1962.3, num_updates=247400, lr=0.000201048, gnorm=1.552, loss_scale=4096, train_wall=117, wall=0
2023-01-12 06:22:12 | INFO | train_inner | epoch 126:    288 / 1978 loss=3.055, nll_loss=0.913, word_ins=2.733, length=3.215, ppl=8.31, wps=50421.3, ups=0.85, wpb=59237.4, bsz=1859.8, num_updates=247500, lr=0.000201008, gnorm=1.554, loss_scale=4096, train_wall=117, wall=0
2023-01-12 06:24:11 | INFO | train_inner | epoch 126:    388 / 1978 loss=3.005, nll_loss=0.874, word_ins=2.698, length=3.071, ppl=8.03, wps=49798.2, ups=0.84, wpb=59384.8, bsz=2107.2, num_updates=247600, lr=0.000200967, gnorm=1.554, loss_scale=4096, train_wall=119, wall=0
2023-01-12 06:26:10 | INFO | train_inner | epoch 126:    488 / 1978 loss=3.019, nll_loss=0.882, word_ins=2.706, length=3.127, ppl=8.11, wps=49628.2, ups=0.84, wpb=59142.3, bsz=2111.8, num_updates=247700, lr=0.000200926, gnorm=1.508, loss_scale=4096, train_wall=119, wall=0
2023-01-12 06:28:08 | INFO | train_inner | epoch 126:    588 / 1978 loss=3.035, nll_loss=0.896, word_ins=2.718, length=3.174, ppl=8.2, wps=50614.3, ups=0.85, wpb=59715.9, bsz=2027, num_updates=247800, lr=0.000200886, gnorm=1.514, loss_scale=4096, train_wall=118, wall=0
2023-01-12 06:30:07 | INFO | train_inner | epoch 126:    688 / 1978 loss=3.033, nll_loss=0.897, word_ins=2.719, length=3.141, ppl=8.18, wps=50037.3, ups=0.84, wpb=59642.4, bsz=2033.3, num_updates=247900, lr=0.000200845, gnorm=1.532, loss_scale=4096, train_wall=119, wall=0
2023-01-12 06:32:05 | INFO | train_inner | epoch 126:    788 / 1978 loss=3.046, nll_loss=0.905, word_ins=2.726, length=3.197, ppl=8.26, wps=50053.3, ups=0.85, wpb=59007.8, bsz=1965.4, num_updates=248000, lr=0.000200805, gnorm=1.508, loss_scale=4096, train_wall=118, wall=0
2023-01-12 06:34:02 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2023-01-12 06:34:04 | INFO | train_inner | epoch 126:    889 / 1978 loss=3.032, nll_loss=0.896, word_ins=2.718, length=3.142, ppl=8.18, wps=49915.7, ups=0.84, wpb=59446.8, bsz=2009.8, num_updates=248100, lr=0.000200764, gnorm=1.52, loss_scale=2048, train_wall=119, wall=0
2023-01-12 06:36:03 | INFO | train_inner | epoch 126:    989 / 1978 loss=3.02, nll_loss=0.885, word_ins=2.708, length=3.118, ppl=8.11, wps=50408.2, ups=0.85, wpb=59575.3, bsz=1962.3, num_updates=248200, lr=0.000200724, gnorm=1.454, loss_scale=2048, train_wall=118, wall=0
2023-01-12 06:38:01 | INFO | train_inner | epoch 126:   1089 / 1978 loss=3.036, nll_loss=0.894, word_ins=2.716, length=3.203, ppl=8.2, wps=50014.9, ups=0.84, wpb=59276.8, bsz=1996, num_updates=248300, lr=0.000200683, gnorm=1.527, loss_scale=2048, train_wall=118, wall=0
2023-01-12 06:40:00 | INFO | train_inner | epoch 126:   1189 / 1978 loss=3.014, nll_loss=0.882, word_ins=2.705, length=3.081, ppl=8.08, wps=49943.2, ups=0.84, wpb=59335.5, bsz=2057.3, num_updates=248400, lr=0.000200643, gnorm=1.529, loss_scale=2048, train_wall=119, wall=0
2023-01-12 06:41:58 | INFO | train_inner | epoch 126:   1289 / 1978 loss=3.036, nll_loss=0.898, word_ins=2.719, length=3.17, ppl=8.2, wps=50175.2, ups=0.84, wpb=59440.7, bsz=2051.4, num_updates=248500, lr=0.000200603, gnorm=1.483, loss_scale=2048, train_wall=118, wall=0
2023-01-12 06:43:57 | INFO | train_inner | epoch 126:   1389 / 1978 loss=3.038, nll_loss=0.9, word_ins=2.722, length=3.168, ppl=8.22, wps=49503.3, ups=0.84, wpb=58713.7, bsz=2027.2, num_updates=248600, lr=0.000200562, gnorm=1.499, loss_scale=2048, train_wall=118, wall=0
2023-01-12 06:45:56 | INFO | train_inner | epoch 126:   1489 / 1978 loss=3.035, nll_loss=0.897, word_ins=2.718, length=3.167, ppl=8.2, wps=49893.9, ups=0.84, wpb=59418.8, bsz=1983.9, num_updates=248700, lr=0.000200522, gnorm=1.536, loss_scale=2048, train_wall=119, wall=0
2023-01-12 06:47:54 | INFO | train_inner | epoch 126:   1589 / 1978 loss=3.06, nll_loss=0.918, word_ins=2.739, length=3.21, ppl=8.34, wps=50065.8, ups=0.85, wpb=59094, bsz=1935.3, num_updates=248800, lr=0.000200482, gnorm=1.538, loss_scale=2048, train_wall=118, wall=0
2023-01-12 06:49:52 | INFO | train_inner | epoch 126:   1689 / 1978 loss=3.062, nll_loss=0.926, word_ins=2.745, length=3.164, ppl=8.35, wps=49740.8, ups=0.85, wpb=58751.7, bsz=2004.5, num_updates=248900, lr=0.000200441, gnorm=1.574, loss_scale=2048, train_wall=118, wall=0
2023-01-12 06:51:50 | INFO | train_inner | epoch 126:   1789 / 1978 loss=3.052, nll_loss=0.915, word_ins=2.735, length=3.177, ppl=8.3, wps=50668, ups=0.85, wpb=59482.8, bsz=1896.6, num_updates=249000, lr=0.000200401, gnorm=1.531, loss_scale=2048, train_wall=117, wall=0
2023-01-12 06:53:49 | INFO | train_inner | epoch 126:   1889 / 1978 loss=3.025, nll_loss=0.887, word_ins=2.709, length=3.157, ppl=8.14, wps=49926.1, ups=0.84, wpb=59739.6, bsz=2093.5, num_updates=249100, lr=0.000200361, gnorm=1.495, loss_scale=2048, train_wall=119, wall=0
2023-01-12 06:55:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 06:55:54 | INFO | valid | epoch 126 | valid on 'valid' subset | loss 4.44 | nll_loss 2.005 | word_ins 3.764 | length 6.764 | ppl 21.71 | wps 161676 | wpb 40242.5 | bsz 1500 | num_updates 249189 | best_loss 4.274
2023-01-12 06:55:54 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 06:55:56 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint126.pt (epoch 126 @ 249189 updates, score 4.44) (writing took 2.898746103979647 seconds)
2023-01-12 06:55:56 | INFO | fairseq_cli.train | end of epoch 126 (average epoch stats below)
2023-01-12 06:55:56 | INFO | train | epoch 126 | loss 3.035 | nll_loss 0.898 | word_ins 2.72 | length 3.156 | ppl 8.2 | wps 49296.9 | ups 0.83 | wpb 59289.3 | bsz 2003 | num_updates 249189 | lr 0.000200325 | gnorm 1.521 | loss_scale 2048 | train_wall 2338 | wall 0
2023-01-12 06:55:56 | INFO | fairseq.trainer | begin training epoch 127
2023-01-12 06:56:25 | INFO | train_inner | epoch 127:     11 / 1978 loss=3.042, nll_loss=0.903, word_ins=2.724, length=3.178, ppl=8.24, wps=37643.3, ups=0.64, wpb=58710.8, bsz=1988.1, num_updates=249200, lr=0.000200321, gnorm=1.5, loss_scale=2048, train_wall=118, wall=0
2023-01-12 06:58:25 | INFO | train_inner | epoch 127:    111 / 1978 loss=3.005, nll_loss=0.871, word_ins=2.695, length=3.094, ppl=8.03, wps=49914.9, ups=0.84, wpb=59767.6, bsz=2107.8, num_updates=249300, lr=0.000200281, gnorm=1.506, loss_scale=2048, train_wall=120, wall=0
2023-01-12 07:00:23 | INFO | train_inner | epoch 127:    211 / 1978 loss=3.047, nll_loss=0.909, word_ins=2.729, length=3.179, ppl=8.27, wps=50050.2, ups=0.85, wpb=59009.2, bsz=1969.1, num_updates=249400, lr=0.00020024, gnorm=1.529, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:02:21 | INFO | train_inner | epoch 127:    311 / 1978 loss=3.036, nll_loss=0.897, word_ins=2.719, length=3.176, ppl=8.2, wps=49228.3, ups=0.84, wpb=58297.1, bsz=1987.4, num_updates=249500, lr=0.0002002, gnorm=1.457, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:04:20 | INFO | train_inner | epoch 127:    411 / 1978 loss=3.019, nll_loss=0.881, word_ins=2.704, length=3.149, ppl=8.11, wps=50219.7, ups=0.84, wpb=59511.7, bsz=2018.1, num_updates=249600, lr=0.00020016, gnorm=1.534, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:06:19 | INFO | train_inner | epoch 127:    511 / 1978 loss=3.019, nll_loss=0.885, word_ins=2.708, length=3.111, ppl=8.11, wps=50376.1, ups=0.84, wpb=59878.1, bsz=2067.7, num_updates=249700, lr=0.00020012, gnorm=1.531, loss_scale=2048, train_wall=119, wall=0
2023-01-12 07:08:17 | INFO | train_inner | epoch 127:    611 / 1978 loss=3.017, nll_loss=0.882, word_ins=2.706, length=3.111, ppl=8.1, wps=50094, ups=0.85, wpb=59077.7, bsz=2014.4, num_updates=249800, lr=0.00020008, gnorm=1.453, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:10:14 | INFO | train_inner | epoch 127:    711 / 1978 loss=3.026, nll_loss=0.887, word_ins=2.71, length=3.164, ppl=8.15, wps=50413.4, ups=0.85, wpb=59269.7, bsz=1976.3, num_updates=249900, lr=0.00020004, gnorm=1.477, loss_scale=2048, train_wall=117, wall=0
2023-01-12 07:13:48 | INFO | train_inner | epoch 127:    811 / 1978 loss=3.035, nll_loss=0.9, word_ins=2.722, length=3.128, ppl=8.2, wps=27754.9, ups=0.47, wpb=59354.3, bsz=2032.6, num_updates=250000, lr=0.0002, gnorm=1.55, loss_scale=2048, train_wall=214, wall=0
2023-01-12 07:15:47 | INFO | train_inner | epoch 127:    911 / 1978 loss=3.017, nll_loss=0.88, word_ins=2.703, length=3.141, ppl=8.1, wps=50217.8, ups=0.84, wpb=59644.8, bsz=2044.3, num_updates=250100, lr=0.00019996, gnorm=1.565, loss_scale=2048, train_wall=119, wall=0
2023-01-12 07:17:45 | INFO | train_inner | epoch 127:   1011 / 1978 loss=3.051, nll_loss=0.91, word_ins=2.731, length=3.204, ppl=8.29, wps=49480, ups=0.84, wpb=58580.5, bsz=1956.4, num_updates=250200, lr=0.00019992, gnorm=1.516, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:19:44 | INFO | train_inner | epoch 127:   1111 / 1978 loss=3.041, nll_loss=0.911, word_ins=2.731, length=3.097, ppl=8.23, wps=50472, ups=0.84, wpb=59791.1, bsz=1982.2, num_updates=250300, lr=0.00019988, gnorm=1.558, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:21:43 | INFO | train_inner | epoch 127:   1211 / 1978 loss=3.043, nll_loss=0.908, word_ins=2.729, length=3.144, ppl=8.24, wps=49855, ups=0.84, wpb=59312.7, bsz=1956.3, num_updates=250400, lr=0.00019984, gnorm=1.526, loss_scale=2048, train_wall=119, wall=0
2023-01-12 07:23:41 | INFO | train_inner | epoch 127:   1311 / 1978 loss=3.038, nll_loss=0.899, word_ins=2.721, length=3.167, ppl=8.21, wps=50171.5, ups=0.85, wpb=59237.6, bsz=2004.1, num_updates=250500, lr=0.0001998, gnorm=1.518, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:25:38 | INFO | train_inner | epoch 127:   1411 / 1978 loss=3.041, nll_loss=0.904, word_ins=2.725, length=3.155, ppl=8.23, wps=50505, ups=0.85, wpb=59357.6, bsz=1953.3, num_updates=250600, lr=0.00019976, gnorm=1.496, loss_scale=2048, train_wall=117, wall=0
2023-01-12 07:27:37 | INFO | train_inner | epoch 127:   1511 / 1978 loss=3.027, nll_loss=0.895, word_ins=2.717, length=3.103, ppl=8.15, wps=49699.4, ups=0.84, wpb=58920.1, bsz=2054.4, num_updates=250700, lr=0.000199721, gnorm=1.481, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:29:35 | INFO | train_inner | epoch 127:   1611 / 1978 loss=3.037, nll_loss=0.905, word_ins=2.726, length=3.106, ppl=8.21, wps=50398.7, ups=0.85, wpb=59603.3, bsz=1987.8, num_updates=250800, lr=0.000199681, gnorm=1.5, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:31:33 | INFO | train_inner | epoch 127:   1711 / 1978 loss=3.056, nll_loss=0.916, word_ins=2.736, length=3.201, ppl=8.32, wps=50041, ups=0.85, wpb=58917.4, bsz=1987, num_updates=250900, lr=0.000199641, gnorm=1.549, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:33:31 | INFO | train_inner | epoch 127:   1811 / 1978 loss=3.05, nll_loss=0.911, word_ins=2.732, length=3.178, ppl=8.28, wps=50328.5, ups=0.85, wpb=59555.4, bsz=1983.4, num_updates=251000, lr=0.000199601, gnorm=1.566, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:35:30 | INFO | train_inner | epoch 127:   1911 / 1978 loss=3.035, nll_loss=0.899, word_ins=2.721, length=3.139, ppl=8.19, wps=50338.2, ups=0.84, wpb=59754.6, bsz=1993.1, num_updates=251100, lr=0.000199561, gnorm=1.504, loss_scale=2048, train_wall=119, wall=0
2023-01-12 07:36:49 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 07:37:08 | INFO | valid | epoch 127 | valid on 'valid' subset | loss 4.485 | nll_loss 2.004 | word_ins 3.758 | length 7.273 | ppl 22.4 | wps 135862 | wpb 40242.5 | bsz 1500 | num_updates 251167 | best_loss 4.274
2023-01-12 07:37:08 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 07:37:10 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint127.pt (epoch 127 @ 251167 updates, score 4.485) (writing took 2.8089070077985525 seconds)
2023-01-12 07:37:10 | INFO | fairseq_cli.train | end of epoch 127 (average epoch stats below)
2023-01-12 07:37:10 | INFO | train | epoch 127 | loss 3.034 | nll_loss 0.897 | word_ins 2.719 | length 3.146 | ppl 8.19 | wps 47398.8 | ups 0.8 | wpb 59284.3 | bsz 2002.6 | num_updates 251167 | lr 0.000199535 | gnorm 1.519 | loss_scale 2048 | train_wall 2433 | wall 0
2023-01-12 07:37:10 | INFO | fairseq.trainer | begin training epoch 128
2023-01-12 07:38:07 | INFO | train_inner | epoch 128:     33 / 1978 loss=3.031, nll_loss=0.893, word_ins=2.715, length=3.16, ppl=8.18, wps=37596.7, ups=0.64, wpb=58972.4, bsz=1959.8, num_updates=251200, lr=0.000199522, gnorm=1.571, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:40:05 | INFO | train_inner | epoch 128:    133 / 1978 loss=3.02, nll_loss=0.887, word_ins=2.71, length=3.107, ppl=8.11, wps=50241.1, ups=0.84, wpb=59548.1, bsz=1990.9, num_updates=251300, lr=0.000199482, gnorm=1.511, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:42:03 | INFO | train_inner | epoch 128:    233 / 1978 loss=3.027, nll_loss=0.892, word_ins=2.714, length=3.127, ppl=8.15, wps=50254.5, ups=0.85, wpb=59257.2, bsz=2017.1, num_updates=251400, lr=0.000199442, gnorm=1.517, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:44:01 | INFO | train_inner | epoch 128:    333 / 1978 loss=3.046, nll_loss=0.905, word_ins=2.727, length=3.196, ppl=8.26, wps=49583.8, ups=0.85, wpb=58561, bsz=1996.6, num_updates=251500, lr=0.000199403, gnorm=1.532, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:45:59 | INFO | train_inner | epoch 128:    433 / 1978 loss=3.033, nll_loss=0.897, word_ins=2.719, length=3.138, ppl=8.18, wps=50473.1, ups=0.85, wpb=59398.1, bsz=1943.8, num_updates=251600, lr=0.000199363, gnorm=1.524, loss_scale=2048, train_wall=117, wall=0
2023-01-12 07:47:58 | INFO | train_inner | epoch 128:    533 / 1978 loss=2.999, nll_loss=0.862, word_ins=2.687, length=3.118, ppl=7.99, wps=50055.3, ups=0.84, wpb=59431.4, bsz=2108.4, num_updates=251700, lr=0.000199323, gnorm=1.482, loss_scale=2048, train_wall=119, wall=0
2023-01-12 07:49:56 | INFO | train_inner | epoch 128:    633 / 1978 loss=3.025, nll_loss=0.887, word_ins=2.709, length=3.156, ppl=8.14, wps=49911.4, ups=0.84, wpb=59144.8, bsz=1984.3, num_updates=251800, lr=0.000199284, gnorm=1.495, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:51:55 | INFO | train_inner | epoch 128:    733 / 1978 loss=3.044, nll_loss=0.904, word_ins=2.725, length=3.194, ppl=8.25, wps=50218, ups=0.84, wpb=59487.4, bsz=1908.6, num_updates=251900, lr=0.000199244, gnorm=1.535, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:53:53 | INFO | train_inner | epoch 128:    833 / 1978 loss=3.018, nll_loss=0.883, word_ins=2.706, length=3.12, ppl=8.1, wps=50118.4, ups=0.84, wpb=59406, bsz=2048.6, num_updates=252000, lr=0.000199205, gnorm=1.479, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:55:51 | INFO | train_inner | epoch 128:    933 / 1978 loss=3.043, nll_loss=0.905, word_ins=2.726, length=3.167, ppl=8.24, wps=50108.2, ups=0.85, wpb=59251, bsz=1973.8, num_updates=252100, lr=0.000199165, gnorm=1.524, loss_scale=2048, train_wall=118, wall=0
2023-01-12 07:57:50 | INFO | train_inner | epoch 128:   1033 / 1978 loss=3.051, nll_loss=0.911, word_ins=2.731, length=3.194, ppl=8.29, wps=49899.8, ups=0.85, wpb=59041.3, bsz=1970.2, num_updates=252200, lr=0.000199126, gnorm=1.587, loss_scale=4096, train_wall=118, wall=0
2023-01-12 07:59:48 | INFO | train_inner | epoch 128:   1133 / 1978 loss=3.037, nll_loss=0.906, word_ins=2.727, length=3.096, ppl=8.21, wps=50416.9, ups=0.85, wpb=59538.3, bsz=2033.2, num_updates=252300, lr=0.000199086, gnorm=1.532, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:01:47 | INFO | train_inner | epoch 128:   1233 / 1978 loss=3.03, nll_loss=0.894, word_ins=2.716, length=3.131, ppl=8.17, wps=49989, ups=0.84, wpb=59364.8, bsz=2010.6, num_updates=252400, lr=0.000199047, gnorm=1.551, loss_scale=4096, train_wall=119, wall=0
2023-01-12 08:03:45 | INFO | train_inner | epoch 128:   1333 / 1978 loss=3.023, nll_loss=0.888, word_ins=2.711, length=3.121, ppl=8.13, wps=50302.8, ups=0.85, wpb=59418.2, bsz=2040, num_updates=252500, lr=0.000199007, gnorm=1.492, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:05:44 | INFO | train_inner | epoch 128:   1433 / 1978 loss=3.034, nll_loss=0.897, word_ins=2.719, length=3.146, ppl=8.19, wps=50040, ups=0.84, wpb=59553.2, bsz=2051.3, num_updates=252600, lr=0.000198968, gnorm=1.552, loss_scale=4096, train_wall=119, wall=0
2023-01-12 08:07:41 | INFO | train_inner | epoch 128:   1533 / 1978 loss=3.074, nll_loss=0.927, word_ins=2.746, length=3.276, ppl=8.42, wps=49997.1, ups=0.85, wpb=58734.3, bsz=1889.4, num_updates=252700, lr=0.000198929, gnorm=1.588, loss_scale=4096, train_wall=117, wall=0
2023-01-12 08:09:40 | INFO | train_inner | epoch 128:   1633 / 1978 loss=3.033, nll_loss=0.898, word_ins=2.72, length=3.13, ppl=8.19, wps=49506.4, ups=0.84, wpb=58774.7, bsz=2048.1, num_updates=252800, lr=0.000198889, gnorm=1.528, loss_scale=4096, train_wall=119, wall=0
2023-01-12 08:11:38 | INFO | train_inner | epoch 128:   1733 / 1978 loss=3.03, nll_loss=0.889, word_ins=2.712, length=3.181, ppl=8.17, wps=50004, ups=0.85, wpb=59016.2, bsz=2018.9, num_updates=252900, lr=0.00019885, gnorm=1.49, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:13:36 | INFO | train_inner | epoch 128:   1833 / 1978 loss=3.043, nll_loss=0.902, word_ins=2.723, length=3.195, ppl=8.24, wps=50553.2, ups=0.85, wpb=59606.6, bsz=1926.2, num_updates=253000, lr=0.000198811, gnorm=1.584, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:15:35 | INFO | train_inner | epoch 128:   1933 / 1978 loss=3.032, nll_loss=0.897, word_ins=2.719, length=3.126, ppl=8.18, wps=50177, ups=0.84, wpb=59585.4, bsz=2053, num_updates=253100, lr=0.000198771, gnorm=1.442, loss_scale=4096, train_wall=119, wall=0
2023-01-12 08:16:29 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 08:16:48 | INFO | valid | epoch 128 | valid on 'valid' subset | loss 4.492 | nll_loss 2.007 | word_ins 3.764 | length 7.281 | ppl 22.5 | wps 88257.3 | wpb 40242.5 | bsz 1500 | num_updates 253145 | best_loss 4.274
2023-01-12 08:16:48 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 08:16:51 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint128.pt (epoch 128 @ 253145 updates, score 4.492) (writing took 2.8284064400941133 seconds)
2023-01-12 08:16:51 | INFO | fairseq_cli.train | end of epoch 128 (average epoch stats below)
2023-01-12 08:16:51 | INFO | train | epoch 128 | loss 3.032 | nll_loss 0.895 | word_ins 2.717 | length 3.15 | ppl 8.18 | wps 49262.5 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 253145 | lr 0.000198754 | gnorm 1.524 | loss_scale 4096 | train_wall 2337 | wall 0
2023-01-12 08:16:51 | INFO | fairseq.trainer | begin training epoch 129
2023-01-12 08:18:12 | INFO | train_inner | epoch 129:     55 / 1978 loss=2.982, nll_loss=0.854, word_ins=2.68, length=3.023, ppl=7.9, wps=37939.7, ups=0.64, wpb=59676.5, bsz=2120.2, num_updates=253200, lr=0.000198732, gnorm=1.478, loss_scale=4096, train_wall=119, wall=0
2023-01-12 08:20:10 | INFO | train_inner | epoch 129:    155 / 1978 loss=3.011, nll_loss=0.877, word_ins=2.701, length=3.092, ppl=8.06, wps=50117.3, ups=0.85, wpb=59269.2, bsz=2009.4, num_updates=253300, lr=0.000198693, gnorm=1.56, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:22:08 | INFO | train_inner | epoch 129:    255 / 1978 loss=3.051, nll_loss=0.914, word_ins=2.735, length=3.17, ppl=8.29, wps=50065.7, ups=0.85, wpb=58917.2, bsz=1933, num_updates=253400, lr=0.000198654, gnorm=1.594, loss_scale=4096, train_wall=117, wall=0
2023-01-12 08:24:06 | INFO | train_inner | epoch 129:    355 / 1978 loss=3.019, nll_loss=0.883, word_ins=2.706, length=3.132, ppl=8.11, wps=49430.9, ups=0.84, wpb=58580.8, bsz=2051.2, num_updates=253500, lr=0.000198615, gnorm=1.409, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:26:05 | INFO | train_inner | epoch 129:    455 / 1978 loss=3.034, nll_loss=0.898, word_ins=2.72, length=3.142, ppl=8.19, wps=49433.6, ups=0.84, wpb=58551.4, bsz=2027, num_updates=253600, lr=0.000198575, gnorm=1.506, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:28:03 | INFO | train_inner | epoch 129:    555 / 1978 loss=3.048, nll_loss=0.911, word_ins=2.732, length=3.158, ppl=8.27, wps=50051.5, ups=0.85, wpb=59069.9, bsz=1936.5, num_updates=253700, lr=0.000198536, gnorm=1.5, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:30:02 | INFO | train_inner | epoch 129:    655 / 1978 loss=3.013, nll_loss=0.875, word_ins=2.699, length=3.143, ppl=8.07, wps=49906.3, ups=0.84, wpb=59538.4, bsz=2041.4, num_updates=253800, lr=0.000198497, gnorm=1.504, loss_scale=4096, train_wall=119, wall=0
2023-01-12 08:32:01 | INFO | train_inner | epoch 129:    755 / 1978 loss=3.023, nll_loss=0.887, word_ins=2.71, length=3.138, ppl=8.13, wps=50344.9, ups=0.84, wpb=59632.1, bsz=1971.5, num_updates=253900, lr=0.000198458, gnorm=1.564, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:33:59 | INFO | train_inner | epoch 129:    855 / 1978 loss=3.039, nll_loss=0.898, word_ins=2.72, length=3.196, ppl=8.22, wps=50331.8, ups=0.85, wpb=59414.5, bsz=1970.9, num_updates=254000, lr=0.000198419, gnorm=1.514, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:35:57 | INFO | train_inner | epoch 129:    955 / 1978 loss=3.04, nll_loss=0.903, word_ins=2.724, length=3.157, ppl=8.22, wps=50098.9, ups=0.84, wpb=59331.6, bsz=1984.5, num_updates=254100, lr=0.00019838, gnorm=1.523, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:37:56 | INFO | train_inner | epoch 129:   1055 / 1978 loss=3.047, nll_loss=0.91, word_ins=2.731, length=3.154, ppl=8.26, wps=49636.8, ups=0.84, wpb=59069.6, bsz=1953, num_updates=254200, lr=0.000198341, gnorm=1.494, loss_scale=4096, train_wall=119, wall=0
2023-01-12 08:39:54 | INFO | train_inner | epoch 129:   1155 / 1978 loss=3.029, nll_loss=0.892, word_ins=2.714, length=3.153, ppl=8.16, wps=50345.7, ups=0.85, wpb=59419.7, bsz=1992.2, num_updates=254300, lr=0.000198302, gnorm=1.59, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:41:54 | INFO | train_inner | epoch 129:   1255 / 1978 loss=3.013, nll_loss=0.883, word_ins=2.706, length=3.076, ppl=8.07, wps=49974.9, ups=0.84, wpb=59769, bsz=2083.5, num_updates=254400, lr=0.000198263, gnorm=1.518, loss_scale=4096, train_wall=119, wall=0
2023-01-12 08:43:51 | INFO | train_inner | epoch 129:   1355 / 1978 loss=3.033, nll_loss=0.896, word_ins=2.718, length=3.148, ppl=8.18, wps=50754.2, ups=0.85, wpb=59797.7, bsz=2052.2, num_updates=254500, lr=0.000198224, gnorm=1.554, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:45:50 | INFO | train_inner | epoch 129:   1455 / 1978 loss=3.034, nll_loss=0.895, word_ins=2.717, length=3.16, ppl=8.19, wps=50216.9, ups=0.84, wpb=59582, bsz=2014.7, num_updates=254600, lr=0.000198185, gnorm=1.562, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:47:49 | INFO | train_inner | epoch 129:   1555 / 1978 loss=3.022, nll_loss=0.886, word_ins=2.709, length=3.136, ppl=8.12, wps=49708.6, ups=0.84, wpb=59207.2, bsz=2086.9, num_updates=254700, lr=0.000198146, gnorm=1.523, loss_scale=4096, train_wall=119, wall=0
2023-01-12 08:49:48 | INFO | train_inner | epoch 129:   1655 / 1978 loss=3.029, nll_loss=0.897, word_ins=2.719, length=3.102, ppl=8.16, wps=49911.3, ups=0.84, wpb=59381.8, bsz=2099.6, num_updates=254800, lr=0.000198107, gnorm=1.498, loss_scale=4096, train_wall=119, wall=0
2023-01-12 08:51:46 | INFO | train_inner | epoch 129:   1755 / 1978 loss=3.034, nll_loss=0.896, word_ins=2.718, length=3.164, ppl=8.19, wps=50208.5, ups=0.85, wpb=59346.7, bsz=1974.2, num_updates=254900, lr=0.000198068, gnorm=1.545, loss_scale=4096, train_wall=118, wall=0
2023-01-12 08:53:44 | INFO | train_inner | epoch 129:   1855 / 1978 loss=3.062, nll_loss=0.921, word_ins=2.741, length=3.209, ppl=8.35, wps=50602, ups=0.85, wpb=59499.3, bsz=1922.7, num_updates=255000, lr=0.00019803, gnorm=1.59, loss_scale=4096, train_wall=117, wall=0
2023-01-12 08:55:42 | INFO | train_inner | epoch 129:   1955 / 1978 loss=3.051, nll_loss=0.91, word_ins=2.731, length=3.201, ppl=8.29, wps=50154.1, ups=0.85, wpb=59008.5, bsz=1915.5, num_updates=255100, lr=0.000197991, gnorm=1.556, loss_scale=4096, train_wall=117, wall=0
2023-01-12 08:56:09 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 08:56:24 | INFO | valid | epoch 129 | valid on 'valid' subset | loss 4.445 | nll_loss 1.985 | word_ins 3.745 | length 7.002 | ppl 21.78 | wps 156495 | wpb 40242.5 | bsz 1500 | num_updates 255123 | best_loss 4.274
2023-01-12 08:56:24 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 08:56:27 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint129.pt (epoch 129 @ 255123 updates, score 4.445) (writing took 3.0918129412457347 seconds)
2023-01-12 08:56:27 | INFO | fairseq_cli.train | end of epoch 129 (average epoch stats below)
2023-01-12 08:56:27 | INFO | train | epoch 129 | loss 3.032 | nll_loss 0.895 | word_ins 2.717 | length 3.145 | ppl 8.18 | wps 49340.9 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 255123 | lr 0.000197982 | gnorm 1.529 | loss_scale 4096 | train_wall 2338 | wall 0
2023-01-12 08:56:27 | INFO | fairseq.trainer | begin training epoch 130
2023-01-12 08:58:14 | INFO | train_inner | epoch 130:     77 / 1978 loss=3.028, nll_loss=0.893, word_ins=2.715, length=3.129, ppl=8.16, wps=38571.5, ups=0.66, wpb=58577.4, bsz=1976.4, num_updates=255200, lr=0.000197952, gnorm=1.558, loss_scale=4096, train_wall=118, wall=0
2023-01-12 09:00:12 | INFO | train_inner | epoch 130:    177 / 1978 loss=3.019, nll_loss=0.882, word_ins=2.706, length=3.137, ppl=8.11, wps=49978.9, ups=0.85, wpb=59034.4, bsz=2035.4, num_updates=255300, lr=0.000197913, gnorm=1.453, loss_scale=4096, train_wall=118, wall=0
2023-01-12 09:02:10 | INFO | train_inner | epoch 130:    277 / 1978 loss=3.024, nll_loss=0.892, word_ins=2.715, length=3.092, ppl=8.13, wps=50321.6, ups=0.84, wpb=59614.9, bsz=1953.7, num_updates=255400, lr=0.000197874, gnorm=1.563, loss_scale=4096, train_wall=118, wall=0
2023-01-12 09:04:10 | INFO | train_inner | epoch 130:    377 / 1978 loss=3.024, nll_loss=0.889, word_ins=2.711, length=3.124, ppl=8.13, wps=49700.8, ups=0.84, wpb=59366.9, bsz=2065.9, num_updates=255500, lr=0.000197836, gnorm=1.521, loss_scale=4096, train_wall=119, wall=0
2023-01-12 09:06:08 | INFO | train_inner | epoch 130:    477 / 1978 loss=3.039, nll_loss=0.901, word_ins=2.723, length=3.158, ppl=8.22, wps=49989.9, ups=0.85, wpb=59109.2, bsz=2017, num_updates=255600, lr=0.000197797, gnorm=1.539, loss_scale=4096, train_wall=118, wall=0
2023-01-12 09:08:07 | INFO | train_inner | epoch 130:    577 / 1978 loss=3.003, nll_loss=0.87, word_ins=2.694, length=3.087, ppl=8.02, wps=50020.1, ups=0.84, wpb=59728, bsz=2063, num_updates=255700, lr=0.000197758, gnorm=1.465, loss_scale=4096, train_wall=119, wall=0
2023-01-12 09:10:07 | INFO | train_inner | epoch 130:    677 / 1978 loss=3.014, nll_loss=0.879, word_ins=2.703, length=3.11, ppl=8.08, wps=49546.1, ups=0.84, wpb=59222.9, bsz=2090.1, num_updates=255800, lr=0.00019772, gnorm=1.504, loss_scale=4096, train_wall=119, wall=0
2023-01-12 09:12:04 | INFO | train_inner | epoch 130:    777 / 1978 loss=3.041, nll_loss=0.902, word_ins=2.724, length=3.167, ppl=8.23, wps=49699.8, ups=0.85, wpb=58464.8, bsz=1980.2, num_updates=255900, lr=0.000197681, gnorm=1.515, loss_scale=4096, train_wall=117, wall=0
2023-01-12 09:14:03 | INFO | train_inner | epoch 130:    877 / 1978 loss=3.035, nll_loss=0.905, word_ins=2.726, length=3.093, ppl=8.2, wps=50042, ups=0.84, wpb=59388.2, bsz=2003.1, num_updates=256000, lr=0.000197642, gnorm=1.504, loss_scale=4096, train_wall=118, wall=0
2023-01-12 09:16:01 | INFO | train_inner | epoch 130:    977 / 1978 loss=3.022, nll_loss=0.887, word_ins=2.709, length=3.13, ppl=8.13, wps=50663.6, ups=0.85, wpb=59742.6, bsz=1969.8, num_updates=256100, lr=0.000197604, gnorm=1.56, loss_scale=4096, train_wall=118, wall=0
2023-01-12 09:17:59 | INFO | train_inner | epoch 130:   1077 / 1978 loss=3.036, nll_loss=0.901, word_ins=2.722, length=3.14, ppl=8.2, wps=49906.9, ups=0.85, wpb=59026.5, bsz=2005.2, num_updates=256200, lr=0.000197565, gnorm=1.53, loss_scale=4096, train_wall=118, wall=0
2023-01-12 09:19:58 | INFO | train_inner | epoch 130:   1177 / 1978 loss=3.033, nll_loss=0.899, word_ins=2.72, length=3.128, ppl=8.19, wps=49691.7, ups=0.84, wpb=58825.5, bsz=2084, num_updates=256300, lr=0.000197527, gnorm=1.501, loss_scale=8192, train_wall=118, wall=0
2023-01-12 09:21:56 | INFO | train_inner | epoch 130:   1277 / 1978 loss=3.043, nll_loss=0.906, word_ins=2.727, length=3.151, ppl=8.24, wps=50616.3, ups=0.85, wpb=59864.4, bsz=1932, num_updates=256400, lr=0.000197488, gnorm=1.586, loss_scale=8192, train_wall=118, wall=0
2023-01-12 09:23:54 | INFO | train_inner | epoch 130:   1377 / 1978 loss=3.034, nll_loss=0.898, word_ins=2.719, length=3.147, ppl=8.19, wps=50500.1, ups=0.85, wpb=59484.1, bsz=1980.2, num_updates=256500, lr=0.00019745, gnorm=1.564, loss_scale=8192, train_wall=118, wall=0
2023-01-12 09:25:52 | INFO | train_inner | epoch 130:   1477 / 1978 loss=3.053, nll_loss=0.911, word_ins=2.732, length=3.21, ppl=8.3, wps=50368.5, ups=0.85, wpb=59399.9, bsz=1935.9, num_updates=256600, lr=0.000197411, gnorm=1.606, loss_scale=8192, train_wall=118, wall=0
2023-01-12 09:27:49 | INFO | train_inner | epoch 130:   1577 / 1978 loss=3.039, nll_loss=0.901, word_ins=2.723, length=3.157, ppl=8.22, wps=50433.7, ups=0.85, wpb=59431.8, bsz=1961.8, num_updates=256700, lr=0.000197373, gnorm=1.545, loss_scale=8192, train_wall=118, wall=0
2023-01-12 09:29:47 | INFO | train_inner | epoch 130:   1677 / 1978 loss=3.025, nll_loss=0.889, word_ins=2.712, length=3.138, ppl=8.14, wps=50397.9, ups=0.85, wpb=59399.8, bsz=1983, num_updates=256800, lr=0.000197334, gnorm=1.543, loss_scale=8192, train_wall=118, wall=0
2023-01-12 09:31:45 | INFO | train_inner | epoch 130:   1777 / 1978 loss=3.043, nll_loss=0.902, word_ins=2.723, length=3.193, ppl=8.24, wps=50209.2, ups=0.85, wpb=59193, bsz=1947.8, num_updates=256900, lr=0.000197296, gnorm=1.501, loss_scale=8192, train_wall=118, wall=0
2023-01-12 09:33:43 | INFO | train_inner | epoch 130:   1877 / 1978 loss=3.021, nll_loss=0.882, word_ins=2.705, length=3.155, ppl=8.11, wps=50481, ups=0.85, wpb=59700.5, bsz=2043.6, num_updates=257000, lr=0.000197257, gnorm=1.535, loss_scale=8192, train_wall=118, wall=0
2023-01-12 09:35:42 | INFO | train_inner | epoch 130:   1977 / 1978 loss=3.025, nll_loss=0.889, word_ins=2.711, length=3.136, ppl=8.14, wps=50160.6, ups=0.85, wpb=59236.4, bsz=2003.6, num_updates=257100, lr=0.000197219, gnorm=1.523, loss_scale=8192, train_wall=118, wall=0
2023-01-12 09:35:43 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 09:36:02 | INFO | valid | epoch 130 | valid on 'valid' subset | loss 4.514 | nll_loss 2.011 | word_ins 3.767 | length 7.48 | ppl 22.85 | wps 135034 | wpb 40242.5 | bsz 1500 | num_updates 257101 | best_loss 4.274
2023-01-12 09:36:02 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 09:36:05 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint130.pt (epoch 130 @ 257101 updates, score 4.514) (writing took 3.0488498262129724 seconds)
2023-01-12 09:36:05 | INFO | fairseq_cli.train | end of epoch 130 (average epoch stats below)
2023-01-12 09:36:05 | INFO | train | epoch 130 | loss 3.03 | nll_loss 0.894 | word_ins 2.716 | length 3.14 | ppl 8.17 | wps 49320.3 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 257101 | lr 0.000197219 | gnorm 1.531 | loss_scale 8192 | train_wall 2337 | wall 0
2023-01-12 09:36:05 | INFO | fairseq.trainer | begin training epoch 131
2023-01-12 09:38:17 | INFO | train_inner | epoch 131:     99 / 1978 loss=3.042, nll_loss=0.905, word_ins=2.727, length=3.147, ppl=8.23, wps=37783, ups=0.64, wpb=58829.5, bsz=1927.3, num_updates=257200, lr=0.000197181, gnorm=1.528, loss_scale=8192, train_wall=118, wall=0
2023-01-12 09:40:17 | INFO | train_inner | epoch 131:    199 / 1978 loss=3.02, nll_loss=0.887, word_ins=2.71, length=3.107, ppl=8.11, wps=49550.7, ups=0.83, wpb=59344.3, bsz=2020.6, num_updates=257300, lr=0.000197142, gnorm=1.492, loss_scale=8192, train_wall=120, wall=0
2023-01-12 09:42:15 | INFO | train_inner | epoch 131:    299 / 1978 loss=3.017, nll_loss=0.881, word_ins=2.704, length=3.125, ppl=8.09, wps=50517.5, ups=0.85, wpb=59713.4, bsz=1955.4, num_updates=257400, lr=0.000197104, gnorm=1.561, loss_scale=8192, train_wall=118, wall=0
2023-01-12 09:44:14 | INFO | train_inner | epoch 131:    399 / 1978 loss=3.026, nll_loss=0.89, word_ins=2.712, length=3.138, ppl=8.15, wps=49852.9, ups=0.84, wpb=59304.7, bsz=2063.5, num_updates=257500, lr=0.000197066, gnorm=1.533, loss_scale=8192, train_wall=119, wall=0
2023-01-12 09:46:13 | INFO | train_inner | epoch 131:    499 / 1978 loss=2.997, nll_loss=0.867, word_ins=2.693, length=3.048, ppl=7.99, wps=49886.7, ups=0.84, wpb=59373.7, bsz=2079.9, num_updates=257600, lr=0.000197028, gnorm=1.503, loss_scale=8192, train_wall=119, wall=0
2023-01-12 09:48:12 | INFO | train_inner | epoch 131:    599 / 1978 loss=3.024, nll_loss=0.887, word_ins=2.71, length=3.14, ppl=8.13, wps=49678.8, ups=0.84, wpb=58881.9, bsz=2040, num_updates=257700, lr=0.000196989, gnorm=1.567, loss_scale=8192, train_wall=118, wall=0
2023-01-12 09:49:01 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2023-01-12 09:50:11 | INFO | train_inner | epoch 131:    700 / 1978 loss=3.043, nll_loss=0.907, word_ins=2.728, length=3.149, ppl=8.24, wps=49829.9, ups=0.84, wpb=59322.3, bsz=1937.2, num_updates=257800, lr=0.000196951, gnorm=1.55, loss_scale=4096, train_wall=119, wall=0
2023-01-12 09:52:09 | INFO | train_inner | epoch 131:    800 / 1978 loss=3.012, nll_loss=0.878, word_ins=2.702, length=3.105, ppl=8.07, wps=50180.8, ups=0.85, wpb=59362.1, bsz=2067.2, num_updates=257900, lr=0.000196913, gnorm=1.506, loss_scale=4096, train_wall=118, wall=0
2023-01-12 09:54:08 | INFO | train_inner | epoch 131:    900 / 1978 loss=3.012, nll_loss=0.878, word_ins=2.702, length=3.102, ppl=8.07, wps=50046.4, ups=0.84, wpb=59325.6, bsz=2110.6, num_updates=258000, lr=0.000196875, gnorm=1.551, loss_scale=4096, train_wall=118, wall=0
2023-01-12 09:56:06 | INFO | train_inner | epoch 131:   1000 / 1978 loss=3.037, nll_loss=0.899, word_ins=2.721, length=3.161, ppl=8.21, wps=50118.9, ups=0.85, wpb=59243.7, bsz=1975.1, num_updates=258100, lr=0.000196837, gnorm=1.5, loss_scale=4096, train_wall=118, wall=0
2023-01-12 09:58:05 | INFO | train_inner | epoch 131:   1100 / 1978 loss=3.026, nll_loss=0.891, word_ins=2.713, length=3.133, ppl=8.15, wps=50011, ups=0.84, wpb=59645.8, bsz=2036.5, num_updates=258200, lr=0.000196799, gnorm=1.505, loss_scale=4096, train_wall=119, wall=0
2023-01-12 10:00:03 | INFO | train_inner | epoch 131:   1200 / 1978 loss=3.057, nll_loss=0.919, word_ins=2.739, length=3.174, ppl=8.32, wps=49503.4, ups=0.85, wpb=58474.6, bsz=1944.4, num_updates=258300, lr=0.00019676, gnorm=1.491, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:02:01 | INFO | train_inner | epoch 131:   1300 / 1978 loss=3.042, nll_loss=0.903, word_ins=2.725, length=3.172, ppl=8.24, wps=50193, ups=0.85, wpb=59236, bsz=1966.4, num_updates=258400, lr=0.000196722, gnorm=1.6, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:04:00 | INFO | train_inner | epoch 131:   1400 / 1978 loss=3.023, nll_loss=0.889, word_ins=2.711, length=3.115, ppl=8.13, wps=50116.2, ups=0.84, wpb=59319.3, bsz=2066.2, num_updates=258500, lr=0.000196684, gnorm=1.533, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:05:59 | INFO | train_inner | epoch 131:   1500 / 1978 loss=3.021, nll_loss=0.89, word_ins=2.712, length=3.093, ppl=8.12, wps=49998.9, ups=0.83, wpb=59933, bsz=2050.3, num_updates=258600, lr=0.000196646, gnorm=1.521, loss_scale=4096, train_wall=120, wall=0
2023-01-12 10:07:57 | INFO | train_inner | epoch 131:   1600 / 1978 loss=3.055, nll_loss=0.911, word_ins=2.731, length=3.239, ppl=8.31, wps=50249.7, ups=0.85, wpb=58857.1, bsz=1893.8, num_updates=258700, lr=0.000196608, gnorm=1.546, loss_scale=4096, train_wall=117, wall=0
2023-01-12 10:09:54 | INFO | train_inner | epoch 131:   1700 / 1978 loss=3.03, nll_loss=0.89, word_ins=2.712, length=3.176, ppl=8.17, wps=50383.9, ups=0.85, wpb=59236.5, bsz=1980.9, num_updates=258800, lr=0.00019657, gnorm=1.531, loss_scale=4096, train_wall=117, wall=0
2023-01-12 10:11:51 | INFO | train_inner | epoch 131:   1800 / 1978 loss=3.032, nll_loss=0.896, word_ins=2.718, length=3.14, ppl=8.18, wps=50568.2, ups=0.85, wpb=59242, bsz=1950.5, num_updates=258900, lr=0.000196532, gnorm=1.482, loss_scale=4096, train_wall=117, wall=0
2023-01-12 10:13:50 | INFO | train_inner | epoch 131:   1900 / 1978 loss=3.02, nll_loss=0.882, word_ins=2.705, length=3.143, ppl=8.11, wps=50267.9, ups=0.85, wpb=59466.5, bsz=1997.2, num_updates=259000, lr=0.000196494, gnorm=1.523, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:15:23 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 10:15:43 | INFO | valid | epoch 131 | valid on 'valid' subset | loss 4.414 | nll_loss 1.988 | word_ins 3.747 | length 6.675 | ppl 21.32 | wps 172050 | wpb 40242.5 | bsz 1500 | num_updates 259078 | best_loss 4.274
2023-01-12 10:15:43 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 10:15:46 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint131.pt (epoch 131 @ 259078 updates, score 4.414) (writing took 2.870853102300316 seconds)
2023-01-12 10:15:46 | INFO | fairseq_cli.train | end of epoch 131 (average epoch stats below)
2023-01-12 10:15:46 | INFO | train | epoch 131 | loss 3.029 | nll_loss 0.893 | word_ins 2.715 | length 3.137 | ppl 8.16 | wps 49231.8 | ups 0.83 | wpb 59290.8 | bsz 2002.7 | num_updates 259078 | lr 0.000196465 | gnorm 1.531 | loss_scale 4096 | train_wall 2339 | wall 0
2023-01-12 10:15:46 | INFO | fairseq.trainer | begin training epoch 132
2023-01-12 10:16:30 | INFO | train_inner | epoch 132:     22 / 1978 loss=3.042, nll_loss=0.908, word_ins=2.728, length=3.139, ppl=8.24, wps=37057.3, ups=0.62, wpb=59331.8, bsz=1956, num_updates=259100, lr=0.000196456, gnorm=1.59, loss_scale=4096, train_wall=120, wall=0
2023-01-12 10:18:28 | INFO | train_inner | epoch 132:    122 / 1978 loss=3.023, nll_loss=0.89, word_ins=2.713, length=3.1, ppl=8.13, wps=49899, ups=0.84, wpb=59100.7, bsz=2044.5, num_updates=259200, lr=0.000196419, gnorm=1.506, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:20:26 | INFO | train_inner | epoch 132:    222 / 1978 loss=3.026, nll_loss=0.887, word_ins=2.71, length=3.159, ppl=8.14, wps=49934.1, ups=0.85, wpb=58639.1, bsz=1990.5, num_updates=259300, lr=0.000196381, gnorm=1.505, loss_scale=4096, train_wall=117, wall=0
2023-01-12 10:22:23 | INFO | train_inner | epoch 132:    322 / 1978 loss=3.048, nll_loss=0.909, word_ins=2.73, length=3.182, ppl=8.27, wps=50552.5, ups=0.85, wpb=59494.3, bsz=1907.6, num_updates=259400, lr=0.000196343, gnorm=1.587, loss_scale=4096, train_wall=117, wall=0
2023-01-12 10:24:23 | INFO | train_inner | epoch 132:    422 / 1978 loss=2.987, nll_loss=0.859, word_ins=2.685, length=3.017, ppl=7.93, wps=49793.8, ups=0.84, wpb=59627.5, bsz=2155.2, num_updates=259500, lr=0.000196305, gnorm=1.448, loss_scale=4096, train_wall=120, wall=0
2023-01-12 10:26:22 | INFO | train_inner | epoch 132:    522 / 1978 loss=3.036, nll_loss=0.901, word_ins=2.722, length=3.135, ppl=8.2, wps=49762.1, ups=0.84, wpb=59303.6, bsz=2010.9, num_updates=259600, lr=0.000196267, gnorm=1.564, loss_scale=4096, train_wall=119, wall=0
2023-01-12 10:28:21 | INFO | train_inner | epoch 132:    622 / 1978 loss=3.027, nll_loss=0.888, word_ins=2.711, length=3.167, ppl=8.15, wps=50115.2, ups=0.84, wpb=59309.7, bsz=1994.5, num_updates=259700, lr=0.000196229, gnorm=1.547, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:30:19 | INFO | train_inner | epoch 132:    722 / 1978 loss=3.02, nll_loss=0.885, word_ins=2.708, length=3.124, ppl=8.11, wps=49621.9, ups=0.84, wpb=58909.6, bsz=2025.5, num_updates=259800, lr=0.000196192, gnorm=1.555, loss_scale=4096, train_wall=119, wall=0
2023-01-12 10:32:19 | INFO | train_inner | epoch 132:    822 / 1978 loss=3.015, nll_loss=0.884, word_ins=2.708, length=3.07, ppl=8.08, wps=49726.2, ups=0.84, wpb=59336.8, bsz=2073.4, num_updates=259900, lr=0.000196154, gnorm=1.516, loss_scale=4096, train_wall=119, wall=0
2023-01-12 10:34:16 | INFO | train_inner | epoch 132:    922 / 1978 loss=3.024, nll_loss=0.888, word_ins=2.71, length=3.145, ppl=8.14, wps=50522.3, ups=0.85, wpb=59489.8, bsz=1949.5, num_updates=260000, lr=0.000196116, gnorm=1.546, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:36:15 | INFO | train_inner | epoch 132:   1022 / 1978 loss=3.026, nll_loss=0.885, word_ins=2.708, length=3.178, ppl=8.14, wps=50357.6, ups=0.85, wpb=59470.5, bsz=2005.2, num_updates=260100, lr=0.000196078, gnorm=1.52, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:38:12 | INFO | train_inner | epoch 132:   1122 / 1978 loss=3.048, nll_loss=0.906, word_ins=2.728, length=3.202, ppl=8.27, wps=50045.9, ups=0.85, wpb=58944.7, bsz=1909.6, num_updates=260200, lr=0.000196041, gnorm=1.508, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:40:11 | INFO | train_inner | epoch 132:   1222 / 1978 loss=3.034, nll_loss=0.898, word_ins=2.72, length=3.143, ppl=8.19, wps=50139.3, ups=0.84, wpb=59405.8, bsz=2049.4, num_updates=260300, lr=0.000196003, gnorm=1.543, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:42:09 | INFO | train_inner | epoch 132:   1322 / 1978 loss=3.022, nll_loss=0.891, word_ins=2.714, length=3.085, ppl=8.13, wps=49997.6, ups=0.85, wpb=59162.6, bsz=2056.2, num_updates=260400, lr=0.000195965, gnorm=1.518, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:44:07 | INFO | train_inner | epoch 132:   1422 / 1978 loss=3.059, nll_loss=0.92, word_ins=2.739, length=3.198, ppl=8.34, wps=50429, ups=0.85, wpb=59263.4, bsz=1850.9, num_updates=260500, lr=0.000195928, gnorm=1.594, loss_scale=4096, train_wall=117, wall=0
2023-01-12 10:46:04 | INFO | train_inner | epoch 132:   1522 / 1978 loss=3.035, nll_loss=0.895, word_ins=2.717, length=3.179, ppl=8.2, wps=50451.5, ups=0.85, wpb=59399.6, bsz=1977.5, num_updates=260600, lr=0.00019589, gnorm=1.56, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:48:03 | INFO | train_inner | epoch 132:   1622 / 1978 loss=3.011, nll_loss=0.875, word_ins=2.699, length=3.119, ppl=8.06, wps=50320.9, ups=0.84, wpb=59598.5, bsz=2037.8, num_updates=260700, lr=0.000195853, gnorm=1.505, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:50:01 | INFO | train_inner | epoch 132:   1722 / 1978 loss=3.032, nll_loss=0.896, word_ins=2.718, length=3.136, ppl=8.18, wps=50116.3, ups=0.84, wpb=59370, bsz=1989.2, num_updates=260800, lr=0.000195815, gnorm=1.565, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:52:00 | INFO | train_inner | epoch 132:   1822 / 1978 loss=3.037, nll_loss=0.9, word_ins=2.721, length=3.16, ppl=8.21, wps=50505.1, ups=0.85, wpb=59740.7, bsz=1955.8, num_updates=260900, lr=0.000195778, gnorm=1.554, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:53:58 | INFO | train_inner | epoch 132:   1922 / 1978 loss=3.012, nll_loss=0.879, word_ins=2.702, length=3.092, ppl=8.06, wps=50268.7, ups=0.85, wpb=59467.4, bsz=2052.7, num_updates=261000, lr=0.00019574, gnorm=1.503, loss_scale=4096, train_wall=118, wall=0
2023-01-12 10:55:04 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 10:55:22 | INFO | valid | epoch 132 | valid on 'valid' subset | loss 4.445 | nll_loss 1.989 | word_ins 3.746 | length 6.997 | ppl 21.79 | wps 195320 | wpb 40242.5 | bsz 1500 | num_updates 261056 | best_loss 4.274
2023-01-12 10:55:22 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 10:55:25 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint132.pt (epoch 132 @ 261056 updates, score 4.445) (writing took 2.782586927060038 seconds)
2023-01-12 10:55:25 | INFO | fairseq_cli.train | end of epoch 132 (average epoch stats below)
2023-01-12 10:55:25 | INFO | train | epoch 132 | loss 3.028 | nll_loss 0.892 | word_ins 2.714 | length 3.135 | ppl 8.15 | wps 49300.3 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 261056 | lr 0.000195719 | gnorm 1.535 | loss_scale 4096 | train_wall 2337 | wall 0
2023-01-12 10:55:25 | INFO | fairseq.trainer | begin training epoch 133
2023-01-12 10:56:34 | INFO | train_inner | epoch 133:     44 / 1978 loss=3.015, nll_loss=0.886, word_ins=2.709, length=3.059, ppl=8.08, wps=37622.9, ups=0.64, wpb=58840.7, bsz=2050.5, num_updates=261100, lr=0.000195703, gnorm=1.568, loss_scale=4096, train_wall=119, wall=0
2023-01-12 10:58:33 | INFO | train_inner | epoch 133:    144 / 1978 loss=3.028, nll_loss=0.889, word_ins=2.713, length=3.156, ppl=8.16, wps=49510.5, ups=0.84, wpb=58725.8, bsz=2011.7, num_updates=261200, lr=0.000195665, gnorm=1.576, loss_scale=4096, train_wall=118, wall=0
2023-01-12 11:00:32 | INFO | train_inner | epoch 133:    244 / 1978 loss=3.027, nll_loss=0.888, word_ins=2.711, length=3.158, ppl=8.15, wps=49805.8, ups=0.84, wpb=59114.9, bsz=2005.2, num_updates=261300, lr=0.000195628, gnorm=1.566, loss_scale=4096, train_wall=119, wall=0
2023-01-12 11:02:29 | INFO | train_inner | epoch 133:    344 / 1978 loss=3.037, nll_loss=0.896, word_ins=2.718, length=3.193, ppl=8.21, wps=50902.1, ups=0.85, wpb=59877.7, bsz=1937, num_updates=261400, lr=0.00019559, gnorm=1.571, loss_scale=4096, train_wall=117, wall=0
2023-01-12 11:04:28 | INFO | train_inner | epoch 133:    444 / 1978 loss=3.01, nll_loss=0.879, word_ins=2.702, length=3.078, ppl=8.06, wps=50284.3, ups=0.84, wpb=59721.1, bsz=2068.2, num_updates=261500, lr=0.000195553, gnorm=1.501, loss_scale=4096, train_wall=119, wall=0
2023-01-12 11:06:27 | INFO | train_inner | epoch 133:    544 / 1978 loss=3.017, nll_loss=0.882, word_ins=2.705, length=3.111, ppl=8.09, wps=49240.5, ups=0.84, wpb=58714.5, bsz=2089, num_updates=261600, lr=0.000195515, gnorm=1.543, loss_scale=4096, train_wall=119, wall=0
2023-01-12 11:08:25 | INFO | train_inner | epoch 133:    644 / 1978 loss=3.033, nll_loss=0.899, word_ins=2.721, length=3.118, ppl=8.18, wps=50588.5, ups=0.85, wpb=59541.3, bsz=1993, num_updates=261700, lr=0.000195478, gnorm=1.532, loss_scale=4096, train_wall=118, wall=0
2023-01-12 11:10:24 | INFO | train_inner | epoch 133:    744 / 1978 loss=3.015, nll_loss=0.879, word_ins=2.702, length=3.123, ppl=8.08, wps=49888.9, ups=0.84, wpb=59322, bsz=2087.2, num_updates=261800, lr=0.000195441, gnorm=1.567, loss_scale=4096, train_wall=119, wall=0
2023-01-12 11:12:22 | INFO | train_inner | epoch 133:    844 / 1978 loss=3.028, nll_loss=0.893, word_ins=2.715, length=3.13, ppl=8.16, wps=50310.3, ups=0.85, wpb=59454.9, bsz=1975.6, num_updates=261900, lr=0.000195403, gnorm=1.568, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:14:19 | INFO | train_inner | epoch 133:    944 / 1978 loss=3.062, nll_loss=0.917, word_ins=2.737, length=3.248, ppl=8.35, wps=49962, ups=0.85, wpb=58646.5, bsz=1905, num_updates=262000, lr=0.000195366, gnorm=1.655, loss_scale=8192, train_wall=117, wall=0
2023-01-12 11:16:18 | INFO | train_inner | epoch 133:   1044 / 1978 loss=3.019, nll_loss=0.883, word_ins=2.706, length=3.132, ppl=8.11, wps=50202.7, ups=0.84, wpb=59481.2, bsz=2002.9, num_updates=262100, lr=0.000195329, gnorm=1.501, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:18:16 | INFO | train_inner | epoch 133:   1144 / 1978 loss=3.042, nll_loss=0.901, word_ins=2.722, length=3.194, ppl=8.23, wps=50276.6, ups=0.85, wpb=59247.8, bsz=1926, num_updates=262200, lr=0.000195292, gnorm=1.55, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:20:14 | INFO | train_inner | epoch 133:   1244 / 1978 loss=3.018, nll_loss=0.887, word_ins=2.709, length=3.09, ppl=8.1, wps=50003.4, ups=0.84, wpb=59350.4, bsz=2009.7, num_updates=262300, lr=0.000195254, gnorm=1.484, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:22:12 | INFO | train_inner | epoch 133:   1344 / 1978 loss=3.021, nll_loss=0.885, word_ins=2.708, length=3.133, ppl=8.12, wps=49509.3, ups=0.85, wpb=58467.2, bsz=2034.1, num_updates=262400, lr=0.000195217, gnorm=1.546, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:24:11 | INFO | train_inner | epoch 133:   1444 / 1978 loss=3.018, nll_loss=0.889, word_ins=2.712, length=3.065, ppl=8.1, wps=50044.9, ups=0.84, wpb=59394.6, bsz=2013.6, num_updates=262500, lr=0.00019518, gnorm=1.551, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:26:09 | INFO | train_inner | epoch 133:   1544 / 1978 loss=3.043, nll_loss=0.901, word_ins=2.722, length=3.202, ppl=8.24, wps=50199.8, ups=0.85, wpb=59210.6, bsz=1943.4, num_updates=262600, lr=0.000195143, gnorm=1.535, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:28:09 | INFO | train_inner | epoch 133:   1644 / 1978 loss=3.029, nll_loss=0.893, word_ins=2.715, length=3.145, ppl=8.16, wps=49742.7, ups=0.84, wpb=59506.6, bsz=2020.6, num_updates=262700, lr=0.000195106, gnorm=1.589, loss_scale=8192, train_wall=119, wall=0
2023-01-12 11:30:07 | INFO | train_inner | epoch 133:   1744 / 1978 loss=3, nll_loss=0.87, word_ins=2.694, length=3.065, ppl=8, wps=50636.4, ups=0.84, wpb=59932.4, bsz=2073.4, num_updates=262800, lr=0.000195069, gnorm=1.522, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:32:06 | INFO | train_inner | epoch 133:   1844 / 1978 loss=3.025, nll_loss=0.888, word_ins=2.711, length=3.146, ppl=8.14, wps=50116, ups=0.84, wpb=59473.5, bsz=1992.3, num_updates=262900, lr=0.000195031, gnorm=1.581, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:34:04 | INFO | train_inner | epoch 133:   1944 / 1978 loss=3.029, nll_loss=0.891, word_ins=2.713, length=3.153, ppl=8.16, wps=50218.3, ups=0.85, wpb=59183.8, bsz=1972.6, num_updates=263000, lr=0.000194994, gnorm=1.477, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:34:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 11:35:07 | INFO | valid | epoch 133 | valid on 'valid' subset | loss 4.488 | nll_loss 1.996 | word_ins 3.75 | length 7.383 | ppl 22.44 | wps 67177.7 | wpb 40242.5 | bsz 1500 | num_updates 263034 | best_loss 4.274
2023-01-12 11:35:07 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 11:35:10 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint133.pt (epoch 133 @ 263034 updates, score 4.488) (writing took 3.0473487451672554 seconds)
2023-01-12 11:35:10 | INFO | fairseq_cli.train | end of epoch 133 (average epoch stats below)
2023-01-12 11:35:10 | INFO | train | epoch 133 | loss 3.026 | nll_loss 0.89 | word_ins 2.712 | length 3.136 | ppl 8.14 | wps 49151.5 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 263034 | lr 0.000194982 | gnorm 1.548 | loss_scale 8192 | train_wall 2338 | wall 0
2023-01-12 11:35:10 | INFO | fairseq.trainer | begin training epoch 134
2023-01-12 11:36:48 | INFO | train_inner | epoch 134:     66 / 1978 loss=3.01, nll_loss=0.879, word_ins=2.703, length=3.075, ppl=8.06, wps=35998, ups=0.61, wpb=59024.7, bsz=2023, num_updates=263100, lr=0.000194957, gnorm=1.501, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:38:47 | INFO | train_inner | epoch 134:    166 / 1978 loss=2.999, nll_loss=0.865, word_ins=2.69, length=3.086, ppl=7.99, wps=49770, ups=0.84, wpb=59422, bsz=2103.8, num_updates=263200, lr=0.00019492, gnorm=1.54, loss_scale=8192, train_wall=119, wall=0
2023-01-12 11:40:45 | INFO | train_inner | epoch 134:    266 / 1978 loss=3.039, nll_loss=0.901, word_ins=2.722, length=3.169, ppl=8.22, wps=49582.5, ups=0.85, wpb=58395.3, bsz=1934.3, num_updates=263300, lr=0.000194883, gnorm=1.576, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:42:44 | INFO | train_inner | epoch 134:    366 / 1978 loss=3.014, nll_loss=0.879, word_ins=2.702, length=3.117, ppl=8.08, wps=49769.1, ups=0.84, wpb=59241.2, bsz=1997.2, num_updates=263400, lr=0.000194846, gnorm=1.496, loss_scale=8192, train_wall=119, wall=0
2023-01-12 11:44:43 | INFO | train_inner | epoch 134:    466 / 1978 loss=3.004, nll_loss=0.875, word_ins=2.698, length=3.056, ppl=8.02, wps=50121.9, ups=0.84, wpb=59802.1, bsz=2104.3, num_updates=263500, lr=0.000194809, gnorm=1.459, loss_scale=8192, train_wall=119, wall=0
2023-01-12 11:46:42 | INFO | train_inner | epoch 134:    566 / 1978 loss=3.026, nll_loss=0.888, word_ins=2.711, length=3.152, ppl=8.14, wps=49820.9, ups=0.84, wpb=58998.2, bsz=2012.3, num_updates=263600, lr=0.000194772, gnorm=1.469, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:48:40 | INFO | train_inner | epoch 134:    666 / 1978 loss=3.01, nll_loss=0.875, word_ins=2.699, length=3.104, ppl=8.05, wps=50377.3, ups=0.84, wpb=59705.8, bsz=2057.3, num_updates=263700, lr=0.000194735, gnorm=1.555, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:50:39 | INFO | train_inner | epoch 134:    766 / 1978 loss=3.035, nll_loss=0.899, word_ins=2.721, length=3.141, ppl=8.19, wps=50342.5, ups=0.84, wpb=59729.7, bsz=1931.2, num_updates=263800, lr=0.000194698, gnorm=1.573, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:52:38 | INFO | train_inner | epoch 134:    866 / 1978 loss=3.016, nll_loss=0.882, word_ins=2.705, length=3.106, ppl=8.09, wps=50162, ups=0.84, wpb=59739.8, bsz=2055.6, num_updates=263900, lr=0.000194662, gnorm=1.583, loss_scale=8192, train_wall=119, wall=0
2023-01-12 11:54:36 | INFO | train_inner | epoch 134:    966 / 1978 loss=3.033, nll_loss=0.894, word_ins=2.716, length=3.17, ppl=8.19, wps=50635.9, ups=0.85, wpb=59796.1, bsz=1968.3, num_updates=264000, lr=0.000194625, gnorm=1.612, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:56:35 | INFO | train_inner | epoch 134:   1066 / 1978 loss=3.01, nll_loss=0.877, word_ins=2.7, length=3.095, ppl=8.05, wps=50170.4, ups=0.84, wpb=59512.3, bsz=2086.7, num_updates=264100, lr=0.000194588, gnorm=1.507, loss_scale=8192, train_wall=118, wall=0
2023-01-12 11:58:33 | INFO | train_inner | epoch 134:   1166 / 1978 loss=3.024, nll_loss=0.886, word_ins=2.709, length=3.148, ppl=8.13, wps=49744.4, ups=0.85, wpb=58760.1, bsz=1968.6, num_updates=264200, lr=0.000194551, gnorm=1.555, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:00:31 | INFO | train_inner | epoch 134:   1266 / 1978 loss=3.036, nll_loss=0.9, word_ins=2.721, length=3.152, ppl=8.2, wps=49852.6, ups=0.85, wpb=58986.1, bsz=1924.7, num_updates=264300, lr=0.000194514, gnorm=1.588, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:02:30 | INFO | train_inner | epoch 134:   1366 / 1978 loss=3.006, nll_loss=0.876, word_ins=2.7, length=3.06, ppl=8.03, wps=50234.5, ups=0.84, wpb=59825.9, bsz=2089.3, num_updates=264400, lr=0.000194477, gnorm=1.511, loss_scale=8192, train_wall=119, wall=0
2023-01-12 12:04:28 | INFO | train_inner | epoch 134:   1466 / 1978 loss=3.057, nll_loss=0.915, word_ins=2.735, length=3.216, ppl=8.32, wps=50106.6, ups=0.85, wpb=59016.9, bsz=1887.4, num_updates=264500, lr=0.000194441, gnorm=1.551, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:06:25 | INFO | train_inner | epoch 134:   1566 / 1978 loss=3.058, nll_loss=0.917, word_ins=2.737, length=3.21, ppl=8.33, wps=50175.1, ups=0.85, wpb=58727.7, bsz=1891.1, num_updates=264600, lr=0.000194404, gnorm=1.607, loss_scale=8192, train_wall=117, wall=0
2023-01-12 12:08:24 | INFO | train_inner | epoch 134:   1666 / 1978 loss=3.037, nll_loss=0.899, word_ins=2.721, length=3.167, ppl=8.21, wps=49635.7, ups=0.84, wpb=58983.6, bsz=1963.4, num_updates=264700, lr=0.000194367, gnorm=1.584, loss_scale=8192, train_wall=119, wall=0
2023-01-12 12:10:23 | INFO | train_inner | epoch 134:   1766 / 1978 loss=3.018, nll_loss=0.883, word_ins=2.706, length=3.12, ppl=8.1, wps=49740.7, ups=0.84, wpb=59224.9, bsz=2085.5, num_updates=264800, lr=0.000194331, gnorm=1.476, loss_scale=8192, train_wall=119, wall=0
2023-01-12 12:12:21 | INFO | train_inner | epoch 134:   1866 / 1978 loss=3.042, nll_loss=0.906, word_ins=2.727, length=3.151, ppl=8.24, wps=50491.1, ups=0.85, wpb=59494.1, bsz=1936, num_updates=264900, lr=0.000194294, gnorm=1.645, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:14:20 | INFO | train_inner | epoch 134:   1966 / 1978 loss=3.011, nll_loss=0.877, word_ins=2.7, length=3.11, ppl=8.06, wps=50125.9, ups=0.84, wpb=59617.6, bsz=2036.8, num_updates=265000, lr=0.000194257, gnorm=1.559, loss_scale=8192, train_wall=119, wall=0
2023-01-12 12:14:34 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 12:15:01 | INFO | valid | epoch 134 | valid on 'valid' subset | loss 4.498 | nll_loss 2.004 | word_ins 3.759 | length 7.388 | ppl 22.6 | wps 53160.7 | wpb 40242.5 | bsz 1500 | num_updates 265012 | best_loss 4.274
2023-01-12 12:15:01 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 12:15:04 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint134.pt (epoch 134 @ 265012 updates, score 4.498) (writing took 2.938993241172284 seconds)
2023-01-12 12:15:04 | INFO | fairseq_cli.train | end of epoch 134 (average epoch stats below)
2023-01-12 12:15:04 | INFO | train | epoch 134 | loss 3.025 | nll_loss 0.889 | word_ins 2.712 | length 3.132 | ppl 8.14 | wps 48998.5 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 265012 | lr 0.000194253 | gnorm 1.547 | loss_scale 8192 | train_wall 2340 | wall 0
2023-01-12 12:15:04 | INFO | fairseq.trainer | begin training epoch 135
2023-01-12 12:17:12 | INFO | train_inner | epoch 135:     88 / 1978 loss=3.03, nll_loss=0.894, word_ins=2.717, length=3.129, ppl=8.17, wps=34359.6, ups=0.58, wpb=59272.4, bsz=1940.7, num_updates=265100, lr=0.000194221, gnorm=1.544, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:19:11 | INFO | train_inner | epoch 135:    188 / 1978 loss=3.012, nll_loss=0.879, word_ins=2.702, length=3.097, ppl=8.07, wps=49772.6, ups=0.84, wpb=59241.2, bsz=2012.2, num_updates=265200, lr=0.000194184, gnorm=1.594, loss_scale=8192, train_wall=119, wall=0
2023-01-12 12:21:09 | INFO | train_inner | epoch 135:    288 / 1978 loss=3.011, nll_loss=0.878, word_ins=2.701, length=3.097, ppl=8.06, wps=50442.9, ups=0.85, wpb=59470, bsz=2016.8, num_updates=265300, lr=0.000194147, gnorm=1.625, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:23:08 | INFO | train_inner | epoch 135:    388 / 1978 loss=2.995, nll_loss=0.862, word_ins=2.687, length=3.078, ppl=7.97, wps=50306.9, ups=0.84, wpb=59765.9, bsz=2059.7, num_updates=265400, lr=0.000194111, gnorm=1.555, loss_scale=8192, train_wall=119, wall=0
2023-01-12 12:25:06 | INFO | train_inner | epoch 135:    488 / 1978 loss=3.046, nll_loss=0.904, word_ins=2.725, length=3.202, ppl=8.26, wps=50008.7, ups=0.85, wpb=59000.4, bsz=1959, num_updates=265500, lr=0.000194074, gnorm=1.583, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:27:04 | INFO | train_inner | epoch 135:    588 / 1978 loss=3.04, nll_loss=0.903, word_ins=2.724, length=3.153, ppl=8.22, wps=50038, ups=0.85, wpb=59139.6, bsz=1951.5, num_updates=265600, lr=0.000194038, gnorm=1.524, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:29:02 | INFO | train_inner | epoch 135:    688 / 1978 loss=3.026, nll_loss=0.892, word_ins=2.714, length=3.124, ppl=8.15, wps=50017.8, ups=0.85, wpb=59019.8, bsz=2003, num_updates=265700, lr=0.000194001, gnorm=1.516, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:31:00 | INFO | train_inner | epoch 135:    788 / 1978 loss=3.035, nll_loss=0.898, word_ins=2.72, length=3.155, ppl=8.2, wps=50387.5, ups=0.85, wpb=59241, bsz=1924.1, num_updates=265800, lr=0.000193965, gnorm=1.541, loss_scale=8192, train_wall=117, wall=0
2023-01-12 12:32:58 | INFO | train_inner | epoch 135:    888 / 1978 loss=3.027, nll_loss=0.891, word_ins=2.713, length=3.133, ppl=8.15, wps=49803.8, ups=0.84, wpb=59133.2, bsz=2046.2, num_updates=265900, lr=0.000193928, gnorm=1.553, loss_scale=8192, train_wall=119, wall=0
2023-01-12 12:33:41 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2023-01-12 12:34:58 | INFO | train_inner | epoch 135:    989 / 1978 loss=3.023, nll_loss=0.888, word_ins=2.71, length=3.13, ppl=8.13, wps=49627.8, ups=0.83, wpb=59543.4, bsz=2036.2, num_updates=266000, lr=0.000193892, gnorm=1.511, loss_scale=8192, train_wall=120, wall=0
2023-01-12 12:36:57 | INFO | train_inner | epoch 135:   1089 / 1978 loss=3.03, nll_loss=0.89, word_ins=2.712, length=3.181, ppl=8.17, wps=50217.9, ups=0.84, wpb=59555.1, bsz=1990.5, num_updates=266100, lr=0.000193855, gnorm=1.626, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:38:55 | INFO | train_inner | epoch 135:   1189 / 1978 loss=3.026, nll_loss=0.885, word_ins=2.708, length=3.182, ppl=8.15, wps=50185.8, ups=0.85, wpb=59294.5, bsz=1964, num_updates=266200, lr=0.000193819, gnorm=1.576, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:40:53 | INFO | train_inner | epoch 135:   1289 / 1978 loss=3.028, nll_loss=0.893, word_ins=2.715, length=3.134, ppl=8.16, wps=50247.9, ups=0.84, wpb=59492.9, bsz=1939.2, num_updates=266300, lr=0.000193782, gnorm=1.618, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:42:53 | INFO | train_inner | epoch 135:   1389 / 1978 loss=3.011, nll_loss=0.88, word_ins=2.703, length=3.078, ppl=8.06, wps=49967.7, ups=0.84, wpb=59522.4, bsz=2091.8, num_updates=266400, lr=0.000193746, gnorm=1.569, loss_scale=8192, train_wall=119, wall=0
2023-01-12 12:44:51 | INFO | train_inner | epoch 135:   1489 / 1978 loss=3.035, nll_loss=0.895, word_ins=2.717, length=3.177, ppl=8.19, wps=49917.3, ups=0.84, wpb=59247.3, bsz=2009, num_updates=266500, lr=0.00019371, gnorm=1.574, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:46:50 | INFO | train_inner | epoch 135:   1589 / 1978 loss=3.014, nll_loss=0.885, word_ins=2.708, length=3.061, ppl=8.08, wps=50003.3, ups=0.84, wpb=59402.2, bsz=2010.2, num_updates=266600, lr=0.000193673, gnorm=1.555, loss_scale=8192, train_wall=119, wall=0
2023-01-12 12:48:49 | INFO | train_inner | epoch 135:   1689 / 1978 loss=3.018, nll_loss=0.881, word_ins=2.704, length=3.142, ppl=8.1, wps=49768.5, ups=0.84, wpb=59028.6, bsz=2058.6, num_updates=266700, lr=0.000193637, gnorm=1.548, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:50:50 | INFO | train_inner | epoch 135:   1789 / 1978 loss=3.032, nll_loss=0.897, word_ins=2.719, length=3.138, ppl=8.18, wps=48383.1, ups=0.82, wpb=58870.8, bsz=1970.1, num_updates=266800, lr=0.000193601, gnorm=1.416, loss_scale=8192, train_wall=121, wall=0
2023-01-12 12:52:50 | INFO | train_inner | epoch 135:   1889 / 1978 loss=3.012, nll_loss=0.877, word_ins=2.7, length=3.118, ppl=8.07, wps=49555.8, ups=0.84, wpb=59139.5, bsz=2114, num_updates=266900, lr=0.000193565, gnorm=1.548, loss_scale=8192, train_wall=119, wall=0
2023-01-12 12:54:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 12:55:01 | INFO | valid | epoch 135 | valid on 'valid' subset | loss 4.498 | nll_loss 1.99 | word_ins 3.747 | length 7.509 | ppl 22.6 | wps 65257.8 | wpb 40242.5 | bsz 1500 | num_updates 266989 | best_loss 4.274
2023-01-12 12:55:01 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 12:55:04 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint135.pt (epoch 135 @ 266989 updates, score 4.498) (writing took 2.976759549230337 seconds)
2023-01-12 12:55:04 | INFO | fairseq_cli.train | end of epoch 135 (average epoch stats below)
2023-01-12 12:55:04 | INFO | train | epoch 135 | loss 3.025 | nll_loss 0.889 | word_ins 2.711 | length 3.134 | ppl 8.14 | wps 48820.3 | ups 0.82 | wpb 59282.9 | bsz 2002.9 | num_updates 266989 | lr 0.000193532 | gnorm 1.557 | loss_scale 8192 | train_wall 2343 | wall 0
2023-01-12 12:55:04 | INFO | fairseq.trainer | begin training epoch 136
2023-01-12 12:55:43 | INFO | train_inner | epoch 136:     11 / 1978 loss=3.061, nll_loss=0.921, word_ins=2.74, length=3.202, ppl=8.34, wps=33980.8, ups=0.58, wpb=58955.4, bsz=1917, num_updates=267000, lr=0.000193528, gnorm=1.598, loss_scale=8192, train_wall=118, wall=0
2023-01-12 12:57:42 | INFO | train_inner | epoch 136:    111 / 1978 loss=3.019, nll_loss=0.885, word_ins=2.709, length=3.102, ppl=8.11, wps=49877.4, ups=0.84, wpb=59231.4, bsz=1991.1, num_updates=267100, lr=0.000193492, gnorm=1.526, loss_scale=8192, train_wall=119, wall=0
2023-01-12 12:59:40 | INFO | train_inner | epoch 136:    211 / 1978 loss=3.013, nll_loss=0.878, word_ins=2.701, length=3.119, ppl=8.07, wps=50485.9, ups=0.85, wpb=59589.7, bsz=1959.1, num_updates=267200, lr=0.000193456, gnorm=1.521, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:01:38 | INFO | train_inner | epoch 136:    311 / 1978 loss=3.013, nll_loss=0.876, word_ins=2.7, length=3.128, ppl=8.07, wps=50018.7, ups=0.84, wpb=59258.6, bsz=1967.1, num_updates=267300, lr=0.00019342, gnorm=1.594, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:03:37 | INFO | train_inner | epoch 136:    411 / 1978 loss=3.037, nll_loss=0.899, word_ins=2.721, length=3.161, ppl=8.21, wps=49978.8, ups=0.84, wpb=59160.8, bsz=1973.2, num_updates=267400, lr=0.000193383, gnorm=1.561, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:05:36 | INFO | train_inner | epoch 136:    511 / 1978 loss=3.016, nll_loss=0.881, word_ins=2.705, length=3.109, ppl=8.09, wps=49565.5, ups=0.84, wpb=58880.3, bsz=2091.6, num_updates=267500, lr=0.000193347, gnorm=1.536, loss_scale=8192, train_wall=119, wall=0
2023-01-12 13:07:34 | INFO | train_inner | epoch 136:    611 / 1978 loss=3.054, nll_loss=0.91, word_ins=2.731, length=3.23, ppl=8.3, wps=50084.9, ups=0.85, wpb=59061.5, bsz=1876.7, num_updates=267600, lr=0.000193311, gnorm=1.594, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:09:32 | INFO | train_inner | epoch 136:    711 / 1978 loss=3.037, nll_loss=0.905, word_ins=2.726, length=3.108, ppl=8.21, wps=49739, ups=0.84, wpb=58964.2, bsz=1973.8, num_updates=267700, lr=0.000193275, gnorm=1.533, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:11:31 | INFO | train_inner | epoch 136:    811 / 1978 loss=2.997, nll_loss=0.865, word_ins=2.69, length=3.068, ppl=7.98, wps=50083.1, ups=0.84, wpb=59379.3, bsz=2154.2, num_updates=267800, lr=0.000193239, gnorm=1.524, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:13:29 | INFO | train_inner | epoch 136:    911 / 1978 loss=3.042, nll_loss=0.908, word_ins=2.728, length=3.139, ppl=8.24, wps=49429.8, ups=0.84, wpb=58703.7, bsz=2013.4, num_updates=267900, lr=0.000193203, gnorm=1.577, loss_scale=8192, train_wall=119, wall=0
2023-01-12 13:15:28 | INFO | train_inner | epoch 136:   1011 / 1978 loss=3.022, nll_loss=0.881, word_ins=2.705, length=3.175, ppl=8.12, wps=50327, ups=0.84, wpb=59581.7, bsz=1968, num_updates=268000, lr=0.000193167, gnorm=1.597, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:17:26 | INFO | train_inner | epoch 136:   1111 / 1978 loss=3.017, nll_loss=0.883, word_ins=2.706, length=3.104, ppl=8.09, wps=49850.2, ups=0.84, wpb=59125.9, bsz=2030.6, num_updates=268100, lr=0.000193131, gnorm=1.523, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:19:24 | INFO | train_inner | epoch 136:   1211 / 1978 loss=3.03, nll_loss=0.89, word_ins=2.712, length=3.177, ppl=8.17, wps=50480.3, ups=0.85, wpb=59564.1, bsz=1919.4, num_updates=268200, lr=0.000193095, gnorm=1.609, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:21:23 | INFO | train_inner | epoch 136:   1311 / 1978 loss=3.034, nll_loss=0.897, word_ins=2.719, length=3.153, ppl=8.19, wps=50062.9, ups=0.84, wpb=59307.2, bsz=1932.3, num_updates=268300, lr=0.000193059, gnorm=1.588, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:23:22 | INFO | train_inner | epoch 136:   1411 / 1978 loss=3.013, nll_loss=0.879, word_ins=2.702, length=3.104, ppl=8.07, wps=50062, ups=0.84, wpb=59518.5, bsz=2044.1, num_updates=268400, lr=0.000193023, gnorm=1.605, loss_scale=8192, train_wall=119, wall=0
2023-01-12 13:25:20 | INFO | train_inner | epoch 136:   1511 / 1978 loss=3.028, nll_loss=0.891, word_ins=2.713, length=3.153, ppl=8.16, wps=50198.7, ups=0.85, wpb=59379.6, bsz=1990.5, num_updates=268500, lr=0.000192987, gnorm=1.604, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:27:19 | INFO | train_inner | epoch 136:   1611 / 1978 loss=3.023, nll_loss=0.889, word_ins=2.711, length=3.126, ppl=8.13, wps=49881.1, ups=0.84, wpb=59284, bsz=2034.7, num_updates=268600, lr=0.000192951, gnorm=1.596, loss_scale=8192, train_wall=119, wall=0
2023-01-12 13:29:18 | INFO | train_inner | epoch 136:   1711 / 1978 loss=2.997, nll_loss=0.865, word_ins=2.69, length=3.078, ppl=7.99, wps=49819.6, ups=0.84, wpb=59577.1, bsz=2118.2, num_updates=268700, lr=0.000192915, gnorm=1.514, loss_scale=8192, train_wall=119, wall=0
2023-01-12 13:31:17 | INFO | train_inner | epoch 136:   1811 / 1978 loss=3.007, nll_loss=0.875, word_ins=2.698, length=3.086, ppl=8.04, wps=50062.6, ups=0.84, wpb=59316.9, bsz=2058.2, num_updates=268800, lr=0.000192879, gnorm=1.446, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:33:15 | INFO | train_inner | epoch 136:   1911 / 1978 loss=3.029, nll_loss=0.893, word_ins=2.715, length=3.144, ppl=8.16, wps=50109.3, ups=0.85, wpb=59186.6, bsz=2006.5, num_updates=268900, lr=0.000192843, gnorm=1.592, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:34:35 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 13:35:00 | INFO | valid | epoch 136 | valid on 'valid' subset | loss 4.446 | nll_loss 1.97 | word_ins 3.727 | length 7.197 | ppl 21.8 | wps 75378 | wpb 40242.5 | bsz 1500 | num_updates 268967 | best_loss 4.274
2023-01-12 13:35:00 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 13:35:03 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint136.pt (epoch 136 @ 268967 updates, score 4.446) (writing took 2.7917973319999874 seconds)
2023-01-12 13:35:03 | INFO | fairseq_cli.train | end of epoch 136 (average epoch stats below)
2023-01-12 13:35:03 | INFO | train | epoch 136 | loss 3.023 | nll_loss 0.888 | word_ins 2.71 | length 3.13 | ppl 8.13 | wps 48883.5 | ups 0.82 | wpb 59284.3 | bsz 2002.6 | num_updates 268967 | lr 0.000192819 | gnorm 1.561 | loss_scale 8192 | train_wall 2340 | wall 0
2023-01-12 13:35:03 | INFO | fairseq.trainer | begin training epoch 137
2023-01-12 13:36:04 | INFO | train_inner | epoch 137:     33 / 1978 loss=3.017, nll_loss=0.887, word_ins=2.71, length=3.077, ppl=8.1, wps=35459.3, ups=0.59, wpb=59794.3, bsz=2009.7, num_updates=269000, lr=0.000192807, gnorm=1.57, loss_scale=8192, train_wall=120, wall=0
2023-01-12 13:38:03 | INFO | train_inner | epoch 137:    133 / 1978 loss=3.018, nll_loss=0.888, word_ins=2.71, length=3.084, ppl=8.1, wps=49674.8, ups=0.84, wpb=59224.7, bsz=1991.2, num_updates=269100, lr=0.000192772, gnorm=1.575, loss_scale=8192, train_wall=119, wall=0
2023-01-12 13:40:01 | INFO | train_inner | epoch 137:    233 / 1978 loss=3.032, nll_loss=0.896, word_ins=2.719, length=3.129, ppl=8.18, wps=50086.4, ups=0.84, wpb=59292.5, bsz=1980.2, num_updates=269200, lr=0.000192736, gnorm=1.597, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:42:00 | INFO | train_inner | epoch 137:    333 / 1978 loss=3.031, nll_loss=0.896, word_ins=2.718, length=3.136, ppl=8.18, wps=50143.7, ups=0.84, wpb=59667.1, bsz=1942.9, num_updates=269300, lr=0.0001927, gnorm=1.579, loss_scale=8192, train_wall=119, wall=0
2023-01-12 13:43:58 | INFO | train_inner | epoch 137:    433 / 1978 loss=3.005, nll_loss=0.875, word_ins=2.699, length=3.066, ppl=8.03, wps=50279, ups=0.85, wpb=59381.2, bsz=2044.3, num_updates=269400, lr=0.000192664, gnorm=1.505, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:45:57 | INFO | train_inner | epoch 137:    533 / 1978 loss=3.025, nll_loss=0.89, word_ins=2.713, length=3.123, ppl=8.14, wps=49917.8, ups=0.84, wpb=59187, bsz=1972.6, num_updates=269500, lr=0.000192629, gnorm=1.541, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:47:56 | INFO | train_inner | epoch 137:    633 / 1978 loss=3.015, nll_loss=0.882, word_ins=2.705, length=3.102, ppl=8.08, wps=50274.3, ups=0.84, wpb=59768.9, bsz=2039.9, num_updates=269600, lr=0.000192593, gnorm=1.563, loss_scale=8192, train_wall=119, wall=0
2023-01-12 13:49:53 | INFO | train_inner | epoch 137:    733 / 1978 loss=3.036, nll_loss=0.893, word_ins=2.715, length=3.204, ppl=8.2, wps=49701.3, ups=0.85, wpb=58437.3, bsz=1935.9, num_updates=269700, lr=0.000192557, gnorm=1.558, loss_scale=8192, train_wall=117, wall=0
2023-01-12 13:51:52 | INFO | train_inner | epoch 137:    833 / 1978 loss=3.012, nll_loss=0.88, word_ins=2.702, length=3.097, ppl=8.07, wps=49976, ups=0.84, wpb=59248.7, bsz=2051.4, num_updates=269800, lr=0.000192521, gnorm=1.521, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:53:51 | INFO | train_inner | epoch 137:    933 / 1978 loss=3, nll_loss=0.872, word_ins=2.696, length=3.049, ppl=8, wps=50138, ups=0.84, wpb=59749.5, bsz=2098.7, num_updates=269900, lr=0.000192486, gnorm=1.634, loss_scale=8192, train_wall=119, wall=0
2023-01-12 13:55:49 | INFO | train_inner | epoch 137:   1033 / 1978 loss=3.043, nll_loss=0.897, word_ins=2.719, length=3.236, ppl=8.24, wps=50041.4, ups=0.85, wpb=59185.6, bsz=1957, num_updates=270000, lr=0.00019245, gnorm=1.561, loss_scale=8192, train_wall=118, wall=0
2023-01-12 13:56:27 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2023-01-12 13:57:49 | INFO | train_inner | epoch 137:   1134 / 1978 loss=3.003, nll_loss=0.873, word_ins=2.696, length=3.069, ppl=8.02, wps=49780.7, ups=0.84, wpb=59487.8, bsz=2057.1, num_updates=270100, lr=0.000192414, gnorm=1.512, loss_scale=8192, train_wall=119, wall=0
2023-01-12 13:59:47 | INFO | train_inner | epoch 137:   1234 / 1978 loss=3.025, nll_loss=0.893, word_ins=2.714, length=3.105, ppl=8.14, wps=50135.9, ups=0.85, wpb=59069, bsz=1932.5, num_updates=270200, lr=0.000192379, gnorm=1.525, loss_scale=8192, train_wall=118, wall=0
2023-01-12 14:01:46 | INFO | train_inner | epoch 137:   1334 / 1978 loss=3.025, nll_loss=0.89, word_ins=2.712, length=3.132, ppl=8.14, wps=49626.2, ups=0.84, wpb=59039.8, bsz=2043.5, num_updates=270300, lr=0.000192343, gnorm=1.58, loss_scale=8192, train_wall=119, wall=0
2023-01-12 14:03:44 | INFO | train_inner | epoch 137:   1434 / 1978 loss=3.014, nll_loss=0.88, word_ins=2.703, length=3.108, ppl=8.08, wps=50187.2, ups=0.85, wpb=59179.9, bsz=1976, num_updates=270400, lr=0.000192308, gnorm=1.537, loss_scale=8192, train_wall=118, wall=0
2023-01-12 14:05:43 | INFO | train_inner | epoch 137:   1534 / 1978 loss=3.015, nll_loss=0.88, word_ins=2.703, length=3.121, ppl=8.08, wps=50073.2, ups=0.84, wpb=59666.8, bsz=2037, num_updates=270500, lr=0.000192272, gnorm=1.603, loss_scale=8192, train_wall=119, wall=0
2023-01-12 14:07:42 | INFO | train_inner | epoch 137:   1634 / 1978 loss=3.013, nll_loss=0.877, word_ins=2.7, length=3.13, ppl=8.07, wps=49752.7, ups=0.84, wpb=59141.6, bsz=2064.9, num_updates=270600, lr=0.000192237, gnorm=1.516, loss_scale=8192, train_wall=119, wall=0
2023-01-12 14:09:40 | INFO | train_inner | epoch 137:   1734 / 1978 loss=3.04, nll_loss=0.902, word_ins=2.723, length=3.164, ppl=8.22, wps=50008.4, ups=0.85, wpb=59054.8, bsz=1981.2, num_updates=270700, lr=0.000192201, gnorm=1.519, loss_scale=8192, train_wall=118, wall=0
2023-01-12 14:11:38 | INFO | train_inner | epoch 137:   1834 / 1978 loss=3.022, nll_loss=0.883, word_ins=2.706, length=3.166, ppl=8.13, wps=50342.6, ups=0.85, wpb=59353.7, bsz=2006.9, num_updates=270800, lr=0.000192166, gnorm=1.557, loss_scale=8192, train_wall=118, wall=0
2023-01-12 14:13:35 | INFO | train_inner | epoch 137:   1934 / 1978 loss=3.051, nll_loss=0.907, word_ins=2.728, length=3.231, ppl=8.29, wps=50206, ups=0.85, wpb=59127, bsz=1872.6, num_updates=270900, lr=0.00019213, gnorm=1.689, loss_scale=8192, train_wall=118, wall=0
2023-01-12 14:14:28 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 14:15:06 | INFO | valid | epoch 137 | valid on 'valid' subset | loss 4.473 | nll_loss 1.973 | word_ins 3.73 | length 7.432 | ppl 22.21 | wps 73593.2 | wpb 40242.5 | bsz 1500 | num_updates 270944 | best_loss 4.274
2023-01-12 14:15:06 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 14:15:08 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint137.pt (epoch 137 @ 270944 updates, score 4.473) (writing took 2.8018160508945584 seconds)
2023-01-12 14:15:08 | INFO | fairseq_cli.train | end of epoch 137 (average epoch stats below)
2023-01-12 14:15:08 | INFO | train | epoch 137 | loss 3.022 | nll_loss 0.887 | word_ins 2.709 | length 3.126 | ppl 8.12 | wps 48731.6 | ups 0.82 | wpb 59286.2 | bsz 2002.9 | num_updates 270944 | lr 0.000192115 | gnorm 1.561 | loss_scale 8192 | train_wall 2341 | wall 0
2023-01-12 14:15:08 | INFO | fairseq.trainer | begin training epoch 138
2023-01-12 14:16:46 | INFO | train_inner | epoch 138:     56 / 1978 loss=3.024, nll_loss=0.889, word_ins=2.712, length=3.119, ppl=8.13, wps=31095.2, ups=0.52, wpb=59286.4, bsz=1995.1, num_updates=271000, lr=0.000192095, gnorm=1.616, loss_scale=8192, train_wall=118, wall=0
2023-01-12 14:18:45 | INFO | train_inner | epoch 138:    156 / 1978 loss=3.015, nll_loss=0.878, word_ins=2.702, length=3.128, ppl=8.08, wps=50164.3, ups=0.84, wpb=59435.1, bsz=2002.3, num_updates=271100, lr=0.000192059, gnorm=1.611, loss_scale=8192, train_wall=118, wall=0
2023-01-12 14:20:43 | INFO | train_inner | epoch 138:    256 / 1978 loss=3.011, nll_loss=0.874, word_ins=2.698, length=3.132, ppl=8.06, wps=50045.7, ups=0.85, wpb=59063, bsz=2031.6, num_updates=271200, lr=0.000192024, gnorm=1.575, loss_scale=8192, train_wall=118, wall=0
2023-01-12 14:22:40 | INFO | train_inner | epoch 138:    356 / 1978 loss=3.028, nll_loss=0.89, word_ins=2.713, length=3.159, ppl=8.16, wps=50123.9, ups=0.85, wpb=59013.6, bsz=1910.6, num_updates=271300, lr=0.000191988, gnorm=1.6, loss_scale=8192, train_wall=118, wall=0
2023-01-12 14:24:40 | INFO | train_inner | epoch 138:    456 / 1978 loss=2.994, nll_loss=0.861, word_ins=2.686, length=3.081, ppl=7.97, wps=49723.6, ups=0.84, wpb=59385.2, bsz=2078.5, num_updates=271400, lr=0.000191953, gnorm=1.545, loss_scale=8192, train_wall=119, wall=0
2023-01-12 14:26:38 | INFO | train_inner | epoch 138:    556 / 1978 loss=3.046, nll_loss=0.907, word_ins=2.728, length=3.175, ppl=8.26, wps=50038.1, ups=0.85, wpb=58988.3, bsz=1934.9, num_updates=271500, lr=0.000191918, gnorm=1.611, loss_scale=8192, train_wall=118, wall=0
2023-01-12 14:28:36 | INFO | train_inner | epoch 138:    656 / 1978 loss=3.018, nll_loss=0.884, word_ins=2.707, length=3.112, ppl=8.1, wps=49982.8, ups=0.84, wpb=59279.7, bsz=1982.7, num_updates=271600, lr=0.000191882, gnorm=1.533, loss_scale=8192, train_wall=118, wall=0
2023-01-12 14:30:35 | INFO | train_inner | epoch 138:    756 / 1978 loss=3.028, nll_loss=0.889, word_ins=2.712, length=3.163, ppl=8.16, wps=50118.4, ups=0.84, wpb=59376, bsz=1957.9, num_updates=271700, lr=0.000191847, gnorm=1.568, loss_scale=8192, train_wall=118, wall=0
2023-01-12 14:32:34 | INFO | train_inner | epoch 138:    856 / 1978 loss=3.023, nll_loss=0.884, word_ins=2.706, length=3.171, ppl=8.13, wps=49906, ups=0.84, wpb=59273.4, bsz=2016.2, num_updates=271800, lr=0.000191812, gnorm=1.601, loss_scale=8192, train_wall=119, wall=0
2023-01-12 14:34:33 | INFO | train_inner | epoch 138:    956 / 1978 loss=3.012, nll_loss=0.879, word_ins=2.702, length=3.103, ppl=8.07, wps=50004.7, ups=0.84, wpb=59710.9, bsz=2063.3, num_updates=271900, lr=0.000191777, gnorm=1.542, loss_scale=8192, train_wall=119, wall=0
2023-01-12 14:36:30 | INFO | train_inner | epoch 138:   1056 / 1978 loss=3.039, nll_loss=0.904, word_ins=2.725, length=3.133, ppl=8.22, wps=50275.1, ups=0.85, wpb=59074.3, bsz=2036.6, num_updates=272000, lr=0.000191741, gnorm=1.545, loss_scale=8192, train_wall=117, wall=0
2023-01-12 14:38:28 | INFO | train_inner | epoch 138:   1156 / 1978 loss=3.024, nll_loss=0.886, word_ins=2.709, length=3.143, ppl=8.13, wps=50222.8, ups=0.85, wpb=59127.2, bsz=1966.8, num_updates=272100, lr=0.000191706, gnorm=1.54, loss_scale=8192, train_wall=118, wall=0
2023-01-12 14:40:26 | INFO | train_inner | epoch 138:   1256 / 1978 loss=3.052, nll_loss=0.911, word_ins=2.731, length=3.211, ppl=8.29, wps=50658.6, ups=0.85, wpb=59657, bsz=1873.6, num_updates=272200, lr=0.000191671, gnorm=1.638, loss_scale=8192, train_wall=118, wall=0
2023-01-12 14:42:25 | INFO | train_inner | epoch 138:   1356 / 1978 loss=2.997, nll_loss=0.867, word_ins=2.691, length=3.056, ppl=7.98, wps=49906.3, ups=0.84, wpb=59324.8, bsz=2130.6, num_updates=272300, lr=0.000191636, gnorm=1.51, loss_scale=8192, train_wall=119, wall=0
2023-01-12 14:44:24 | INFO | train_inner | epoch 138:   1456 / 1978 loss=3.004, nll_loss=0.873, word_ins=2.696, length=3.08, ppl=8.02, wps=49844.9, ups=0.84, wpb=59297.3, bsz=2061, num_updates=272400, lr=0.0001916, gnorm=1.508, loss_scale=8192, train_wall=119, wall=0
2023-01-12 14:46:23 | INFO | train_inner | epoch 138:   1556 / 1978 loss=3.005, nll_loss=0.875, word_ins=2.698, length=3.064, ppl=8.03, wps=50219.7, ups=0.84, wpb=59716.3, bsz=2016.1, num_updates=272500, lr=0.000191565, gnorm=1.559, loss_scale=8192, train_wall=119, wall=0
2023-01-12 14:48:22 | INFO | train_inner | epoch 138:   1656 / 1978 loss=3.021, nll_loss=0.884, word_ins=2.706, length=3.144, ppl=8.11, wps=49901.6, ups=0.84, wpb=59371, bsz=2000.6, num_updates=272600, lr=0.00019153, gnorm=1.537, loss_scale=8192, train_wall=119, wall=0
2023-01-12 14:50:24 | INFO | train_inner | epoch 138:   1756 / 1978 loss=3.03, nll_loss=0.9, word_ins=2.721, length=3.091, ppl=8.17, wps=48450.6, ups=0.82, wpb=59409.6, bsz=1980.6, num_updates=272700, lr=0.000191495, gnorm=1.577, loss_scale=8192, train_wall=122, wall=0
2023-01-12 14:52:05 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2023-01-12 14:52:24 | INFO | train_inner | epoch 138:   1857 / 1978 loss=3.02, nll_loss=0.887, word_ins=2.709, length=3.114, ppl=8.11, wps=49436.9, ups=0.83, wpb=59245.6, bsz=2087, num_updates=272800, lr=0.00019146, gnorm=1.552, loss_scale=4096, train_wall=120, wall=0
2023-01-12 14:54:23 | INFO | train_inner | epoch 138:   1957 / 1978 loss=3.028, nll_loss=0.892, word_ins=2.714, length=3.144, ppl=8.16, wps=50032, ups=0.84, wpb=59306.1, bsz=1987.3, num_updates=272900, lr=0.000191425, gnorm=1.546, loss_scale=4096, train_wall=118, wall=0
2023-01-12 14:54:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 14:55:14 | INFO | valid | epoch 138 | valid on 'valid' subset | loss 4.432 | nll_loss 1.985 | word_ins 3.741 | length 6.914 | ppl 21.59 | wps 86463.3 | wpb 40242.5 | bsz 1500 | num_updates 272921 | best_loss 4.274
2023-01-12 14:55:14 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 14:55:17 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint138.pt (epoch 138 @ 272921 updates, score 4.432) (writing took 2.7874509640969336 seconds)
2023-01-12 14:55:17 | INFO | fairseq_cli.train | end of epoch 138 (average epoch stats below)
2023-01-12 14:55:17 | INFO | train | epoch 138 | loss 3.021 | nll_loss 0.886 | word_ins 2.709 | length 3.127 | ppl 8.12 | wps 48664 | ups 0.82 | wpb 59292.8 | bsz 2002.9 | num_updates 272921 | lr 0.000191417 | gnorm 1.566 | loss_scale 4096 | train_wall 2343 | wall 0
2023-01-12 14:55:17 | INFO | fairseq.trainer | begin training epoch 139
2023-01-12 14:57:16 | INFO | train_inner | epoch 139:     79 / 1978 loss=3.009, nll_loss=0.879, word_ins=2.703, length=3.063, ppl=8.05, wps=34142.8, ups=0.58, wpb=59211, bsz=2011.4, num_updates=273000, lr=0.00019139, gnorm=1.53, loss_scale=4096, train_wall=118, wall=0
2023-01-12 14:59:15 | INFO | train_inner | epoch 139:    179 / 1978 loss=2.993, nll_loss=0.861, word_ins=2.685, length=3.074, ppl=7.96, wps=50486.9, ups=0.84, wpb=60017, bsz=2075.5, num_updates=273100, lr=0.000191355, gnorm=1.552, loss_scale=4096, train_wall=119, wall=0
2023-01-12 15:01:12 | INFO | train_inner | epoch 139:    279 / 1978 loss=3.042, nll_loss=0.903, word_ins=2.725, length=3.17, ppl=8.24, wps=50213.3, ups=0.85, wpb=58827.2, bsz=1947.7, num_updates=273200, lr=0.00019132, gnorm=1.612, loss_scale=4096, train_wall=117, wall=0
2023-01-12 15:03:11 | INFO | train_inner | epoch 139:    379 / 1978 loss=2.995, nll_loss=0.863, word_ins=2.687, length=3.075, ppl=7.97, wps=50270.5, ups=0.84, wpb=59662.6, bsz=2045.8, num_updates=273300, lr=0.000191285, gnorm=1.531, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:05:09 | INFO | train_inner | epoch 139:    479 / 1978 loss=3.032, nll_loss=0.893, word_ins=2.715, length=3.165, ppl=8.18, wps=49901.9, ups=0.85, wpb=59053.2, bsz=1965.5, num_updates=273400, lr=0.00019125, gnorm=1.571, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:07:08 | INFO | train_inner | epoch 139:    579 / 1978 loss=3.024, nll_loss=0.888, word_ins=2.71, length=3.142, ppl=8.14, wps=50199.9, ups=0.84, wpb=59424.1, bsz=1985, num_updates=273500, lr=0.000191215, gnorm=1.621, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:09:06 | INFO | train_inner | epoch 139:    679 / 1978 loss=3.038, nll_loss=0.899, word_ins=2.721, length=3.173, ppl=8.21, wps=49985.6, ups=0.85, wpb=59012.3, bsz=1950.3, num_updates=273600, lr=0.00019118, gnorm=1.603, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:11:04 | INFO | train_inner | epoch 139:    779 / 1978 loss=3.037, nll_loss=0.901, word_ins=2.722, length=3.15, ppl=8.21, wps=50694.2, ups=0.85, wpb=59801.6, bsz=1943.6, num_updates=273700, lr=0.000191145, gnorm=1.583, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:13:02 | INFO | train_inner | epoch 139:    879 / 1978 loss=2.996, nll_loss=0.866, word_ins=2.69, length=3.061, ppl=7.98, wps=50188.3, ups=0.84, wpb=59522.9, bsz=2105, num_updates=273800, lr=0.00019111, gnorm=1.522, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:15:00 | INFO | train_inner | epoch 139:    979 / 1978 loss=3.017, nll_loss=0.883, word_ins=2.706, length=3.114, ppl=8.1, wps=50242.1, ups=0.85, wpb=59176.2, bsz=1978.7, num_updates=273900, lr=0.000191075, gnorm=1.543, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:17:00 | INFO | train_inner | epoch 139:   1079 / 1978 loss=2.988, nll_loss=0.859, word_ins=2.684, length=3.039, ppl=7.93, wps=49721.5, ups=0.84, wpb=59484.5, bsz=2133.6, num_updates=274000, lr=0.00019104, gnorm=1.504, loss_scale=4096, train_wall=119, wall=0
2023-01-12 15:18:58 | INFO | train_inner | epoch 139:   1179 / 1978 loss=3.027, nll_loss=0.89, word_ins=2.712, length=3.149, ppl=8.15, wps=49919.4, ups=0.84, wpb=59084.2, bsz=1985.8, num_updates=274100, lr=0.000191005, gnorm=1.566, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:20:56 | INFO | train_inner | epoch 139:   1279 / 1978 loss=3.016, nll_loss=0.88, word_ins=2.702, length=3.134, ppl=8.09, wps=50150.8, ups=0.85, wpb=59265.9, bsz=1961.4, num_updates=274200, lr=0.00019097, gnorm=1.576, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:22:54 | INFO | train_inner | epoch 139:   1379 / 1978 loss=3.026, nll_loss=0.887, word_ins=2.709, length=3.165, ppl=8.14, wps=49832.6, ups=0.85, wpb=58939.4, bsz=2028.5, num_updates=274300, lr=0.000190936, gnorm=1.567, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:24:53 | INFO | train_inner | epoch 139:   1479 / 1978 loss=3.032, nll_loss=0.901, word_ins=2.722, length=3.104, ppl=8.18, wps=50221, ups=0.85, wpb=59371.3, bsz=1954.8, num_updates=274400, lr=0.000190901, gnorm=1.62, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:26:51 | INFO | train_inner | epoch 139:   1579 / 1978 loss=3.031, nll_loss=0.893, word_ins=2.715, length=3.166, ppl=8.17, wps=49958, ups=0.85, wpb=59045, bsz=1965.1, num_updates=274500, lr=0.000190866, gnorm=1.559, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:28:49 | INFO | train_inner | epoch 139:   1679 / 1978 loss=3.017, nll_loss=0.884, word_ins=2.707, length=3.1, ppl=8.09, wps=49875.3, ups=0.84, wpb=59143.6, bsz=2033.4, num_updates=274600, lr=0.000190831, gnorm=1.529, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:30:48 | INFO | train_inner | epoch 139:   1779 / 1978 loss=3.039, nll_loss=0.898, word_ins=2.72, length=3.195, ppl=8.22, wps=49962.2, ups=0.85, wpb=59091.3, bsz=1948.5, num_updates=274700, lr=0.000190797, gnorm=1.606, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:32:46 | INFO | train_inner | epoch 139:   1879 / 1978 loss=3.025, nll_loss=0.887, word_ins=2.71, length=3.153, ppl=8.14, wps=49960.8, ups=0.84, wpb=59201.2, bsz=2002.6, num_updates=274800, lr=0.000190762, gnorm=1.645, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:34:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 15:34:59 | INFO | valid | epoch 139 | valid on 'valid' subset | loss 4.421 | nll_loss 1.985 | word_ins 3.739 | length 6.825 | ppl 21.42 | wps 157233 | wpb 40242.5 | bsz 1500 | num_updates 274899 | best_loss 4.274
2023-01-12 15:34:59 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 15:35:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint139.pt (epoch 139 @ 274899 updates, score 4.421) (writing took 3.4755004020407796 seconds)
2023-01-12 15:35:02 | INFO | fairseq_cli.train | end of epoch 139 (average epoch stats below)
2023-01-12 15:35:02 | INFO | train | epoch 139 | loss 3.02 | nll_loss 0.885 | word_ins 2.707 | length 3.125 | ppl 8.11 | wps 49166.4 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 274899 | lr 0.000190728 | gnorm 1.573 | loss_scale 4096 | train_wall 2337 | wall 0
2023-01-12 15:35:02 | INFO | fairseq.trainer | begin training epoch 140
2023-01-12 15:35:12 | INFO | train_inner | epoch 140:      1 / 1978 loss=3.02, nll_loss=0.887, word_ins=2.709, length=3.106, ppl=8.11, wps=40390.2, ups=0.68, wpb=58975.6, bsz=2008.5, num_updates=274900, lr=0.000190727, gnorm=1.594, loss_scale=4096, train_wall=119, wall=0
2023-01-12 15:37:11 | INFO | train_inner | epoch 140:    101 / 1978 loss=3.024, nll_loss=0.886, word_ins=2.709, length=3.145, ppl=8.13, wps=49863.9, ups=0.84, wpb=59088.4, bsz=1996, num_updates=275000, lr=0.000190693, gnorm=1.54, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:39:10 | INFO | train_inner | epoch 140:    201 / 1978 loss=2.997, nll_loss=0.865, word_ins=2.689, length=3.075, ppl=7.98, wps=50283.8, ups=0.84, wpb=59808.9, bsz=2068.4, num_updates=275100, lr=0.000190658, gnorm=1.656, loss_scale=4096, train_wall=119, wall=0
2023-01-12 15:41:08 | INFO | train_inner | epoch 140:    301 / 1978 loss=3.026, nll_loss=0.889, word_ins=2.712, length=3.145, ppl=8.15, wps=50106.8, ups=0.85, wpb=59203.5, bsz=1973.6, num_updates=275200, lr=0.000190623, gnorm=1.614, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:43:06 | INFO | train_inner | epoch 140:    401 / 1978 loss=3.011, nll_loss=0.875, word_ins=2.698, length=3.122, ppl=8.06, wps=50384.4, ups=0.84, wpb=59764.2, bsz=1979.5, num_updates=275300, lr=0.000190589, gnorm=1.55, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:45:04 | INFO | train_inner | epoch 140:    501 / 1978 loss=3.007, nll_loss=0.872, word_ins=2.696, length=3.114, ppl=8.04, wps=50266.3, ups=0.85, wpb=59204.9, bsz=1993.1, num_updates=275400, lr=0.000190554, gnorm=1.523, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:47:02 | INFO | train_inner | epoch 140:    601 / 1978 loss=3.033, nll_loss=0.899, word_ins=2.72, length=3.128, ppl=8.19, wps=50256, ups=0.85, wpb=59410.6, bsz=1948, num_updates=275500, lr=0.000190519, gnorm=1.603, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:49:01 | INFO | train_inner | epoch 140:    701 / 1978 loss=3.023, nll_loss=0.887, word_ins=2.71, length=3.126, ppl=8.13, wps=50205.8, ups=0.85, wpb=59343.6, bsz=1931.4, num_updates=275600, lr=0.000190485, gnorm=1.6, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:50:59 | INFO | train_inner | epoch 140:    801 / 1978 loss=3.018, nll_loss=0.879, word_ins=2.702, length=3.158, ppl=8.1, wps=50093.3, ups=0.85, wpb=59088.7, bsz=1996.2, num_updates=275700, lr=0.00019045, gnorm=1.55, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:52:57 | INFO | train_inner | epoch 140:    901 / 1978 loss=3.015, nll_loss=0.878, word_ins=2.701, length=3.134, ppl=8.08, wps=49360, ups=0.85, wpb=58340.4, bsz=2039.8, num_updates=275800, lr=0.000190416, gnorm=1.563, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:54:55 | INFO | train_inner | epoch 140:   1001 / 1978 loss=3.021, nll_loss=0.884, word_ins=2.706, length=3.143, ppl=8.12, wps=50582.8, ups=0.85, wpb=59718, bsz=2012.1, num_updates=275900, lr=0.000190381, gnorm=1.599, loss_scale=4096, train_wall=118, wall=0
2023-01-12 15:56:54 | INFO | train_inner | epoch 140:   1101 / 1978 loss=3.005, nll_loss=0.875, word_ins=2.699, length=3.059, ppl=8.03, wps=49709.2, ups=0.84, wpb=59118.6, bsz=2101.4, num_updates=276000, lr=0.000190347, gnorm=1.477, loss_scale=4096, train_wall=119, wall=0
2023-01-12 15:58:53 | INFO | train_inner | epoch 140:   1201 / 1978 loss=3.015, nll_loss=0.887, word_ins=2.709, length=3.058, ppl=8.08, wps=49964.1, ups=0.84, wpb=59455, bsz=2057.7, num_updates=276100, lr=0.000190312, gnorm=1.527, loss_scale=4096, train_wall=119, wall=0
2023-01-12 16:00:51 | INFO | train_inner | epoch 140:   1301 / 1978 loss=3.007, nll_loss=0.87, word_ins=2.694, length=3.133, ppl=8.04, wps=50364.4, ups=0.85, wpb=59373.8, bsz=1996.4, num_updates=276200, lr=0.000190278, gnorm=1.543, loss_scale=4096, train_wall=118, wall=0
2023-01-12 16:02:50 | INFO | train_inner | epoch 140:   1401 / 1978 loss=3.002, nll_loss=0.871, word_ins=2.695, length=3.064, ppl=8.01, wps=49776.8, ups=0.84, wpb=59465.1, bsz=2113, num_updates=276300, lr=0.000190243, gnorm=1.567, loss_scale=4096, train_wall=119, wall=0
2023-01-12 16:04:48 | INFO | train_inner | epoch 140:   1501 / 1978 loss=3.039, nll_loss=0.907, word_ins=2.728, length=3.116, ppl=8.22, wps=50148.2, ups=0.85, wpb=59295.5, bsz=1923.4, num_updates=276400, lr=0.000190209, gnorm=1.631, loss_scale=4096, train_wall=118, wall=0
2023-01-12 16:06:47 | INFO | train_inner | epoch 140:   1601 / 1978 loss=3.034, nll_loss=0.901, word_ins=2.722, length=3.118, ppl=8.19, wps=49540.5, ups=0.84, wpb=58972.3, bsz=2017.3, num_updates=276500, lr=0.000190175, gnorm=1.603, loss_scale=4096, train_wall=119, wall=0
2023-01-12 16:08:46 | INFO | train_inner | epoch 140:   1701 / 1978 loss=3.041, nll_loss=0.904, word_ins=2.725, length=3.159, ppl=8.23, wps=49941.4, ups=0.84, wpb=59133, bsz=1953.5, num_updates=276600, lr=0.00019014, gnorm=1.674, loss_scale=4096, train_wall=118, wall=0
2023-01-12 16:10:44 | INFO | train_inner | epoch 140:   1801 / 1978 loss=3.002, nll_loss=0.87, word_ins=2.694, length=3.077, ppl=8.01, wps=50394.2, ups=0.84, wpb=59766.5, bsz=2051.2, num_updates=276700, lr=0.000190106, gnorm=1.569, loss_scale=4096, train_wall=118, wall=0
2023-01-12 16:12:42 | INFO | train_inner | epoch 140:   1901 / 1978 loss=3.042, nll_loss=0.904, word_ins=2.725, length=3.17, ppl=8.24, wps=49987, ups=0.85, wpb=58902.2, bsz=1899.4, num_updates=276800, lr=0.000190071, gnorm=1.596, loss_scale=4096, train_wall=118, wall=0
2023-01-12 16:14:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 16:14:27 | INFO | valid | epoch 140 | valid on 'valid' subset | loss 4.467 | nll_loss 1.99 | word_ins 3.75 | length 7.168 | ppl 22.11 | wps 127726 | wpb 40242.5 | bsz 1500 | num_updates 276877 | best_loss 4.274
2023-01-12 16:14:27 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 16:14:30 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint140.pt (epoch 140 @ 276877 updates, score 4.467) (writing took 2.810742271132767 seconds)
2023-01-12 16:14:30 | INFO | fairseq_cli.train | end of epoch 140 (average epoch stats below)
2023-01-12 16:14:30 | INFO | train | epoch 140 | loss 3.019 | nll_loss 0.884 | word_ins 2.707 | length 3.119 | ppl 8.11 | wps 49527.8 | ups 0.84 | wpb 59284.3 | bsz 2002.6 | num_updates 276877 | lr 0.000190045 | gnorm 1.577 | loss_scale 4096 | train_wall 2338 | wall 0
2023-01-12 16:14:30 | INFO | fairseq.trainer | begin training epoch 141
2023-01-12 16:15:06 | INFO | train_inner | epoch 141:     23 / 1978 loss=3.012, nll_loss=0.876, word_ins=2.699, length=3.127, ppl=8.07, wps=41498.4, ups=0.7, wpb=59550.7, bsz=2001.9, num_updates=276900, lr=0.000190037, gnorm=1.558, loss_scale=8192, train_wall=118, wall=0
2023-01-12 16:17:04 | INFO | train_inner | epoch 141:    123 / 1978 loss=3.009, nll_loss=0.874, word_ins=2.698, length=3.115, ppl=8.05, wps=49869.6, ups=0.85, wpb=58903.3, bsz=1971.8, num_updates=277000, lr=0.000190003, gnorm=1.594, loss_scale=8192, train_wall=118, wall=0
2023-01-12 16:19:02 | INFO | train_inner | epoch 141:    223 / 1978 loss=3.004, nll_loss=0.87, word_ins=2.694, length=3.096, ppl=8.02, wps=50128.8, ups=0.84, wpb=59354.9, bsz=2040.2, num_updates=277100, lr=0.000189969, gnorm=1.555, loss_scale=8192, train_wall=118, wall=0
2023-01-12 16:21:02 | INFO | train_inner | epoch 141:    323 / 1978 loss=2.995, nll_loss=0.866, word_ins=2.69, length=3.049, ppl=7.97, wps=50022, ups=0.84, wpb=59825.3, bsz=2079.1, num_updates=277200, lr=0.000189934, gnorm=1.551, loss_scale=8192, train_wall=119, wall=0
2023-01-12 16:23:02 | INFO | train_inner | epoch 141:    423 / 1978 loss=2.997, nll_loss=0.867, word_ins=2.691, length=3.057, ppl=7.98, wps=49637.3, ups=0.84, wpb=59411.9, bsz=2055.3, num_updates=277300, lr=0.0001899, gnorm=1.555, loss_scale=8192, train_wall=119, wall=0
2023-01-12 16:25:01 | INFO | train_inner | epoch 141:    523 / 1978 loss=3.011, nll_loss=0.879, word_ins=2.702, length=3.094, ppl=8.06, wps=49708.5, ups=0.84, wpb=59139.3, bsz=2060.7, num_updates=277400, lr=0.000189866, gnorm=1.597, loss_scale=8192, train_wall=119, wall=0
2023-01-12 16:26:58 | INFO | train_inner | epoch 141:    623 / 1978 loss=3.035, nll_loss=0.897, word_ins=2.719, length=3.161, ppl=8.2, wps=50712.6, ups=0.85, wpb=59653.7, bsz=1869.5, num_updates=277500, lr=0.000189832, gnorm=1.636, loss_scale=8192, train_wall=117, wall=0
2023-01-12 16:28:57 | INFO | train_inner | epoch 141:    723 / 1978 loss=3.027, nll_loss=0.893, word_ins=2.715, length=3.122, ppl=8.15, wps=49788.4, ups=0.84, wpb=59114.8, bsz=1987.7, num_updates=277600, lr=0.000189797, gnorm=1.58, loss_scale=8192, train_wall=118, wall=0
2023-01-12 16:30:56 | INFO | train_inner | epoch 141:    823 / 1978 loss=2.996, nll_loss=0.863, word_ins=2.687, length=3.086, ppl=7.98, wps=50224, ups=0.84, wpb=59867.4, bsz=2052.6, num_updates=277700, lr=0.000189763, gnorm=1.537, loss_scale=8192, train_wall=119, wall=0
2023-01-12 16:32:54 | INFO | train_inner | epoch 141:    923 / 1978 loss=3.018, nll_loss=0.886, word_ins=2.709, length=3.094, ppl=8.1, wps=50158.5, ups=0.85, wpb=58985.1, bsz=2025.9, num_updates=277800, lr=0.000189729, gnorm=1.53, loss_scale=8192, train_wall=117, wall=0
2023-01-12 16:34:52 | INFO | train_inner | epoch 141:   1023 / 1978 loss=3.028, nll_loss=0.897, word_ins=2.719, length=3.094, ppl=8.16, wps=49860.6, ups=0.85, wpb=58757.6, bsz=1967.1, num_updates=277900, lr=0.000189695, gnorm=1.576, loss_scale=8192, train_wall=118, wall=0
2023-01-12 16:36:50 | INFO | train_inner | epoch 141:   1123 / 1978 loss=3.007, nll_loss=0.87, word_ins=2.694, length=3.127, ppl=8.04, wps=49711.6, ups=0.84, wpb=59012.2, bsz=2061.8, num_updates=278000, lr=0.000189661, gnorm=1.561, loss_scale=8192, train_wall=118, wall=0
2023-01-12 16:38:50 | INFO | train_inner | epoch 141:   1223 / 1978 loss=3.002, nll_loss=0.87, word_ins=2.694, length=3.079, ppl=8.01, wps=49488.1, ups=0.84, wpb=59231.5, bsz=2121.4, num_updates=278100, lr=0.000189627, gnorm=1.597, loss_scale=8192, train_wall=119, wall=0
2023-01-12 16:40:48 | INFO | train_inner | epoch 141:   1323 / 1978 loss=3.041, nll_loss=0.901, word_ins=2.722, length=3.184, ppl=8.23, wps=50406.7, ups=0.85, wpb=59503.8, bsz=1882.4, num_updates=278200, lr=0.000189593, gnorm=1.645, loss_scale=8192, train_wall=118, wall=0
2023-01-12 16:42:46 | INFO | train_inner | epoch 141:   1423 / 1978 loss=3.009, nll_loss=0.877, word_ins=2.7, length=3.092, ppl=8.05, wps=50577, ups=0.85, wpb=59747.2, bsz=2031, num_updates=278300, lr=0.000189559, gnorm=1.518, loss_scale=8192, train_wall=118, wall=0
2023-01-12 16:44:46 | INFO | train_inner | epoch 141:   1523 / 1978 loss=3.02, nll_loss=0.887, word_ins=2.709, length=3.104, ppl=8.11, wps=49765.4, ups=0.84, wpb=59391.7, bsz=2065.8, num_updates=278400, lr=0.000189525, gnorm=1.591, loss_scale=8192, train_wall=119, wall=0
2023-01-12 16:46:43 | INFO | train_inner | epoch 141:   1623 / 1978 loss=3.063, nll_loss=0.922, word_ins=2.742, length=3.209, ppl=8.36, wps=49938, ups=0.85, wpb=58826.3, bsz=1839.3, num_updates=278500, lr=0.00018949, gnorm=1.639, loss_scale=8192, train_wall=118, wall=0
2023-01-12 16:48:42 | INFO | train_inner | epoch 141:   1723 / 1978 loss=3.012, nll_loss=0.875, word_ins=2.698, length=3.144, ppl=8.07, wps=50136.1, ups=0.85, wpb=59245.2, bsz=2006.7, num_updates=278600, lr=0.000189456, gnorm=1.537, loss_scale=8192, train_wall=118, wall=0
2023-01-12 16:50:39 | INFO | train_inner | epoch 141:   1823 / 1978 loss=3.028, nll_loss=0.889, word_ins=2.711, length=3.175, ppl=8.16, wps=50319.2, ups=0.85, wpb=59307.3, bsz=2005.5, num_updates=278700, lr=0.000189422, gnorm=1.615, loss_scale=8192, train_wall=118, wall=0
2023-01-12 16:52:38 | INFO | train_inner | epoch 141:   1923 / 1978 loss=3.024, nll_loss=0.891, word_ins=2.713, length=3.109, ppl=8.13, wps=49637.9, ups=0.84, wpb=58860.1, bsz=2037.3, num_updates=278800, lr=0.000189389, gnorm=1.535, loss_scale=8192, train_wall=118, wall=0
2023-01-12 16:53:42 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 16:53:55 | INFO | valid | epoch 141 | valid on 'valid' subset | loss 4.419 | nll_loss 1.988 | word_ins 3.746 | length 6.741 | ppl 21.4 | wps 100633 | wpb 40242.5 | bsz 1500 | num_updates 278855 | best_loss 4.274
2023-01-12 16:53:55 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 16:53:58 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint141.pt (epoch 141 @ 278855 updates, score 4.419) (writing took 2.9480547881685197 seconds)
2023-01-12 16:53:58 | INFO | fairseq_cli.train | end of epoch 141 (average epoch stats below)
2023-01-12 16:53:58 | INFO | train | epoch 141 | loss 3.019 | nll_loss 0.884 | word_ins 2.707 | length 3.119 | ppl 8.1 | wps 49521.3 | ups 0.84 | wpb 59284.3 | bsz 2002.6 | num_updates 278855 | lr 0.00018937 | gnorm 1.582 | loss_scale 8192 | train_wall 2339 | wall 0
2023-01-12 16:53:58 | INFO | fairseq.trainer | begin training epoch 142
2023-01-12 16:54:59 | INFO | train_inner | epoch 142:     45 / 1978 loss=3.047, nll_loss=0.906, word_ins=2.727, length=3.199, ppl=8.27, wps=41561.3, ups=0.71, wpb=58653.3, bsz=1857.4, num_updates=278900, lr=0.000189355, gnorm=1.646, loss_scale=8192, train_wall=117, wall=0
2023-01-12 16:56:58 | INFO | train_inner | epoch 142:    145 / 1978 loss=2.99, nll_loss=0.858, word_ins=2.683, length=3.071, ppl=7.95, wps=50045, ups=0.84, wpb=59477.5, bsz=2098.2, num_updates=279000, lr=0.000189321, gnorm=1.552, loss_scale=8192, train_wall=119, wall=0
2023-01-12 16:58:57 | INFO | train_inner | epoch 142:    245 / 1978 loss=3.042, nll_loss=0.9, word_ins=2.722, length=3.197, ppl=8.23, wps=49861.7, ups=0.84, wpb=59164.1, bsz=1893.8, num_updates=279100, lr=0.000189287, gnorm=1.668, loss_scale=8192, train_wall=118, wall=0
2023-01-12 17:00:55 | INFO | train_inner | epoch 142:    345 / 1978 loss=3.014, nll_loss=0.88, word_ins=2.704, length=3.105, ppl=8.08, wps=49660.4, ups=0.85, wpb=58716.8, bsz=2022, num_updates=279200, lr=0.000189253, gnorm=1.598, loss_scale=8192, train_wall=118, wall=0
2023-01-12 17:02:53 | INFO | train_inner | epoch 142:    445 / 1978 loss=2.998, nll_loss=0.866, word_ins=2.69, length=3.075, ppl=7.99, wps=50148.2, ups=0.84, wpb=59392.8, bsz=2052.7, num_updates=279300, lr=0.000189219, gnorm=1.574, loss_scale=8192, train_wall=118, wall=0
2023-01-12 17:04:52 | INFO | train_inner | epoch 142:    545 / 1978 loss=3.002, nll_loss=0.869, word_ins=2.692, length=3.097, ppl=8.01, wps=50215.5, ups=0.84, wpb=59705.8, bsz=2026.2, num_updates=279400, lr=0.000189185, gnorm=1.603, loss_scale=8192, train_wall=119, wall=0
2023-01-12 17:05:11 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 4096.0
2023-01-12 17:06:53 | INFO | train_inner | epoch 142:    646 / 1978 loss=2.993, nll_loss=0.863, word_ins=2.687, length=3.057, ppl=7.96, wps=49435.2, ups=0.83, wpb=59652.1, bsz=2112.1, num_updates=279500, lr=0.000189151, gnorm=1.514, loss_scale=4096, train_wall=120, wall=0
2023-01-12 17:08:51 | INFO | train_inner | epoch 142:    746 / 1978 loss=3.01, nll_loss=0.875, word_ins=2.699, length=3.113, ppl=8.06, wps=49953.2, ups=0.84, wpb=59183.2, bsz=2002.9, num_updates=279600, lr=0.000189117, gnorm=1.627, loss_scale=4096, train_wall=118, wall=0
2023-01-12 17:10:49 | INFO | train_inner | epoch 142:    846 / 1978 loss=3.019, nll_loss=0.884, word_ins=2.706, length=3.127, ppl=8.11, wps=50682.8, ups=0.85, wpb=59875.6, bsz=1974, num_updates=279700, lr=0.000189084, gnorm=1.605, loss_scale=4096, train_wall=118, wall=0
2023-01-12 17:12:48 | INFO | train_inner | epoch 142:    946 / 1978 loss=3.018, nll_loss=0.883, word_ins=2.705, length=3.128, ppl=8.1, wps=49868.1, ups=0.84, wpb=59321.9, bsz=1974.3, num_updates=279800, lr=0.00018905, gnorm=1.563, loss_scale=4096, train_wall=119, wall=0
2023-01-12 17:14:47 | INFO | train_inner | epoch 142:   1046 / 1978 loss=3.028, nll_loss=0.894, word_ins=2.716, length=3.122, ppl=8.16, wps=50021.1, ups=0.84, wpb=59399.5, bsz=2023, num_updates=279900, lr=0.000189016, gnorm=1.671, loss_scale=4096, train_wall=119, wall=0
2023-01-12 17:16:45 | INFO | train_inner | epoch 142:   1146 / 1978 loss=3.002, nll_loss=0.87, word_ins=2.693, length=3.086, ppl=8.01, wps=50122.3, ups=0.85, wpb=59282.5, bsz=2074.6, num_updates=280000, lr=0.000188982, gnorm=1.572, loss_scale=4096, train_wall=118, wall=0
2023-01-12 17:18:44 | INFO | train_inner | epoch 142:   1246 / 1978 loss=3.03, nll_loss=0.893, word_ins=2.715, length=3.154, ppl=8.17, wps=49956.3, ups=0.84, wpb=59154.1, bsz=1967, num_updates=280100, lr=0.000188948, gnorm=1.607, loss_scale=4096, train_wall=118, wall=0
2023-01-12 17:20:42 | INFO | train_inner | epoch 142:   1346 / 1978 loss=3.022, nll_loss=0.884, word_ins=2.707, length=3.146, ppl=8.12, wps=50551.1, ups=0.85, wpb=59698.8, bsz=1964.1, num_updates=280200, lr=0.000188915, gnorm=1.533, loss_scale=4096, train_wall=118, wall=0
2023-01-12 17:22:40 | INFO | train_inner | epoch 142:   1446 / 1978 loss=3.039, nll_loss=0.9, word_ins=2.722, length=3.17, ppl=8.22, wps=49895, ups=0.85, wpb=58763.8, bsz=1947, num_updates=280300, lr=0.000188881, gnorm=1.587, loss_scale=4096, train_wall=118, wall=0
2023-01-12 17:24:37 | INFO | train_inner | epoch 142:   1546 / 1978 loss=3.043, nll_loss=0.902, word_ins=2.723, length=3.205, ppl=8.24, wps=50584.8, ups=0.85, wpb=59394.6, bsz=1897.4, num_updates=280400, lr=0.000188847, gnorm=1.647, loss_scale=4096, train_wall=117, wall=0
2023-01-12 17:26:37 | INFO | train_inner | epoch 142:   1646 / 1978 loss=3.002, nll_loss=0.87, word_ins=2.694, length=3.086, ppl=8.01, wps=50073.1, ups=0.84, wpb=59736.1, bsz=2063.9, num_updates=280500, lr=0.000188814, gnorm=1.609, loss_scale=4096, train_wall=119, wall=0
2023-01-12 17:28:35 | INFO | train_inner | epoch 142:   1746 / 1978 loss=3.01, nll_loss=0.879, word_ins=2.701, length=3.082, ppl=8.05, wps=49852.7, ups=0.84, wpb=59316.9, bsz=2015.3, num_updates=280600, lr=0.00018878, gnorm=1.589, loss_scale=4096, train_wall=119, wall=0
2023-01-12 17:29:55 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2023-01-12 17:30:35 | INFO | train_inner | epoch 142:   1847 / 1978 loss=3.033, nll_loss=0.895, word_ins=2.717, length=3.161, ppl=8.18, wps=49161.3, ups=0.84, wpb=58869.3, bsz=1939.4, num_updates=280700, lr=0.000188746, gnorm=1.57, loss_scale=2048, train_wall=120, wall=0
2023-01-12 17:32:34 | INFO | train_inner | epoch 142:   1947 / 1978 loss=3.032, nll_loss=0.896, word_ins=2.717, length=3.144, ppl=8.18, wps=49816.5, ups=0.84, wpb=59048.4, bsz=2004.2, num_updates=280800, lr=0.000188713, gnorm=1.625, loss_scale=2048, train_wall=118, wall=0
2023-01-12 17:33:10 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 17:33:23 | INFO | valid | epoch 142 | valid on 'valid' subset | loss 4.503 | nll_loss 2.004 | word_ins 3.757 | length 7.454 | ppl 22.67 | wps 123855 | wpb 40242.5 | bsz 1500 | num_updates 280831 | best_loss 4.274
2023-01-12 17:33:23 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 17:33:28 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint142.pt (epoch 142 @ 280831 updates, score 4.503) (writing took 4.519744077697396 seconds)
2023-01-12 17:33:28 | INFO | fairseq_cli.train | end of epoch 142 (average epoch stats below)
2023-01-12 17:33:28 | INFO | train | epoch 142 | loss 3.016 | nll_loss 0.881 | word_ins 2.704 | length 3.12 | ppl 8.09 | wps 49431.3 | ups 0.83 | wpb 59291 | bsz 2003.1 | num_updates 280831 | lr 0.000188702 | gnorm 1.59 | loss_scale 2048 | train_wall 2340 | wall 0
2023-01-12 17:33:28 | INFO | fairseq.trainer | begin training epoch 143
2023-01-12 17:35:00 | INFO | train_inner | epoch 143:     69 / 1978 loss=2.978, nll_loss=0.854, word_ins=2.679, length=2.994, ppl=7.88, wps=40422, ups=0.69, wpb=58919.7, bsz=2130.2, num_updates=280900, lr=0.000188679, gnorm=1.519, loss_scale=2048, train_wall=119, wall=0
2023-01-12 17:36:59 | INFO | train_inner | epoch 143:    169 / 1978 loss=3.017, nll_loss=0.884, word_ins=2.707, length=3.101, ppl=8.09, wps=49601.8, ups=0.84, wpb=59044, bsz=2043.4, num_updates=281000, lr=0.000188646, gnorm=1.538, loss_scale=2048, train_wall=119, wall=0
2023-01-12 17:38:57 | INFO | train_inner | epoch 143:    269 / 1978 loss=3.003, nll_loss=0.873, word_ins=2.697, length=3.064, ppl=8.02, wps=50153, ups=0.85, wpb=59343, bsz=2043.4, num_updates=281100, lr=0.000188612, gnorm=1.578, loss_scale=2048, train_wall=118, wall=0
2023-01-12 17:40:56 | INFO | train_inner | epoch 143:    369 / 1978 loss=3.019, nll_loss=0.885, word_ins=2.708, length=3.109, ppl=8.11, wps=50333.7, ups=0.84, wpb=60001.4, bsz=1983, num_updates=281200, lr=0.000188579, gnorm=1.66, loss_scale=2048, train_wall=119, wall=0
2023-01-12 17:42:54 | INFO | train_inner | epoch 143:    469 / 1978 loss=3.035, nll_loss=0.893, word_ins=2.715, length=3.201, ppl=8.2, wps=49818, ups=0.85, wpb=58908.1, bsz=1934, num_updates=281300, lr=0.000188545, gnorm=1.624, loss_scale=2048, train_wall=118, wall=0
2023-01-12 17:44:53 | INFO | train_inner | epoch 143:    569 / 1978 loss=3.01, nll_loss=0.88, word_ins=2.703, length=3.077, ppl=8.06, wps=50203.9, ups=0.84, wpb=59744.1, bsz=2025, num_updates=281400, lr=0.000188512, gnorm=1.572, loss_scale=2048, train_wall=119, wall=0
2023-01-12 17:46:52 | INFO | train_inner | epoch 143:    669 / 1978 loss=3.033, nll_loss=0.894, word_ins=2.716, length=3.169, ppl=8.18, wps=50039.7, ups=0.85, wpb=59195.5, bsz=1993.8, num_updates=281500, lr=0.000188478, gnorm=1.605, loss_scale=2048, train_wall=118, wall=0
2023-01-12 17:48:49 | INFO | train_inner | epoch 143:    769 / 1978 loss=3.026, nll_loss=0.89, word_ins=2.712, length=3.136, ppl=8.14, wps=50160.6, ups=0.85, wpb=59088.6, bsz=1976.7, num_updates=281600, lr=0.000188445, gnorm=1.599, loss_scale=2048, train_wall=118, wall=0
2023-01-12 17:50:48 | INFO | train_inner | epoch 143:    869 / 1978 loss=3.004, nll_loss=0.872, word_ins=2.696, length=3.088, ppl=8.02, wps=50558.8, ups=0.85, wpb=59754.3, bsz=1970.8, num_updates=281700, lr=0.000188411, gnorm=1.598, loss_scale=2048, train_wall=118, wall=0
2023-01-12 17:52:46 | INFO | train_inner | epoch 143:    969 / 1978 loss=3.002, nll_loss=0.868, word_ins=2.692, length=3.107, ppl=8.01, wps=49844.9, ups=0.84, wpb=59168.3, bsz=2022.7, num_updates=281800, lr=0.000188378, gnorm=1.564, loss_scale=2048, train_wall=118, wall=0
2023-01-12 17:54:44 | INFO | train_inner | epoch 143:   1069 / 1978 loss=3.02, nll_loss=0.882, word_ins=2.705, length=3.151, ppl=8.11, wps=50382.3, ups=0.85, wpb=59334.3, bsz=1917.7, num_updates=281900, lr=0.000188344, gnorm=1.573, loss_scale=2048, train_wall=118, wall=0
2023-01-12 17:56:42 | INFO | train_inner | epoch 143:   1169 / 1978 loss=3.016, nll_loss=0.881, word_ins=2.703, length=3.133, ppl=8.09, wps=50581.9, ups=0.85, wpb=59655.6, bsz=1985.1, num_updates=282000, lr=0.000188311, gnorm=1.586, loss_scale=2048, train_wall=118, wall=0
2023-01-12 17:58:40 | INFO | train_inner | epoch 143:   1269 / 1978 loss=3.023, nll_loss=0.886, word_ins=2.708, length=3.155, ppl=8.13, wps=50080.6, ups=0.85, wpb=59250.1, bsz=2009.8, num_updates=282100, lr=0.000188278, gnorm=1.599, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:00:38 | INFO | train_inner | epoch 143:   1369 / 1978 loss=3.022, nll_loss=0.887, word_ins=2.709, length=3.125, ppl=8.12, wps=49938.3, ups=0.85, wpb=58947, bsz=2000.8, num_updates=282200, lr=0.000188244, gnorm=1.638, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:02:36 | INFO | train_inner | epoch 143:   1469 / 1978 loss=3.003, nll_loss=0.871, word_ins=2.695, length=3.087, ppl=8.02, wps=49945.9, ups=0.85, wpb=58845.4, bsz=2007.4, num_updates=282300, lr=0.000188211, gnorm=1.539, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:04:34 | INFO | train_inner | epoch 143:   1569 / 1978 loss=3.024, nll_loss=0.889, word_ins=2.711, length=3.13, ppl=8.13, wps=49973.9, ups=0.85, wpb=58979.3, bsz=1994.6, num_updates=282400, lr=0.000188177, gnorm=1.542, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:06:33 | INFO | train_inner | epoch 143:   1669 / 1978 loss=3.008, nll_loss=0.877, word_ins=2.7, length=3.087, ppl=8.05, wps=49719.8, ups=0.84, wpb=59008.1, bsz=2045.8, num_updates=282500, lr=0.000188144, gnorm=1.552, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:08:32 | INFO | train_inner | epoch 143:   1769 / 1978 loss=3.017, nll_loss=0.883, word_ins=2.705, length=3.124, ppl=8.1, wps=49582.8, ups=0.84, wpb=58832.4, bsz=2031.9, num_updates=282600, lr=0.000188111, gnorm=1.653, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:10:31 | INFO | train_inner | epoch 143:   1869 / 1978 loss=3.001, nll_loss=0.868, word_ins=2.692, length=3.09, ppl=8, wps=50372.5, ups=0.84, wpb=59916.9, bsz=2030.8, num_updates=282700, lr=0.000188078, gnorm=1.653, loss_scale=2048, train_wall=119, wall=0
2023-01-12 18:12:29 | INFO | train_inner | epoch 143:   1969 / 1978 loss=3.028, nll_loss=0.888, word_ins=2.71, length=3.187, ppl=8.16, wps=50421.1, ups=0.84, wpb=59777.8, bsz=1957, num_updates=282800, lr=0.000188044, gnorm=1.619, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:12:40 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 18:12:52 | INFO | valid | epoch 143 | valid on 'valid' subset | loss 4.511 | nll_loss 2.009 | word_ins 3.757 | length 7.54 | ppl 22.8 | wps 159870 | wpb 40242.5 | bsz 1500 | num_updates 282809 | best_loss 4.274
2023-01-12 18:12:52 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 18:12:55 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint143.pt (epoch 143 @ 282809 updates, score 4.511) (writing took 3.0723237837664783 seconds)
2023-01-12 18:12:55 | INFO | fairseq_cli.train | end of epoch 143 (average epoch stats below)
2023-01-12 18:12:55 | INFO | train | epoch 143 | loss 3.015 | nll_loss 0.881 | word_ins 2.704 | length 3.119 | ppl 8.09 | wps 49539.9 | ups 0.84 | wpb 59284.3 | bsz 2002.6 | num_updates 282809 | lr 0.000188041 | gnorm 1.593 | loss_scale 2048 | train_wall 2339 | wall 0
2023-01-12 18:12:55 | INFO | fairseq.trainer | begin training epoch 144
2023-01-12 18:14:53 | INFO | train_inner | epoch 144:     91 / 1978 loss=3.002, nll_loss=0.868, word_ins=2.692, length=3.101, ppl=8.01, wps=41458.1, ups=0.7, wpb=59486.6, bsz=2062.9, num_updates=282900, lr=0.000188011, gnorm=1.559, loss_scale=2048, train_wall=119, wall=0
2023-01-12 18:16:51 | INFO | train_inner | epoch 144:    191 / 1978 loss=3.008, nll_loss=0.877, word_ins=2.7, length=3.08, ppl=8.04, wps=49957.9, ups=0.84, wpb=59277.1, bsz=2003.5, num_updates=283000, lr=0.000187978, gnorm=1.609, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:18:50 | INFO | train_inner | epoch 144:    291 / 1978 loss=3.013, nll_loss=0.877, word_ins=2.7, length=3.127, ppl=8.07, wps=49580.5, ups=0.84, wpb=59092.2, bsz=2027, num_updates=283100, lr=0.000187945, gnorm=1.63, loss_scale=2048, train_wall=119, wall=0
2023-01-12 18:20:50 | INFO | train_inner | epoch 144:    391 / 1978 loss=2.993, nll_loss=0.864, word_ins=2.688, length=3.053, ppl=7.96, wps=49976.1, ups=0.84, wpb=59838.9, bsz=2086.3, num_updates=283200, lr=0.000187912, gnorm=1.579, loss_scale=2048, train_wall=120, wall=0
2023-01-12 18:22:49 | INFO | train_inner | epoch 144:    491 / 1978 loss=3.007, nll_loss=0.876, word_ins=2.699, length=3.084, ppl=8.04, wps=49756.5, ups=0.84, wpb=59153.4, bsz=1988.7, num_updates=283300, lr=0.000187878, gnorm=1.547, loss_scale=2048, train_wall=119, wall=0
2023-01-12 18:24:47 | INFO | train_inner | epoch 144:    591 / 1978 loss=3.021, nll_loss=0.885, word_ins=2.708, length=3.129, ppl=8.12, wps=50261.4, ups=0.85, wpb=59091.4, bsz=1917.8, num_updates=283400, lr=0.000187845, gnorm=1.602, loss_scale=2048, train_wall=117, wall=0
2023-01-12 18:27:58 | INFO | train_inner | epoch 144:    691 / 1978 loss=3.003, nll_loss=0.869, word_ins=2.693, length=3.103, ppl=8.02, wps=30963.2, ups=0.52, wpb=59096.5, bsz=2021.8, num_updates=283500, lr=0.000187812, gnorm=1.577, loss_scale=2048, train_wall=191, wall=0
2023-01-12 18:29:55 | INFO | train_inner | epoch 144:    791 / 1978 loss=3.019, nll_loss=0.88, word_ins=2.703, length=3.158, ppl=8.1, wps=50463.5, ups=0.85, wpb=59444.3, bsz=1956, num_updates=283600, lr=0.000187779, gnorm=1.614, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:31:53 | INFO | train_inner | epoch 144:    891 / 1978 loss=3.007, nll_loss=0.873, word_ins=2.696, length=3.105, ppl=8.04, wps=50305.7, ups=0.85, wpb=59217.7, bsz=1981.1, num_updates=283700, lr=0.000187746, gnorm=1.556, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:33:52 | INFO | train_inner | epoch 144:    991 / 1978 loss=3, nll_loss=0.867, word_ins=2.691, length=3.087, ppl=8, wps=49803.5, ups=0.84, wpb=59178.5, bsz=2067.8, num_updates=283800, lr=0.000187713, gnorm=1.533, loss_scale=2048, train_wall=119, wall=0
2023-01-12 18:35:50 | INFO | train_inner | epoch 144:   1091 / 1978 loss=3.012, nll_loss=0.878, word_ins=2.701, length=3.107, ppl=8.07, wps=50009.4, ups=0.85, wpb=58982.5, bsz=2014.6, num_updates=283900, lr=0.00018768, gnorm=1.469, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:37:48 | INFO | train_inner | epoch 144:   1191 / 1978 loss=3.021, nll_loss=0.887, word_ins=2.709, length=3.127, ppl=8.12, wps=50423.6, ups=0.85, wpb=59509.4, bsz=2005.1, num_updates=284000, lr=0.000187647, gnorm=1.578, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:39:46 | INFO | train_inner | epoch 144:   1291 / 1978 loss=3.032, nll_loss=0.897, word_ins=2.719, length=3.138, ppl=8.18, wps=50289.1, ups=0.85, wpb=59269.2, bsz=1937.6, num_updates=284100, lr=0.000187614, gnorm=1.685, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:41:44 | INFO | train_inner | epoch 144:   1391 / 1978 loss=3.013, nll_loss=0.879, word_ins=2.703, length=3.098, ppl=8.07, wps=50331, ups=0.85, wpb=59484.2, bsz=1975, num_updates=284200, lr=0.000187581, gnorm=1.666, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:43:42 | INFO | train_inner | epoch 144:   1491 / 1978 loss=3.019, nll_loss=0.884, word_ins=2.707, length=3.122, ppl=8.11, wps=49954.1, ups=0.85, wpb=58874.8, bsz=1984.2, num_updates=284300, lr=0.000187548, gnorm=1.569, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:45:41 | INFO | train_inner | epoch 144:   1591 / 1978 loss=3.014, nll_loss=0.877, word_ins=2.701, length=3.127, ppl=8.08, wps=49677.5, ups=0.84, wpb=58998.9, bsz=2047.3, num_updates=284400, lr=0.000187515, gnorm=1.545, loss_scale=2048, train_wall=119, wall=0
2023-01-12 18:47:40 | INFO | train_inner | epoch 144:   1691 / 1978 loss=3.013, nll_loss=0.878, word_ins=2.701, length=3.124, ppl=8.07, wps=49567.3, ups=0.84, wpb=59166.7, bsz=2067, num_updates=284500, lr=0.000187482, gnorm=1.608, loss_scale=2048, train_wall=119, wall=0
2023-01-12 18:49:38 | INFO | train_inner | epoch 144:   1791 / 1978 loss=3.024, nll_loss=0.889, word_ins=2.712, length=3.128, ppl=8.14, wps=50446.2, ups=0.85, wpb=59467.4, bsz=1964.4, num_updates=284600, lr=0.000187449, gnorm=1.611, loss_scale=2048, train_wall=118, wall=0
2023-01-12 18:51:35 | INFO | train_inner | epoch 144:   1891 / 1978 loss=3.036, nll_loss=0.898, word_ins=2.718, length=3.176, ppl=8.2, wps=50844.5, ups=0.85, wpb=59782.7, bsz=1911.7, num_updates=284700, lr=0.000187416, gnorm=1.65, loss_scale=2048, train_wall=117, wall=0
2023-01-12 18:53:19 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 18:53:32 | INFO | valid | epoch 144 | valid on 'valid' subset | loss 4.498 | nll_loss 1.979 | word_ins 3.735 | length 7.638 | ppl 22.6 | wps 173097 | wpb 40242.5 | bsz 1500 | num_updates 284787 | best_loss 4.274
2023-01-12 18:53:32 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 18:53:35 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint144.pt (epoch 144 @ 284787 updates, score 4.498) (writing took 2.787642390001565 seconds)
2023-01-12 18:53:35 | INFO | fairseq_cli.train | end of epoch 144 (average epoch stats below)
2023-01-12 18:53:35 | INFO | train | epoch 144 | loss 3.014 | nll_loss 0.88 | word_ins 2.702 | length 3.116 | ppl 8.08 | wps 48065.6 | ups 0.81 | wpb 59284.3 | bsz 2002.6 | num_updates 284787 | lr 0.000187387 | gnorm 1.591 | loss_scale 4096 | train_wall 2411 | wall 0
2023-01-12 18:53:35 | INFO | fairseq.trainer | begin training epoch 145
2023-01-12 18:53:59 | INFO | train_inner | epoch 145:     13 / 1978 loss=3.022, nll_loss=0.886, word_ins=2.708, length=3.14, ppl=8.12, wps=41186.4, ups=0.7, wpb=59216.3, bsz=2043.5, num_updates=284800, lr=0.000187383, gnorm=1.603, loss_scale=4096, train_wall=119, wall=0
2023-01-12 18:56:00 | INFO | train_inner | epoch 145:    113 / 1978 loss=2.962, nll_loss=0.833, word_ins=2.66, length=3.018, ppl=7.79, wps=49941.9, ups=0.83, wpb=60146.7, bsz=2209, num_updates=284900, lr=0.00018735, gnorm=1.622, loss_scale=4096, train_wall=120, wall=0
2023-01-12 18:57:57 | INFO | train_inner | epoch 145:    213 / 1978 loss=3.02, nll_loss=0.884, word_ins=2.707, length=3.133, ppl=8.11, wps=50466.3, ups=0.85, wpb=59264.7, bsz=1905.4, num_updates=285000, lr=0.000187317, gnorm=1.631, loss_scale=4096, train_wall=117, wall=0
2023-01-12 18:59:56 | INFO | train_inner | epoch 145:    313 / 1978 loss=2.996, nll_loss=0.859, word_ins=2.684, length=3.116, ppl=7.98, wps=49956.7, ups=0.84, wpb=59323.5, bsz=2062.7, num_updates=285100, lr=0.000187284, gnorm=1.59, loss_scale=4096, train_wall=119, wall=0
2023-01-12 19:01:54 | INFO | train_inner | epoch 145:    413 / 1978 loss=3.028, nll_loss=0.886, word_ins=2.709, length=3.187, ppl=8.15, wps=49860.9, ups=0.85, wpb=58862.8, bsz=2050.3, num_updates=285200, lr=0.000187251, gnorm=1.605, loss_scale=4096, train_wall=118, wall=0
2023-01-12 19:03:52 | INFO | train_inner | epoch 145:    513 / 1978 loss=3.01, nll_loss=0.88, word_ins=2.703, length=3.075, ppl=8.06, wps=50171.6, ups=0.84, wpb=59434.9, bsz=2027.9, num_updates=285300, lr=0.000187219, gnorm=1.586, loss_scale=4096, train_wall=118, wall=0
2023-01-12 19:05:51 | INFO | train_inner | epoch 145:    613 / 1978 loss=3.011, nll_loss=0.88, word_ins=2.703, length=3.079, ppl=8.06, wps=49790.3, ups=0.84, wpb=58927.1, bsz=2011.5, num_updates=285400, lr=0.000187186, gnorm=1.606, loss_scale=4096, train_wall=118, wall=0
2023-01-12 19:07:49 | INFO | train_inner | epoch 145:    713 / 1978 loss=3.003, nll_loss=0.868, word_ins=2.692, length=3.106, ppl=8.01, wps=50228.9, ups=0.84, wpb=59451, bsz=1970.2, num_updates=285500, lr=0.000187153, gnorm=1.574, loss_scale=4096, train_wall=118, wall=0
2023-01-12 19:09:48 | INFO | train_inner | epoch 145:    813 / 1978 loss=3.004, nll_loss=0.871, word_ins=2.694, length=3.096, ppl=8.02, wps=49724.9, ups=0.84, wpb=59387.1, bsz=2038.3, num_updates=285600, lr=0.00018712, gnorm=1.56, loss_scale=4096, train_wall=119, wall=0
2023-01-12 19:11:46 | INFO | train_inner | epoch 145:    913 / 1978 loss=3.034, nll_loss=0.893, word_ins=2.715, length=3.197, ppl=8.19, wps=50371.6, ups=0.85, wpb=59293.1, bsz=1910.5, num_updates=285700, lr=0.000187088, gnorm=1.557, loss_scale=4096, train_wall=118, wall=0
2023-01-12 19:13:45 | INFO | train_inner | epoch 145:   1013 / 1978 loss=2.996, nll_loss=0.864, word_ins=2.689, length=3.08, ppl=7.98, wps=50025.1, ups=0.84, wpb=59405.4, bsz=2028.3, num_updates=285800, lr=0.000187055, gnorm=1.549, loss_scale=4096, train_wall=119, wall=0
2023-01-12 19:15:44 | INFO | train_inner | epoch 145:   1113 / 1978 loss=3.022, nll_loss=0.888, word_ins=2.711, length=3.118, ppl=8.13, wps=49863.4, ups=0.84, wpb=59263.2, bsz=1976.8, num_updates=285900, lr=0.000187022, gnorm=1.631, loss_scale=4096, train_wall=119, wall=0
2023-01-12 19:17:43 | INFO | train_inner | epoch 145:   1213 / 1978 loss=3.013, nll_loss=0.882, word_ins=2.704, length=3.089, ppl=8.07, wps=50160.7, ups=0.84, wpb=59609.9, bsz=1971.8, num_updates=286000, lr=0.000186989, gnorm=1.645, loss_scale=4096, train_wall=119, wall=0
2023-01-12 19:19:40 | INFO | train_inner | epoch 145:   1313 / 1978 loss=3.023, nll_loss=0.888, word_ins=2.71, length=3.13, ppl=8.13, wps=50733.3, ups=0.85, wpb=59749.6, bsz=1906.2, num_updates=286100, lr=0.000186957, gnorm=1.678, loss_scale=4096, train_wall=118, wall=0
2023-01-12 19:21:38 | INFO | train_inner | epoch 145:   1413 / 1978 loss=3.024, nll_loss=0.886, word_ins=2.708, length=3.155, ppl=8.13, wps=49933, ups=0.85, wpb=58661.6, bsz=1983.1, num_updates=286200, lr=0.000186924, gnorm=1.618, loss_scale=4096, train_wall=117, wall=0
2023-01-12 19:23:36 | INFO | train_inner | epoch 145:   1513 / 1978 loss=3.041, nll_loss=0.902, word_ins=2.723, length=3.185, ppl=8.23, wps=50203.8, ups=0.85, wpb=59240.9, bsz=1893.1, num_updates=286300, lr=0.000186891, gnorm=1.624, loss_scale=4096, train_wall=118, wall=0
2023-01-12 19:25:35 | INFO | train_inner | epoch 145:   1613 / 1978 loss=2.988, nll_loss=0.856, word_ins=2.682, length=3.065, ppl=7.94, wps=49305.6, ups=0.84, wpb=58857.3, bsz=2098.4, num_updates=286400, lr=0.000186859, gnorm=1.562, loss_scale=4096, train_wall=119, wall=0
2023-01-12 19:27:34 | INFO | train_inner | epoch 145:   1713 / 1978 loss=3.011, nll_loss=0.878, word_ins=2.701, length=3.1, ppl=8.06, wps=50184.2, ups=0.84, wpb=59496.7, bsz=1990, num_updates=286500, lr=0.000186826, gnorm=1.517, loss_scale=4096, train_wall=118, wall=0
2023-01-12 19:29:32 | INFO | train_inner | epoch 145:   1813 / 1978 loss=3.021, nll_loss=0.883, word_ins=2.705, length=3.159, ppl=8.12, wps=49954, ups=0.85, wpb=58919.4, bsz=1972.7, num_updates=286600, lr=0.000186794, gnorm=1.558, loss_scale=4096, train_wall=118, wall=0
2023-01-12 19:31:04 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 2048.0
2023-01-12 19:31:31 | INFO | train_inner | epoch 145:   1914 / 1978 loss=3.049, nll_loss=0.915, word_ins=2.734, length=3.144, ppl=8.27, wps=49496.3, ups=0.84, wpb=59204.3, bsz=1937, num_updates=286700, lr=0.000186761, gnorm=1.645, loss_scale=2048, train_wall=119, wall=0
2023-01-12 19:32:47 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 19:33:00 | INFO | valid | epoch 145 | valid on 'valid' subset | loss 4.451 | nll_loss 1.99 | word_ins 3.751 | length 7.003 | ppl 21.88 | wps 117012 | wpb 40242.5 | bsz 1500 | num_updates 286764 | best_loss 4.274
2023-01-12 19:33:00 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 19:33:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint145.pt (epoch 145 @ 286764 updates, score 4.451) (writing took 2.974092182237655 seconds)
2023-01-12 19:33:02 | INFO | fairseq_cli.train | end of epoch 145 (average epoch stats below)
2023-01-12 19:33:02 | INFO | train | epoch 145 | loss 3.013 | nll_loss 0.878 | word_ins 2.701 | length 3.114 | ppl 8.07 | wps 49498.8 | ups 0.83 | wpb 59284.1 | bsz 2001.5 | num_updates 286764 | lr 0.00018674 | gnorm 1.596 | loss_scale 2048 | train_wall 2340 | wall 0
2023-01-12 19:33:03 | INFO | fairseq.trainer | begin training epoch 146
2023-01-12 19:33:54 | INFO | train_inner | epoch 146:     36 / 1978 loss=2.997, nll_loss=0.867, word_ins=2.691, length=3.052, ppl=7.98, wps=41529.8, ups=0.7, wpb=59400.8, bsz=2093.3, num_updates=286800, lr=0.000186728, gnorm=1.635, loss_scale=2048, train_wall=119, wall=0
2023-01-12 19:35:53 | INFO | train_inner | epoch 146:    136 / 1978 loss=3.025, nll_loss=0.891, word_ins=2.713, length=3.123, ppl=8.14, wps=50127.5, ups=0.84, wpb=59391.4, bsz=1934.5, num_updates=286900, lr=0.000186696, gnorm=1.644, loss_scale=2048, train_wall=118, wall=0
2023-01-12 19:37:51 | INFO | train_inner | epoch 146:    236 / 1978 loss=3.034, nll_loss=0.896, word_ins=2.718, length=3.162, ppl=8.19, wps=49658.7, ups=0.85, wpb=58651, bsz=1942.2, num_updates=287000, lr=0.000186663, gnorm=1.626, loss_scale=2048, train_wall=118, wall=0
2023-01-12 19:39:48 | INFO | train_inner | epoch 146:    336 / 1978 loss=3.022, nll_loss=0.882, word_ins=2.705, length=3.172, ppl=8.13, wps=50704.5, ups=0.85, wpb=59397.6, bsz=1920.2, num_updates=287100, lr=0.000186631, gnorm=1.632, loss_scale=2048, train_wall=117, wall=0
2023-01-12 19:41:47 | INFO | train_inner | epoch 146:    436 / 1978 loss=2.995, nll_loss=0.865, word_ins=2.689, length=3.062, ppl=7.97, wps=50317.7, ups=0.84, wpb=59667.7, bsz=2033.4, num_updates=287200, lr=0.000186598, gnorm=1.629, loss_scale=2048, train_wall=118, wall=0
2023-01-12 19:43:44 | INFO | train_inner | epoch 146:    536 / 1978 loss=3.023, nll_loss=0.888, word_ins=2.71, length=3.128, ppl=8.13, wps=50507.4, ups=0.85, wpb=59377.6, bsz=1946.6, num_updates=287300, lr=0.000186566, gnorm=1.637, loss_scale=2048, train_wall=117, wall=0
2023-01-12 19:45:42 | INFO | train_inner | epoch 146:    636 / 1978 loss=3.028, nll_loss=0.893, word_ins=2.715, length=3.135, ppl=8.16, wps=50337.5, ups=0.85, wpb=59465.6, bsz=1897.7, num_updates=287400, lr=0.000186533, gnorm=1.651, loss_scale=2048, train_wall=118, wall=0
2023-01-12 19:47:41 | INFO | train_inner | epoch 146:    736 / 1978 loss=3.003, nll_loss=0.872, word_ins=2.696, length=3.076, ppl=8.02, wps=49908.9, ups=0.84, wpb=59270.1, bsz=2054.9, num_updates=287500, lr=0.000186501, gnorm=1.607, loss_scale=2048, train_wall=119, wall=0
2023-01-12 19:49:40 | INFO | train_inner | epoch 146:    836 / 1978 loss=3.001, nll_loss=0.869, word_ins=2.693, length=3.079, ppl=8, wps=49735.2, ups=0.84, wpb=59042.9, bsz=2108.4, num_updates=287600, lr=0.000186469, gnorm=1.528, loss_scale=2048, train_wall=119, wall=0
2023-01-12 19:51:37 | INFO | train_inner | epoch 146:    936 / 1978 loss=3.009, nll_loss=0.877, word_ins=2.7, length=3.092, ppl=8.05, wps=50223.1, ups=0.85, wpb=59022.1, bsz=1967.3, num_updates=287700, lr=0.000186436, gnorm=1.52, loss_scale=2048, train_wall=117, wall=0
2023-01-12 19:53:37 | INFO | train_inner | epoch 146:   1036 / 1978 loss=2.982, nll_loss=0.852, word_ins=2.677, length=3.045, ppl=7.9, wps=49753.2, ups=0.84, wpb=59285.3, bsz=2138.7, num_updates=287800, lr=0.000186404, gnorm=1.532, loss_scale=2048, train_wall=119, wall=0
2023-01-12 19:55:36 | INFO | train_inner | epoch 146:   1136 / 1978 loss=2.998, nll_loss=0.867, word_ins=2.69, length=3.076, ppl=7.99, wps=50253.3, ups=0.84, wpb=59738.6, bsz=2069.4, num_updates=287900, lr=0.000186371, gnorm=1.567, loss_scale=2048, train_wall=119, wall=0
2023-01-12 19:57:34 | INFO | train_inner | epoch 146:   1236 / 1978 loss=3.001, nll_loss=0.871, word_ins=2.694, length=3.071, ppl=8.01, wps=50063.9, ups=0.84, wpb=59438.3, bsz=2018.7, num_updates=288000, lr=0.000186339, gnorm=1.604, loss_scale=2048, train_wall=119, wall=0
2023-01-12 19:59:33 | INFO | train_inner | epoch 146:   1336 / 1978 loss=3.017, nll_loss=0.885, word_ins=2.707, length=3.096, ppl=8.09, wps=49583.1, ups=0.85, wpb=58665.8, bsz=2000, num_updates=288100, lr=0.000186307, gnorm=1.597, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:01:31 | INFO | train_inner | epoch 146:   1436 / 1978 loss=3.016, nll_loss=0.879, word_ins=2.702, length=3.14, ppl=8.09, wps=50028.2, ups=0.85, wpb=59073.5, bsz=1990.6, num_updates=288200, lr=0.000186274, gnorm=1.696, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:03:30 | INFO | train_inner | epoch 146:   1536 / 1978 loss=3.007, nll_loss=0.876, word_ins=2.699, length=3.082, ppl=8.04, wps=50164, ups=0.84, wpb=59739.4, bsz=2057.4, num_updates=288300, lr=0.000186242, gnorm=1.557, loss_scale=2048, train_wall=119, wall=0
2023-01-12 20:05:28 | INFO | train_inner | epoch 146:   1636 / 1978 loss=3.023, nll_loss=0.887, word_ins=2.709, length=3.138, ppl=8.13, wps=50267.3, ups=0.84, wpb=59533.9, bsz=1955.8, num_updates=288400, lr=0.00018621, gnorm=1.653, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:07:27 | INFO | train_inner | epoch 146:   1736 / 1978 loss=3.031, nll_loss=0.893, word_ins=2.714, length=3.17, ppl=8.18, wps=49747.7, ups=0.84, wpb=59020.1, bsz=1979.7, num_updates=288500, lr=0.000186177, gnorm=1.602, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:09:25 | INFO | train_inner | epoch 146:   1836 / 1978 loss=3.031, nll_loss=0.89, word_ins=2.712, length=3.197, ppl=8.18, wps=50129.1, ups=0.85, wpb=59120.7, bsz=1948.2, num_updates=288600, lr=0.000186145, gnorm=1.636, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:11:23 | INFO | train_inner | epoch 146:   1936 / 1978 loss=3.002, nll_loss=0.869, word_ins=2.693, length=3.098, ppl=8.01, wps=49922.5, ups=0.84, wpb=59219.8, bsz=2038.6, num_updates=288700, lr=0.000186113, gnorm=1.539, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:12:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 20:12:27 | INFO | valid | epoch 146 | valid on 'valid' subset | loss 4.457 | nll_loss 1.984 | word_ins 3.74 | length 7.173 | ppl 21.97 | wps 120576 | wpb 40242.5 | bsz 1500 | num_updates 288742 | best_loss 4.274
2023-01-12 20:12:27 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 20:12:30 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint146.pt (epoch 146 @ 288742 updates, score 4.457) (writing took 2.9273214312270284 seconds)
2023-01-12 20:12:30 | INFO | fairseq_cli.train | end of epoch 146 (average epoch stats below)
2023-01-12 20:12:30 | INFO | train | epoch 146 | loss 3.012 | nll_loss 0.879 | word_ins 2.701 | length 3.111 | ppl 8.07 | wps 49526.5 | ups 0.84 | wpb 59284.3 | bsz 2002.6 | num_updates 288742 | lr 0.000186099 | gnorm 1.606 | loss_scale 2048 | train_wall 2338 | wall 0
2023-01-12 20:12:30 | INFO | fairseq.trainer | begin training epoch 147
2023-01-12 20:13:48 | INFO | train_inner | epoch 147:     58 / 1978 loss=2.993, nll_loss=0.862, word_ins=2.687, length=3.065, ppl=7.96, wps=40879, ups=0.69, wpb=59081.3, bsz=2075.9, num_updates=288800, lr=0.000186081, gnorm=1.623, loss_scale=2048, train_wall=119, wall=0
2023-01-12 20:15:46 | INFO | train_inner | epoch 147:    158 / 1978 loss=3.025, nll_loss=0.89, word_ins=2.711, length=3.136, ppl=8.14, wps=50551.5, ups=0.85, wpb=59557.6, bsz=1880.3, num_updates=288900, lr=0.000186049, gnorm=1.663, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:17:45 | INFO | train_inner | epoch 147:    258 / 1978 loss=3.002, nll_loss=0.866, word_ins=2.69, length=3.116, ppl=8.01, wps=50206.5, ups=0.84, wpb=59657, bsz=2005, num_updates=289000, lr=0.000186016, gnorm=1.632, loss_scale=2048, train_wall=119, wall=0
2023-01-12 20:19:44 | INFO | train_inner | epoch 147:    358 / 1978 loss=2.997, nll_loss=0.868, word_ins=2.691, length=3.052, ppl=7.98, wps=50025, ups=0.84, wpb=59629.5, bsz=2014.3, num_updates=289100, lr=0.000185984, gnorm=1.653, loss_scale=2048, train_wall=119, wall=0
2023-01-12 20:21:42 | INFO | train_inner | epoch 147:    458 / 1978 loss=3.028, nll_loss=0.892, word_ins=2.714, length=3.145, ppl=8.16, wps=49953.2, ups=0.85, wpb=58848.7, bsz=1901.2, num_updates=289200, lr=0.000185952, gnorm=1.622, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:23:40 | INFO | train_inner | epoch 147:    558 / 1978 loss=3.014, nll_loss=0.879, word_ins=2.701, length=3.133, ppl=8.08, wps=50253, ups=0.85, wpb=59429.5, bsz=1971.4, num_updates=289300, lr=0.00018592, gnorm=1.632, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:25:38 | INFO | train_inner | epoch 147:    658 / 1978 loss=3.002, nll_loss=0.869, word_ins=2.692, length=3.093, ppl=8.01, wps=49837.1, ups=0.84, wpb=59060.4, bsz=2001.8, num_updates=289400, lr=0.000185888, gnorm=1.572, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:27:37 | INFO | train_inner | epoch 147:    758 / 1978 loss=3.005, nll_loss=0.873, word_ins=2.697, length=3.082, ppl=8.03, wps=49988.2, ups=0.84, wpb=59481.6, bsz=2074.4, num_updates=289500, lr=0.000185856, gnorm=1.547, loss_scale=2048, train_wall=119, wall=0
2023-01-12 20:29:36 | INFO | train_inner | epoch 147:    858 / 1978 loss=3.034, nll_loss=0.897, word_ins=2.719, length=3.149, ppl=8.19, wps=49832.8, ups=0.85, wpb=58944.3, bsz=1929.8, num_updates=289600, lr=0.000185824, gnorm=1.625, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:31:35 | INFO | train_inner | epoch 147:    958 / 1978 loss=3.003, nll_loss=0.867, word_ins=2.691, length=3.118, ppl=8.02, wps=49882, ups=0.84, wpb=59583.2, bsz=2070.3, num_updates=289700, lr=0.000185791, gnorm=1.681, loss_scale=2048, train_wall=119, wall=0
2023-01-12 20:33:34 | INFO | train_inner | epoch 147:   1058 / 1978 loss=2.998, nll_loss=0.867, word_ins=2.691, length=3.073, ppl=7.99, wps=49833.5, ups=0.84, wpb=59271, bsz=2070.9, num_updates=289800, lr=0.000185759, gnorm=1.661, loss_scale=2048, train_wall=119, wall=0
2023-01-12 20:35:33 | INFO | train_inner | epoch 147:   1158 / 1978 loss=2.994, nll_loss=0.858, word_ins=2.683, length=3.11, ppl=7.96, wps=50051.9, ups=0.84, wpb=59503.7, bsz=2013, num_updates=289900, lr=0.000185727, gnorm=1.602, loss_scale=2048, train_wall=119, wall=0
2023-01-12 20:37:32 | INFO | train_inner | epoch 147:   1258 / 1978 loss=3.004, nll_loss=0.871, word_ins=2.694, length=3.098, ppl=8.02, wps=49431.7, ups=0.84, wpb=58724, bsz=2060.1, num_updates=290000, lr=0.000185695, gnorm=1.528, loss_scale=2048, train_wall=119, wall=0
2023-01-12 20:39:30 | INFO | train_inner | epoch 147:   1358 / 1978 loss=3.011, nll_loss=0.88, word_ins=2.703, length=3.075, ppl=8.06, wps=50041.6, ups=0.84, wpb=59372.4, bsz=2078.5, num_updates=290100, lr=0.000185663, gnorm=1.604, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:41:29 | INFO | train_inner | epoch 147:   1458 / 1978 loss=3.005, nll_loss=0.873, word_ins=2.696, length=3.087, ppl=8.03, wps=50346.6, ups=0.85, wpb=59542.4, bsz=1996.4, num_updates=290200, lr=0.000185631, gnorm=1.603, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:43:26 | INFO | train_inner | epoch 147:   1558 / 1978 loss=3.039, nll_loss=0.898, word_ins=2.719, length=3.193, ppl=8.22, wps=50372.7, ups=0.85, wpb=59132.1, bsz=1888.4, num_updates=290300, lr=0.000185599, gnorm=1.733, loss_scale=2048, train_wall=117, wall=0
2023-01-12 20:45:25 | INFO | train_inner | epoch 147:   1658 / 1978 loss=3.046, nll_loss=0.907, word_ins=2.727, length=3.191, ppl=8.26, wps=48903.3, ups=0.84, wpb=58053.5, bsz=1961.5, num_updates=290400, lr=0.000185567, gnorm=1.554, loss_scale=2048, train_wall=119, wall=0
2023-01-12 20:47:24 | INFO | train_inner | epoch 147:   1758 / 1978 loss=3.005, nll_loss=0.878, word_ins=2.701, length=3.044, ppl=8.03, wps=49854.6, ups=0.84, wpb=59435.5, bsz=2096.5, num_updates=290500, lr=0.000185535, gnorm=1.551, loss_scale=2048, train_wall=119, wall=0
2023-01-12 20:49:22 | INFO | train_inner | epoch 147:   1858 / 1978 loss=3.013, nll_loss=0.872, word_ins=2.695, length=3.182, ppl=8.07, wps=50713, ups=0.85, wpb=59817, bsz=1974.3, num_updates=290600, lr=0.000185504, gnorm=1.643, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:51:20 | INFO | train_inner | epoch 147:   1958 / 1978 loss=3.01, nll_loss=0.875, word_ins=2.698, length=3.123, ppl=8.06, wps=50632, ups=0.85, wpb=59857.7, bsz=1973.2, num_updates=290700, lr=0.000185472, gnorm=1.63, loss_scale=2048, train_wall=118, wall=0
2023-01-12 20:51:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 20:51:56 | INFO | valid | epoch 147 | valid on 'valid' subset | loss 4.433 | nll_loss 1.984 | word_ins 3.742 | length 6.915 | ppl 21.61 | wps 94563.6 | wpb 40242.5 | bsz 1500 | num_updates 290720 | best_loss 4.274
2023-01-12 20:51:56 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 20:51:59 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint147.pt (epoch 147 @ 290720 updates, score 4.433) (writing took 2.9426700733602047 seconds)
2023-01-12 20:51:59 | INFO | fairseq_cli.train | end of epoch 147 (average epoch stats below)
2023-01-12 20:51:59 | INFO | train | epoch 147 | loss 3.011 | nll_loss 0.877 | word_ins 2.7 | length 3.113 | ppl 8.06 | wps 49507.5 | ups 0.84 | wpb 59284.3 | bsz 2002.6 | num_updates 290720 | lr 0.000185465 | gnorm 1.615 | loss_scale 2048 | train_wall 2341 | wall 0
2023-01-12 20:51:59 | INFO | fairseq.trainer | begin training epoch 148
2023-01-12 20:53:42 | INFO | train_inner | epoch 148:     80 / 1978 loss=3.011, nll_loss=0.877, word_ins=2.7, length=3.115, ppl=8.06, wps=41090.9, ups=0.7, wpb=58390.9, bsz=1970.9, num_updates=290800, lr=0.00018544, gnorm=1.6, loss_scale=4096, train_wall=118, wall=0
2023-01-12 20:55:41 | INFO | train_inner | epoch 148:    180 / 1978 loss=2.986, nll_loss=0.857, word_ins=2.683, length=3.031, ppl=7.92, wps=49800.5, ups=0.84, wpb=59197.4, bsz=2059.2, num_updates=290900, lr=0.000185408, gnorm=1.529, loss_scale=4096, train_wall=119, wall=0
2023-01-12 20:57:40 | INFO | train_inner | epoch 148:    280 / 1978 loss=3.005, nll_loss=0.87, word_ins=2.694, length=3.111, ppl=8.03, wps=49766.8, ups=0.84, wpb=59015.1, bsz=2043.8, num_updates=291000, lr=0.000185376, gnorm=1.643, loss_scale=4096, train_wall=118, wall=0
2023-01-12 20:59:38 | INFO | train_inner | epoch 148:    380 / 1978 loss=3.009, nll_loss=0.871, word_ins=2.695, length=3.137, ppl=8.05, wps=49743.6, ups=0.84, wpb=58999.6, bsz=2008.4, num_updates=291100, lr=0.000185344, gnorm=1.61, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:01:37 | INFO | train_inner | epoch 148:    480 / 1978 loss=2.993, nll_loss=0.86, word_ins=2.685, length=3.082, ppl=7.96, wps=50068.6, ups=0.84, wpb=59558.2, bsz=2065.6, num_updates=291200, lr=0.000185312, gnorm=1.543, loss_scale=4096, train_wall=119, wall=0
2023-01-12 21:03:35 | INFO | train_inner | epoch 148:    580 / 1978 loss=3.015, nll_loss=0.882, word_ins=2.705, length=3.106, ppl=8.08, wps=50185.1, ups=0.85, wpb=59243.6, bsz=1958.6, num_updates=291300, lr=0.000185281, gnorm=1.581, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:05:33 | INFO | train_inner | epoch 148:    680 / 1978 loss=3.018, nll_loss=0.882, word_ins=2.705, length=3.135, ppl=8.1, wps=50487.3, ups=0.85, wpb=59633.5, bsz=1917.7, num_updates=291400, lr=0.000185249, gnorm=1.647, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:07:33 | INFO | train_inner | epoch 148:    780 / 1978 loss=2.99, nll_loss=0.857, word_ins=2.682, length=3.081, ppl=7.94, wps=49996.9, ups=0.84, wpb=59498.8, bsz=2054.2, num_updates=291500, lr=0.000185217, gnorm=1.587, loss_scale=4096, train_wall=119, wall=0
2023-01-12 21:09:31 | INFO | train_inner | epoch 148:    880 / 1978 loss=3.011, nll_loss=0.876, word_ins=2.7, length=3.108, ppl=8.06, wps=49701.8, ups=0.84, wpb=58963, bsz=2021.3, num_updates=291600, lr=0.000185185, gnorm=1.579, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:11:30 | INFO | train_inner | epoch 148:    980 / 1978 loss=3.02, nll_loss=0.888, word_ins=2.71, length=3.099, ppl=8.11, wps=49592, ups=0.84, wpb=58950.5, bsz=2051.7, num_updates=291700, lr=0.000185153, gnorm=1.544, loss_scale=4096, train_wall=119, wall=0
2023-01-12 21:13:28 | INFO | train_inner | epoch 148:   1080 / 1978 loss=2.994, nll_loss=0.857, word_ins=2.682, length=3.118, ppl=7.96, wps=49920.6, ups=0.85, wpb=59074.5, bsz=2022.1, num_updates=291800, lr=0.000185122, gnorm=1.581, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:15:28 | INFO | train_inner | epoch 148:   1180 / 1978 loss=3.02, nll_loss=0.886, word_ins=2.708, length=3.115, ppl=8.11, wps=50193.6, ups=0.84, wpb=60057.3, bsz=1982.4, num_updates=291900, lr=0.00018509, gnorm=1.757, loss_scale=4096, train_wall=119, wall=0
2023-01-12 21:17:27 | INFO | train_inner | epoch 148:   1280 / 1978 loss=3.004, nll_loss=0.87, word_ins=2.693, length=3.105, ppl=8.02, wps=50260.3, ups=0.84, wpb=59731.8, bsz=2021.8, num_updates=292000, lr=0.000185058, gnorm=1.628, loss_scale=4096, train_wall=119, wall=0
2023-01-12 21:19:25 | INFO | train_inner | epoch 148:   1380 / 1978 loss=3.032, nll_loss=0.896, word_ins=2.717, length=3.154, ppl=8.18, wps=50016.1, ups=0.84, wpb=59269.5, bsz=1884.1, num_updates=292100, lr=0.000185027, gnorm=1.658, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:21:24 | INFO | train_inner | epoch 148:   1480 / 1978 loss=3.009, nll_loss=0.875, word_ins=2.699, length=3.109, ppl=8.05, wps=49925.5, ups=0.84, wpb=59273.7, bsz=2043.5, num_updates=292200, lr=0.000184995, gnorm=1.697, loss_scale=4096, train_wall=119, wall=0
2023-01-12 21:23:22 | INFO | train_inner | epoch 148:   1580 / 1978 loss=3.015, nll_loss=0.883, word_ins=2.705, length=3.097, ppl=8.08, wps=49832.1, ups=0.85, wpb=58956.2, bsz=2020.5, num_updates=292300, lr=0.000184963, gnorm=1.583, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:25:22 | INFO | train_inner | epoch 148:   1680 / 1978 loss=3.02, nll_loss=0.883, word_ins=2.706, length=3.141, ppl=8.11, wps=49728.1, ups=0.84, wpb=59222.7, bsz=2001.6, num_updates=292400, lr=0.000184932, gnorm=1.589, loss_scale=4096, train_wall=119, wall=0
2023-01-12 21:27:21 | INFO | train_inner | epoch 148:   1780 / 1978 loss=3.005, nll_loss=0.874, word_ins=2.697, length=3.077, ppl=8.03, wps=50048.3, ups=0.84, wpb=59613, bsz=2066.4, num_updates=292500, lr=0.0001849, gnorm=1.603, loss_scale=4096, train_wall=119, wall=0
2023-01-12 21:29:19 | INFO | train_inner | epoch 148:   1880 / 1978 loss=3.03, nll_loss=0.892, word_ins=2.714, length=3.153, ppl=8.17, wps=50233.2, ups=0.85, wpb=59323.6, bsz=1922.6, num_updates=292600, lr=0.000184868, gnorm=1.644, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:31:15 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 21:31:28 | INFO | valid | epoch 148 | valid on 'valid' subset | loss 4.424 | nll_loss 1.996 | word_ins 3.746 | length 6.775 | ppl 21.46 | wps 131048 | wpb 40242.5 | bsz 1500 | num_updates 292698 | best_loss 4.274
2023-01-12 21:31:28 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 21:31:31 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint148.pt (epoch 148 @ 292698 updates, score 4.424) (writing took 2.8689549416303635 seconds)
2023-01-12 21:31:31 | INFO | fairseq_cli.train | end of epoch 148 (average epoch stats below)
2023-01-12 21:31:31 | INFO | train | epoch 148 | loss 3.01 | nll_loss 0.876 | word_ins 2.699 | length 3.111 | ppl 8.06 | wps 49436.1 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 292698 | lr 0.000184838 | gnorm 1.615 | loss_scale 4096 | train_wall 2343 | wall 0
2023-01-12 21:31:31 | INFO | fairseq.trainer | begin training epoch 149
2023-01-12 21:31:42 | INFO | train_inner | epoch 149:      2 / 1978 loss=3.017, nll_loss=0.88, word_ins=2.702, length=3.142, ppl=8.09, wps=41416.1, ups=0.7, wpb=59435.4, bsz=1964.6, num_updates=292700, lr=0.000184837, gnorm=1.669, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:33:41 | INFO | train_inner | epoch 149:    102 / 1978 loss=3.008, nll_loss=0.873, word_ins=2.697, length=3.104, ppl=8.04, wps=49750.3, ups=0.84, wpb=59141.2, bsz=2011.9, num_updates=292800, lr=0.000184805, gnorm=1.6, loss_scale=4096, train_wall=119, wall=0
2023-01-12 21:35:40 | INFO | train_inner | epoch 149:    202 / 1978 loss=3.008, nll_loss=0.874, word_ins=2.698, length=3.103, ppl=8.05, wps=49926.4, ups=0.84, wpb=59420.2, bsz=1983.7, num_updates=292900, lr=0.000184774, gnorm=1.63, loss_scale=4096, train_wall=119, wall=0
2023-01-12 21:37:39 | INFO | train_inner | epoch 149:    302 / 1978 loss=3.025, nll_loss=0.888, word_ins=2.711, length=3.139, ppl=8.14, wps=49664.4, ups=0.84, wpb=58802.1, bsz=1912.6, num_updates=293000, lr=0.000184742, gnorm=1.65, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:39:37 | INFO | train_inner | epoch 149:    402 / 1978 loss=3.009, nll_loss=0.87, word_ins=2.694, length=3.15, ppl=8.05, wps=50375.6, ups=0.85, wpb=59502.7, bsz=1990.2, num_updates=293100, lr=0.000184711, gnorm=1.596, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:41:36 | INFO | train_inner | epoch 149:    502 / 1978 loss=2.972, nll_loss=0.847, word_ins=2.673, length=2.994, ppl=7.85, wps=49890.1, ups=0.84, wpb=59731.5, bsz=2116.2, num_updates=293200, lr=0.000184679, gnorm=1.595, loss_scale=4096, train_wall=120, wall=0
2023-01-12 21:43:36 | INFO | train_inner | epoch 149:    602 / 1978 loss=3.006, nll_loss=0.873, word_ins=2.697, length=3.093, ppl=8.03, wps=49642.1, ups=0.84, wpb=59282.1, bsz=2044.7, num_updates=293300, lr=0.000184648, gnorm=1.655, loss_scale=4096, train_wall=119, wall=0
2023-01-12 21:45:34 | INFO | train_inner | epoch 149:    702 / 1978 loss=3.009, nll_loss=0.875, word_ins=2.697, length=3.118, ppl=8.05, wps=50392.6, ups=0.85, wpb=59420.2, bsz=1990.3, num_updates=293400, lr=0.000184616, gnorm=1.628, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:47:32 | INFO | train_inner | epoch 149:    802 / 1978 loss=2.999, nll_loss=0.862, word_ins=2.687, length=3.121, ppl=7.99, wps=49608.7, ups=0.85, wpb=58562, bsz=2023.5, num_updates=293500, lr=0.000184585, gnorm=1.528, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:49:29 | INFO | train_inner | epoch 149:    902 / 1978 loss=3.022, nll_loss=0.884, word_ins=2.707, length=3.152, ppl=8.12, wps=50196.3, ups=0.85, wpb=59046.8, bsz=1922, num_updates=293600, lr=0.000184553, gnorm=1.629, loss_scale=4096, train_wall=117, wall=0
2023-01-12 21:51:28 | INFO | train_inner | epoch 149:   1002 / 1978 loss=3.029, nll_loss=0.896, word_ins=2.717, length=3.12, ppl=8.16, wps=50136.3, ups=0.84, wpb=59466.6, bsz=1986.2, num_updates=293700, lr=0.000184522, gnorm=1.633, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:53:27 | INFO | train_inner | epoch 149:   1102 / 1978 loss=3.015, nll_loss=0.882, word_ins=2.704, length=3.108, ppl=8.08, wps=50256.5, ups=0.84, wpb=59607, bsz=2000.2, num_updates=293800, lr=0.000184491, gnorm=1.646, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:55:25 | INFO | train_inner | epoch 149:   1202 / 1978 loss=3.027, nll_loss=0.892, word_ins=2.713, length=3.137, ppl=8.15, wps=50402.6, ups=0.85, wpb=59515.9, bsz=1921.9, num_updates=293900, lr=0.000184459, gnorm=1.705, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:57:23 | INFO | train_inner | epoch 149:   1302 / 1978 loss=2.991, nll_loss=0.858, word_ins=2.683, length=3.081, ppl=7.95, wps=50374.5, ups=0.84, wpb=59652.9, bsz=2038.9, num_updates=294000, lr=0.000184428, gnorm=1.596, loss_scale=4096, train_wall=118, wall=0
2023-01-12 21:59:21 | INFO | train_inner | epoch 149:   1402 / 1978 loss=3.016, nll_loss=0.882, word_ins=2.705, length=3.119, ppl=8.09, wps=50156.6, ups=0.85, wpb=59015.2, bsz=1977.5, num_updates=294100, lr=0.000184396, gnorm=1.6, loss_scale=4096, train_wall=117, wall=0
2023-01-12 22:01:19 | INFO | train_inner | epoch 149:   1502 / 1978 loss=3.014, nll_loss=0.878, word_ins=2.701, length=3.128, ppl=8.08, wps=50036.9, ups=0.85, wpb=59146.4, bsz=1954.4, num_updates=294200, lr=0.000184365, gnorm=1.579, loss_scale=4096, train_wall=118, wall=0
2023-01-12 22:03:17 | INFO | train_inner | epoch 149:   1602 / 1978 loss=3.018, nll_loss=0.882, word_ins=2.704, length=3.131, ppl=8.1, wps=50046.8, ups=0.85, wpb=59025.8, bsz=2003.3, num_updates=294300, lr=0.000184334, gnorm=1.606, loss_scale=4096, train_wall=118, wall=0
2023-01-12 22:05:16 | INFO | train_inner | epoch 149:   1702 / 1978 loss=3.009, nll_loss=0.876, word_ins=2.699, length=3.097, ppl=8.05, wps=49901.3, ups=0.84, wpb=59249.4, bsz=2045.1, num_updates=294400, lr=0.000184302, gnorm=1.625, loss_scale=4096, train_wall=119, wall=0
2023-01-12 22:07:15 | INFO | train_inner | epoch 149:   1802 / 1978 loss=3.002, nll_loss=0.866, word_ins=2.69, length=3.128, ppl=8.01, wps=49760.2, ups=0.84, wpb=59338.5, bsz=2038.5, num_updates=294500, lr=0.000184271, gnorm=1.583, loss_scale=4096, train_wall=119, wall=0
2023-01-12 22:09:13 | INFO | train_inner | epoch 149:   1902 / 1978 loss=3.001, nll_loss=0.866, word_ins=2.69, length=3.107, ppl=8.01, wps=50045.6, ups=0.85, wpb=59045.4, bsz=2005.4, num_updates=294600, lr=0.00018424, gnorm=1.594, loss_scale=4096, train_wall=118, wall=0
2023-01-12 22:10:44 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 22:10:59 | INFO | valid | epoch 149 | valid on 'valid' subset | loss 4.49 | nll_loss 2.004 | word_ins 3.757 | length 7.325 | ppl 22.48 | wps 136787 | wpb 40242.5 | bsz 1500 | num_updates 294676 | best_loss 4.274
2023-01-12 22:10:59 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 22:11:02 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint149.pt (epoch 149 @ 294676 updates, score 4.49) (writing took 3.0041416799649596 seconds)
2023-01-12 22:11:02 | INFO | fairseq_cli.train | end of epoch 149 (average epoch stats below)
2023-01-12 22:11:02 | INFO | train | epoch 149 | loss 3.008 | nll_loss 0.874 | word_ins 2.697 | length 3.107 | ppl 8.05 | wps 49445.1 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 294676 | lr 0.000184216 | gnorm 1.614 | loss_scale 4096 | train_wall 2340 | wall 0
2023-01-12 22:11:02 | INFO | fairseq.trainer | begin training epoch 150
2023-01-12 22:11:40 | INFO | train_inner | epoch 150:     24 / 1978 loss=2.979, nll_loss=0.853, word_ins=2.678, length=3.009, ppl=7.89, wps=40594.6, ups=0.68, wpb=59779.1, bsz=2107.3, num_updates=294700, lr=0.000184209, gnorm=1.617, loss_scale=4096, train_wall=120, wall=0
2023-01-12 22:13:39 | INFO | train_inner | epoch 150:    124 / 1978 loss=2.975, nll_loss=0.848, word_ins=2.674, length=3.009, ppl=7.86, wps=50020.3, ups=0.84, wpb=59524.4, bsz=2014.5, num_updates=294800, lr=0.000184177, gnorm=1.597, loss_scale=4096, train_wall=119, wall=0
2023-01-12 22:15:39 | INFO | train_inner | epoch 150:    224 / 1978 loss=2.975, nll_loss=0.848, word_ins=2.674, length=3.006, ppl=7.86, wps=49595.9, ups=0.83, wpb=59411.2, bsz=2113.6, num_updates=294900, lr=0.000184146, gnorm=1.563, loss_scale=8192, train_wall=120, wall=0
2023-01-12 22:17:38 | INFO | train_inner | epoch 150:    324 / 1978 loss=3.007, nll_loss=0.877, word_ins=2.701, length=3.063, ppl=8.04, wps=49906.9, ups=0.84, wpb=59375.2, bsz=1978.3, num_updates=295000, lr=0.000184115, gnorm=1.578, loss_scale=8192, train_wall=119, wall=0
2023-01-12 22:19:37 | INFO | train_inner | epoch 150:    424 / 1978 loss=2.988, nll_loss=0.86, word_ins=2.685, length=3.024, ppl=7.93, wps=50013.9, ups=0.84, wpb=59590.9, bsz=2049, num_updates=295100, lr=0.000184084, gnorm=1.668, loss_scale=8192, train_wall=119, wall=0
2023-01-12 22:21:36 | INFO | train_inner | epoch 150:    524 / 1978 loss=3.011, nll_loss=0.872, word_ins=2.696, length=3.157, ppl=8.06, wps=49759, ups=0.84, wpb=59016.9, bsz=2010.2, num_updates=295200, lr=0.000184053, gnorm=1.567, loss_scale=8192, train_wall=118, wall=0
2023-01-12 22:23:34 | INFO | train_inner | epoch 150:    624 / 1978 loss=3.025, nll_loss=0.886, word_ins=2.708, length=3.166, ppl=8.14, wps=50040.2, ups=0.85, wpb=59110.5, bsz=1929, num_updates=295300, lr=0.000184021, gnorm=1.657, loss_scale=8192, train_wall=118, wall=0
2023-01-12 22:25:31 | INFO | train_inner | epoch 150:    724 / 1978 loss=3.023, nll_loss=0.884, word_ins=2.706, length=3.162, ppl=8.13, wps=50466.4, ups=0.85, wpb=59240.8, bsz=1919.6, num_updates=295400, lr=0.00018399, gnorm=1.659, loss_scale=8192, train_wall=117, wall=0
2023-01-12 22:27:30 | INFO | train_inner | epoch 150:    824 / 1978 loss=3.001, nll_loss=0.869, word_ins=2.693, length=3.078, ppl=8.01, wps=49288.3, ups=0.84, wpb=58382.8, bsz=2053.4, num_updates=295500, lr=0.000183959, gnorm=1.562, loss_scale=8192, train_wall=118, wall=0
2023-01-12 22:29:28 | INFO | train_inner | epoch 150:    924 / 1978 loss=3.005, nll_loss=0.868, word_ins=2.692, length=3.123, ppl=8.03, wps=50203.6, ups=0.85, wpb=59346.8, bsz=1995.9, num_updates=295600, lr=0.000183928, gnorm=1.591, loss_scale=8192, train_wall=118, wall=0
2023-01-12 22:31:26 | INFO | train_inner | epoch 150:   1024 / 1978 loss=3.009, nll_loss=0.879, word_ins=2.702, length=3.076, ppl=8.05, wps=50047.6, ups=0.84, wpb=59295.2, bsz=2011.3, num_updates=295700, lr=0.000183897, gnorm=1.619, loss_scale=8192, train_wall=118, wall=0
2023-01-12 22:33:25 | INFO | train_inner | epoch 150:   1124 / 1978 loss=3.012, nll_loss=0.878, word_ins=2.701, length=3.107, ppl=8.07, wps=50029.7, ups=0.84, wpb=59296.9, bsz=2037.9, num_updates=295800, lr=0.000183866, gnorm=1.608, loss_scale=8192, train_wall=118, wall=0
2023-01-12 22:35:24 | INFO | train_inner | epoch 150:   1224 / 1978 loss=3.013, nll_loss=0.876, word_ins=2.699, length=3.143, ppl=8.07, wps=49394, ups=0.84, wpb=58672.4, bsz=2026.2, num_updates=295900, lr=0.000183835, gnorm=1.623, loss_scale=8192, train_wall=119, wall=0
2023-01-12 22:37:22 | INFO | train_inner | epoch 150:   1324 / 1978 loss=3.013, nll_loss=0.879, word_ins=2.702, length=3.111, ppl=8.07, wps=50198.5, ups=0.84, wpb=59568, bsz=2000, num_updates=296000, lr=0.000183804, gnorm=1.67, loss_scale=8192, train_wall=118, wall=0
2023-01-12 22:39:20 | INFO | train_inner | epoch 150:   1424 / 1978 loss=3.007, nll_loss=0.875, word_ins=2.698, length=3.088, ppl=8.04, wps=50474.5, ups=0.85, wpb=59570, bsz=2014.4, num_updates=296100, lr=0.000183773, gnorm=1.571, loss_scale=8192, train_wall=118, wall=0
2023-01-12 22:41:18 | INFO | train_inner | epoch 150:   1524 / 1978 loss=3.023, nll_loss=0.884, word_ins=2.706, length=3.168, ppl=8.13, wps=50137.1, ups=0.85, wpb=58848.3, bsz=1940.9, num_updates=296200, lr=0.000183742, gnorm=1.616, loss_scale=8192, train_wall=117, wall=0
2023-01-12 22:43:17 | INFO | train_inner | epoch 150:   1624 / 1978 loss=3, nll_loss=0.864, word_ins=2.688, length=3.117, ppl=8, wps=49895.5, ups=0.84, wpb=59387.3, bsz=2075.9, num_updates=296300, lr=0.000183711, gnorm=1.591, loss_scale=8192, train_wall=119, wall=0
2023-01-12 22:45:15 | INFO | train_inner | epoch 150:   1724 / 1978 loss=3.031, nll_loss=0.896, word_ins=2.717, length=3.136, ppl=8.17, wps=50190.9, ups=0.84, wpb=59410.6, bsz=1886.2, num_updates=296400, lr=0.00018368, gnorm=1.682, loss_scale=8192, train_wall=118, wall=0
2023-01-12 22:47:13 | INFO | train_inner | epoch 150:   1824 / 1978 loss=3.014, nll_loss=0.884, word_ins=2.706, length=3.081, ppl=8.08, wps=50871.3, ups=0.85, wpb=59698.8, bsz=1941.4, num_updates=296500, lr=0.000183649, gnorm=1.643, loss_scale=8192, train_wall=117, wall=0
2023-01-12 22:49:12 | INFO | train_inner | epoch 150:   1924 / 1978 loss=3.008, nll_loss=0.874, word_ins=2.697, length=3.111, ppl=8.04, wps=49742, ups=0.84, wpb=59470.3, bsz=2060.5, num_updates=296600, lr=0.000183618, gnorm=1.643, loss_scale=8192, train_wall=119, wall=0
2023-01-12 22:50:16 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 22:50:30 | INFO | valid | epoch 150 | valid on 'valid' subset | loss 4.464 | nll_loss 2.007 | word_ins 3.761 | length 7.03 | ppl 22.08 | wps 129384 | wpb 40242.5 | bsz 1500 | num_updates 296654 | best_loss 4.274
2023-01-12 22:50:30 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 22:50:33 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint150.pt (epoch 150 @ 296654 updates, score 4.464) (writing took 2.9071688554249704 seconds)
2023-01-12 22:50:33 | INFO | fairseq_cli.train | end of epoch 150 (average epoch stats below)
2023-01-12 22:50:33 | INFO | train | epoch 150 | loss 3.007 | nll_loss 0.874 | word_ins 2.697 | length 3.103 | ppl 8.04 | wps 49475.3 | ups 0.83 | wpb 59284.3 | bsz 2002.6 | num_updates 296654 | lr 0.000183601 | gnorm 1.617 | loss_scale 8192 | train_wall 2341 | wall 0
2023-01-12 22:50:33 | INFO | fairseq.trainer | begin training epoch 151
2023-01-12 22:51:37 | INFO | train_inner | epoch 151:     46 / 1978 loss=2.998, nll_loss=0.865, word_ins=2.689, length=3.089, ppl=7.99, wps=41155.6, ups=0.69, wpb=59478.7, bsz=2036.3, num_updates=296700, lr=0.000183587, gnorm=1.605, loss_scale=8192, train_wall=119, wall=0
2023-01-12 22:53:36 | INFO | train_inner | epoch 151:    146 / 1978 loss=2.985, nll_loss=0.855, word_ins=2.68, length=3.047, ppl=7.92, wps=49616, ups=0.84, wpb=59295, bsz=2075.7, num_updates=296800, lr=0.000183556, gnorm=1.599, loss_scale=8192, train_wall=119, wall=0
2023-01-12 22:55:34 | INFO | train_inner | epoch 151:    246 / 1978 loss=3.023, nll_loss=0.887, word_ins=2.71, length=3.131, ppl=8.13, wps=50273.6, ups=0.85, wpb=59269.2, bsz=1947.5, num_updates=296900, lr=0.000183525, gnorm=1.704, loss_scale=8192, train_wall=118, wall=0
2023-01-12 22:57:33 | INFO | train_inner | epoch 151:    346 / 1978 loss=3.002, nll_loss=0.87, word_ins=2.694, length=3.079, ppl=8.01, wps=50175.5, ups=0.84, wpb=59482.3, bsz=2013.8, num_updates=297000, lr=0.000183494, gnorm=1.618, loss_scale=8192, train_wall=118, wall=0
2023-01-12 22:59:32 | INFO | train_inner | epoch 151:    446 / 1978 loss=3.002, nll_loss=0.869, word_ins=2.693, length=3.082, ppl=8.01, wps=50076.5, ups=0.84, wpb=59739.6, bsz=1997.5, num_updates=297100, lr=0.000183463, gnorm=1.698, loss_scale=8192, train_wall=119, wall=0
2023-01-12 23:01:31 | INFO | train_inner | epoch 151:    546 / 1978 loss=3.013, nll_loss=0.879, word_ins=2.702, length=3.101, ppl=8.07, wps=49649.1, ups=0.84, wpb=58891.6, bsz=1978.2, num_updates=297200, lr=0.000183432, gnorm=1.591, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:03:29 | INFO | train_inner | epoch 151:    646 / 1978 loss=3.025, nll_loss=0.888, word_ins=2.71, length=3.145, ppl=8.14, wps=49737.1, ups=0.84, wpb=59088.2, bsz=2009, num_updates=297300, lr=0.000183401, gnorm=1.614, loss_scale=8192, train_wall=119, wall=0
2023-01-12 23:05:27 | INFO | train_inner | epoch 151:    746 / 1978 loss=3.007, nll_loss=0.876, word_ins=2.7, length=3.07, ppl=8.04, wps=50504.2, ups=0.85, wpb=59559.3, bsz=1960.3, num_updates=297400, lr=0.000183371, gnorm=1.652, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:07:26 | INFO | train_inner | epoch 151:    846 / 1978 loss=2.999, nll_loss=0.864, word_ins=2.688, length=3.115, ppl=8, wps=50326.1, ups=0.85, wpb=59523.9, bsz=2034.4, num_updates=297500, lr=0.00018334, gnorm=1.616, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:09:25 | INFO | train_inner | epoch 151:    946 / 1978 loss=2.993, nll_loss=0.865, word_ins=2.689, length=3.039, ppl=7.96, wps=50120.5, ups=0.84, wpb=59715.6, bsz=2027.8, num_updates=297600, lr=0.000183309, gnorm=1.603, loss_scale=8192, train_wall=119, wall=0
2023-01-12 23:11:23 | INFO | train_inner | epoch 151:   1046 / 1978 loss=2.987, nll_loss=0.856, word_ins=2.681, length=3.066, ppl=7.93, wps=50197.7, ups=0.85, wpb=59384.1, bsz=2076.8, num_updates=297700, lr=0.000183278, gnorm=1.589, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:13:21 | INFO | train_inner | epoch 151:   1146 / 1978 loss=3.015, nll_loss=0.881, word_ins=2.704, length=3.114, ppl=8.08, wps=50094.3, ups=0.85, wpb=59256.6, bsz=1986, num_updates=297800, lr=0.000183247, gnorm=1.627, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:15:19 | INFO | train_inner | epoch 151:   1246 / 1978 loss=3.022, nll_loss=0.884, word_ins=2.707, length=3.152, ppl=8.12, wps=50289.5, ups=0.85, wpb=59054.6, bsz=1960.1, num_updates=297900, lr=0.000183217, gnorm=1.587, loss_scale=8192, train_wall=117, wall=0
2023-01-12 23:17:17 | INFO | train_inner | epoch 151:   1346 / 1978 loss=3.016, nll_loss=0.879, word_ins=2.702, length=3.141, ppl=8.09, wps=49804.8, ups=0.85, wpb=58826.1, bsz=1958.2, num_updates=298000, lr=0.000183186, gnorm=1.585, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:19:15 | INFO | train_inner | epoch 151:   1446 / 1978 loss=3.015, nll_loss=0.879, word_ins=2.702, length=3.133, ppl=8.08, wps=50235.3, ups=0.85, wpb=59145.8, bsz=1971.7, num_updates=298100, lr=0.000183155, gnorm=1.638, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:21:14 | INFO | train_inner | epoch 151:   1546 / 1978 loss=2.993, nll_loss=0.867, word_ins=2.691, length=3.027, ppl=7.96, wps=50015.6, ups=0.84, wpb=59500.6, bsz=2099.5, num_updates=298200, lr=0.000183124, gnorm=1.624, loss_scale=8192, train_wall=119, wall=0
2023-01-12 23:23:11 | INFO | train_inner | epoch 151:   1646 / 1978 loss=3.008, nll_loss=0.872, word_ins=2.695, length=3.131, ppl=8.04, wps=50565.6, ups=0.85, wpb=59564, bsz=1950.9, num_updates=298300, lr=0.000183094, gnorm=1.666, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:25:10 | INFO | train_inner | epoch 151:   1746 / 1978 loss=3.012, nll_loss=0.878, word_ins=2.701, length=3.105, ppl=8.07, wps=49795, ups=0.84, wpb=59197.5, bsz=2019.9, num_updates=298400, lr=0.000183063, gnorm=1.652, loss_scale=8192, train_wall=119, wall=0
2023-01-12 23:27:08 | INFO | train_inner | epoch 151:   1846 / 1978 loss=3.026, nll_loss=0.886, word_ins=2.708, length=3.173, ppl=8.14, wps=50209.5, ups=0.85, wpb=59055.6, bsz=1905.2, num_updates=298500, lr=0.000183032, gnorm=1.636, loss_scale=8192, train_wall=117, wall=0
2023-01-12 23:29:07 | INFO | train_inner | epoch 151:   1946 / 1978 loss=3.002, nll_loss=0.873, word_ins=2.696, length=3.054, ppl=8.01, wps=49389.2, ups=0.84, wpb=58837.3, bsz=2059, num_updates=298600, lr=0.000183002, gnorm=1.585, loss_scale=8192, train_wall=119, wall=0
2023-01-12 23:29:45 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 23:29:57 | INFO | valid | epoch 151 | valid on 'valid' subset | loss 4.463 | nll_loss 2.006 | word_ins 3.757 | length 7.057 | ppl 22.05 | wps 133771 | wpb 40242.5 | bsz 1500 | num_updates 298632 | best_loss 4.274
2023-01-12 23:29:57 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 23:29:59 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint151.pt (epoch 151 @ 298632 updates, score 4.463) (writing took 2.7956354450434446 seconds)
2023-01-12 23:29:59 | INFO | fairseq_cli.train | end of epoch 151 (average epoch stats below)
2023-01-12 23:29:59 | INFO | train | epoch 151 | loss 3.007 | nll_loss 0.874 | word_ins 2.697 | length 3.1 | ppl 8.04 | wps 49546 | ups 0.84 | wpb 59284.3 | bsz 2002.6 | num_updates 298632 | lr 0.000182992 | gnorm 1.628 | loss_scale 8192 | train_wall 2339 | wall 0
2023-01-12 23:29:59 | INFO | fairseq.trainer | begin training epoch 152
2023-01-12 23:31:30 | INFO | train_inner | epoch 152:     68 / 1978 loss=3.01, nll_loss=0.872, word_ins=2.695, length=3.147, ppl=8.06, wps=41540.5, ups=0.7, wpb=59182.1, bsz=1995.8, num_updates=298700, lr=0.000182971, gnorm=1.692, loss_scale=8192, train_wall=119, wall=0
2023-01-12 23:33:27 | INFO | train_inner | epoch 152:    168 / 1978 loss=3.014, nll_loss=0.878, word_ins=2.701, length=3.131, ppl=8.08, wps=50209, ups=0.85, wpb=59207.3, bsz=1953, num_updates=298800, lr=0.00018294, gnorm=1.618, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:35:26 | INFO | train_inner | epoch 152:    268 / 1978 loss=2.994, nll_loss=0.862, word_ins=2.687, length=3.074, ppl=7.97, wps=50178.7, ups=0.84, wpb=59523.1, bsz=2036.4, num_updates=298900, lr=0.00018291, gnorm=1.659, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:36:44 | INFO | fairseq.trainer | NOTE: overflow detected, setting loss scale to: 8192.0
2023-01-12 23:37:26 | INFO | train_inner | epoch 152:    369 / 1978 loss=2.995, nll_loss=0.863, word_ins=2.688, length=3.077, ppl=7.97, wps=49493.9, ups=0.83, wpb=59463.3, bsz=2054.1, num_updates=299000, lr=0.000182879, gnorm=1.612, loss_scale=8192, train_wall=120, wall=0
2023-01-12 23:39:24 | INFO | train_inner | epoch 152:    469 / 1978 loss=3.007, nll_loss=0.871, word_ins=2.695, length=3.124, ppl=8.04, wps=49997.9, ups=0.85, wpb=58921.4, bsz=1944.6, num_updates=299100, lr=0.000182849, gnorm=1.6, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:41:22 | INFO | train_inner | epoch 152:    569 / 1978 loss=3.01, nll_loss=0.875, word_ins=2.698, length=3.114, ppl=8.05, wps=49921, ups=0.85, wpb=58958.8, bsz=1965.8, num_updates=299200, lr=0.000182818, gnorm=1.603, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:43:21 | INFO | train_inner | epoch 152:    669 / 1978 loss=2.992, nll_loss=0.859, word_ins=2.684, length=3.078, ppl=7.96, wps=49689.1, ups=0.84, wpb=59114.8, bsz=2029.7, num_updates=299300, lr=0.000182788, gnorm=1.605, loss_scale=8192, train_wall=119, wall=0
2023-01-12 23:45:20 | INFO | train_inner | epoch 152:    769 / 1978 loss=3.012, nll_loss=0.876, word_ins=2.699, length=3.132, ppl=8.07, wps=49874.9, ups=0.84, wpb=59066.8, bsz=1968.5, num_updates=299400, lr=0.000182757, gnorm=1.67, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:47:20 | INFO | train_inner | epoch 152:    869 / 1978 loss=2.987, nll_loss=0.853, word_ins=2.678, length=3.087, ppl=7.93, wps=49812.1, ups=0.83, wpb=59875.2, bsz=2100.5, num_updates=299500, lr=0.000182727, gnorm=1.628, loss_scale=8192, train_wall=120, wall=0
2023-01-12 23:49:18 | INFO | train_inner | epoch 152:    969 / 1978 loss=3.013, nll_loss=0.882, word_ins=2.705, length=3.081, ppl=8.07, wps=49565.3, ups=0.84, wpb=58710.5, bsz=1964.5, num_updates=299600, lr=0.000182696, gnorm=1.568, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:51:17 | INFO | train_inner | epoch 152:   1069 / 1978 loss=3.003, nll_loss=0.866, word_ins=2.69, length=3.134, ppl=8.02, wps=49924.8, ups=0.84, wpb=59257.1, bsz=1987.7, num_updates=299700, lr=0.000182666, gnorm=1.621, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:53:16 | INFO | train_inner | epoch 152:   1169 / 1978 loss=3.003, nll_loss=0.871, word_ins=2.695, length=3.085, ppl=8.02, wps=50085.2, ups=0.84, wpb=59475.6, bsz=2002.6, num_updates=299800, lr=0.000182635, gnorm=1.595, loss_scale=8192, train_wall=119, wall=0
2023-01-12 23:55:15 | INFO | train_inner | epoch 152:   1269 / 1978 loss=3.011, nll_loss=0.875, word_ins=2.698, length=3.125, ppl=8.06, wps=50065.7, ups=0.84, wpb=59535.1, bsz=1998.2, num_updates=299900, lr=0.000182605, gnorm=1.669, loss_scale=8192, train_wall=119, wall=0
2023-01-12 23:57:13 | INFO | train_inner | epoch 152:   1369 / 1978 loss=3.037, nll_loss=0.899, word_ins=2.72, length=3.16, ppl=8.21, wps=50072.2, ups=0.85, wpb=59164, bsz=1940.7, num_updates=300000, lr=0.000182574, gnorm=1.673, loss_scale=8192, train_wall=118, wall=0
2023-01-12 23:57:13 | INFO | fairseq_cli.train | begin validation on "valid" subset
2023-01-12 23:57:25 | INFO | valid | epoch 152 | valid on 'valid' subset | loss 4.49 | nll_loss 1.986 | word_ins 3.745 | length 7.45 | ppl 22.47 | wps 167718 | wpb 40242.5 | bsz 1500 | num_updates 300000 | best_loss 4.274
2023-01-12 23:57:25 | INFO | fairseq_cli.train | begin save checkpoint
2023-01-12 23:57:27 | INFO | fairseq.checkpoint_utils | saved checkpoint /mnt/cache/jiangshuyang/checkpoints/WMTdeen_distill_CMLMC_L5D3_300k_covamlpbothself_amlpseqcross_w5/checkpoint_last.pt (epoch 152 @ 300000 updates, score 4.49) (writing took 2.114807491656393 seconds)
2023-01-12 23:57:27 | INFO | fairseq_cli.train | end of epoch 152 (average epoch stats below)
2023-01-12 23:57:27 | INFO | train | epoch 152 | loss 3.006 | nll_loss 0.871 | word_ins 2.695 | length 3.108 | ppl 8.03 | wps 49187.4 | ups 0.83 | wpb 59251.2 | bsz 1998.6 | num_updates 300000 | lr 0.000182574 | gnorm 1.625 | loss_scale 8192 | train_wall 1621 | wall 0
2023-01-12 23:57:27 | INFO | fairseq_cli.train | done training in 104106.7 seconds
